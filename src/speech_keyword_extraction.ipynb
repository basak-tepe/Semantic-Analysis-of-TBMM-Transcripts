{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speech Keyword Extraction using Aya Expanse 8B\n",
        "\n",
        "This notebook extracts 10 keywords from each parliament speech using the Aya Expanse 8B language model.\n",
        "Keywords prioritize topic-related words and are saved to a CSV file with speech_id and keywords columns.\n",
        "\n",
        "**Key Features:**\n",
        "- ‚ö° **Batch processing** for 10-30x speedup (optimized for 45GB GPU with batch_size=32)\n",
        "- üíæ **Auto-saves to Elasticsearch** every 100 speeches (no data loss on interruption)\n",
        "- üîÑ **Resume mode**: Automatically skips already processed speeches when re-run\n",
        "- üéØ **Topic-aware**: Uses topic labels to extract more relevant keywords\n",
        "\n",
        "**Elasticsearch Fields Created:**\n",
        "- `keywords`: Array of keyword strings\n",
        "- `keywords_str`: Comma-separated keyword string\n",
        "\n",
        "## Requirements:\n",
        "- transformers library\n",
        "- torch\n",
        "- elasticsearch\n",
        "- pandas\n",
        "- tqdm (for progress bars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if keywords field already exists in Elasticsearch\n",
        "print(\"üîç Checking for existing keywords in Elasticsearch...\\n\")\n",
        "\n",
        "try:\n",
        "    es_check = Elasticsearch(hosts=[ELASTICSEARCH_HOST])\n",
        "    \n",
        "    if es_check.ping():\n",
        "        # Check total documents\n",
        "        total_count = es_check.count(index=ELASTICSEARCH_INDEX)\n",
        "        print(f\"Total documents in index: {total_count['count']:,}\\n\")\n",
        "        \n",
        "        # Query for documents with keywords\n",
        "        query_with_kw = {\n",
        "            'query': {'exists': {'field': 'keywords'}},\n",
        "            'size': 3,\n",
        "            '_source': ['speech_giver', 'keywords', 'keywords_str', 'groq_topic_label', 'year']\n",
        "        }\n",
        "        \n",
        "        result = es_check.search(index=ELASTICSEARCH_INDEX, body=query_with_kw)\n",
        "        docs_with_kw = result['hits']['total']['value']\n",
        "        \n",
        "        print(f\"üìä Documents WITH keywords: {docs_with_kw:,}\")\n",
        "        print(f\"üìä Documents WITHOUT keywords: {total_count['count'] - docs_with_kw:,}\\n\")\n",
        "        \n",
        "        if docs_with_kw > 0:\n",
        "            percentage = (docs_with_kw / total_count['count']) * 100\n",
        "            print(f\"‚úÖ Progress: {percentage:.1f}% complete\\n\")\n",
        "            print(\"üìã Example documents with keywords:\\n\" + \"=\"*80)\n",
        "            \n",
        "            for i, hit in enumerate(result['hits']['hits'], 1):\n",
        "                source = hit['_source']\n",
        "                print(f\"\\nExample {i}:\")\n",
        "                print(f\"  Speech ID: {hit['_id']}\")\n",
        "                print(f\"  Speaker: {source.get('speech_giver', 'N/A')}\")\n",
        "                print(f\"  Year: {source.get('year', 'N/A')}\")\n",
        "                print(f\"  Topic: {source.get('groq_topic_label', 'N/A')}\")\n",
        "                \n",
        "                if 'keywords' in source:\n",
        "                    kw = source['keywords']\n",
        "                    print(f\"  Keywords (array): {kw}\")\n",
        "                    print(f\"  Count: {len(kw)} keywords\")\n",
        "                \n",
        "                if 'keywords_str' in source:\n",
        "                    print(f\"  Keywords (string): {source['keywords_str']}\")\n",
        "                print(\"-\"*80)\n",
        "        else:\n",
        "            print(\"‚ùå No keywords found yet. Run the extraction process below.\")\n",
        "    else:\n",
        "        print(\"‚ùå Cannot connect to Elasticsearch\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Error checking Elasticsearch: {e}\")\n",
        "    print(\"   Will proceed with keyword extraction...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from elasticsearch import Elasticsearch\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "ELASTICSEARCH_HOST = os.getenv(\"ELASTICSEARCH_HOST\", \"http://localhost:9200\")\n",
        "ELASTICSEARCH_INDEX = os.getenv(\"ELASTICSEARCH_INDEX\", \"parliament_speeches\")\n",
        "OUTPUT_CSV = \"../data/speech_keywords.csv\"\n",
        "BATCH_SIZE = 1000  # Batch size for fetching speeches\n",
        "MODEL_ID = \"CohereLabs/aya-expanse-8b\"\n",
        "\n",
        "# Check GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Aya Expanse 8B Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Loading model: {MODEL_ID}...\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Fix padding for decoder-only models (required for batch processing)\n",
        "tokenizer.padding_side = 'left'\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\" if device == \"cuda\" else None,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "if device == \"cpu\":\n",
        "    model = model.to(device)\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Connect to Elasticsearch and Fetch Speeches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def connect_to_elasticsearch() -> Elasticsearch:\n",
        "    \"\"\"Connect to Elasticsearch and verify connection.\"\"\"\n",
        "    print(f\"üîå Connecting to Elasticsearch at {ELASTICSEARCH_HOST}...\")\n",
        "    \n",
        "    try:\n",
        "        es = Elasticsearch(hosts=[ELASTICSEARCH_HOST])\n",
        "        \n",
        "        if es.ping():\n",
        "            count = es.count(index=ELASTICSEARCH_INDEX)\n",
        "            total_docs = count.get('count', 0)\n",
        "            print(f\"‚úÖ Connected to Elasticsearch\")\n",
        "            print(f\"üìä Index: {ELASTICSEARCH_INDEX}\")\n",
        "            print(f\"üìä Total documents: {total_docs:,}\")\n",
        "            return es\n",
        "        else:\n",
        "            raise Exception(\"Ping failed\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to connect to Elasticsearch: {e}\")\n",
        "        print(f\"   Make sure Elasticsearch is running on {ELASTICSEARCH_HOST}\")\n",
        "        raise\n",
        "\n",
        "# Connect\n",
        "es = connect_to_elasticsearch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_all_speeches(es: Elasticsearch, limit: int = None, skip_processed: bool = True) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Fetch speeches from Elasticsearch using scroll API.\n",
        "    \n",
        "    Args:\n",
        "        es: Elasticsearch client\n",
        "        limit: Optional limit on number of speeches to fetch (for testing)\n",
        "        skip_processed: Skip speeches that already have keywords (for resuming)\n",
        "    \n",
        "    Returns:\n",
        "        List of speech dictionaries with id, content, and metadata\n",
        "    \"\"\"\n",
        "    print(f\"\\nüì• Fetching speeches from Elasticsearch...\")\n",
        "    if skip_processed:\n",
        "        print(\"   Skipping speeches that already have keywords (resume mode)...\")\n",
        "    \n",
        "    # Build query - optionally skip already processed speeches\n",
        "    must_conditions = [{\"exists\": {\"field\": \"content\"}}]\n",
        "    must_not_conditions = [{\"term\": {\"content\": \"\"}}]\n",
        "    \n",
        "    if skip_processed:\n",
        "        # Skip speeches that already have keywords field\n",
        "        must_not_conditions.append({\"exists\": {\"field\": \"keywords\"}})\n",
        "    \n",
        "    query = {\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": must_conditions,\n",
        "                \"must_not\": must_not_conditions\n",
        "            }\n",
        "        },\n",
        "        \"size\": BATCH_SIZE,\n",
        "        \"_source\": [\n",
        "            \"content\", \"speech_giver\", \"term\", \"year\", \n",
        "            \"session_date\", \"topic_label\", \"groq_topic_label\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    speeches = []\n",
        "    scroll_id = None\n",
        "    batch_count = 0\n",
        "    \n",
        "    try:\n",
        "        response = es.search(\n",
        "            index=ELASTICSEARCH_INDEX,\n",
        "            body=query,\n",
        "            scroll='5m'\n",
        "        )\n",
        "        \n",
        "        scroll_id = response['_scroll_id']\n",
        "        hits = response['hits']['hits']\n",
        "        \n",
        "        while hits:\n",
        "            batch_count += 1\n",
        "            print(f\"Batch {batch_count}: Processing {len(hits)} speeches...\")\n",
        "            \n",
        "            for hit in hits:\n",
        "                source = hit['_source']\n",
        "                \n",
        "                if source.get('content') and source['content'].strip():\n",
        "                    speeches.append({\n",
        "                        'speech_id': hit['_id'],\n",
        "                        'content': source['content'],\n",
        "                        'speech_giver': source.get('speech_giver', ''),\n",
        "                        'topic_label': source.get('topic_label', ''),\n",
        "                        'groq_topic_label': source.get('groq_topic_label', ''),\n",
        "                        'year': source.get('year'),\n",
        "                    })\n",
        "            \n",
        "            # Check if limit reached\n",
        "            if limit and len(speeches) >= limit:\n",
        "                speeches = speeches[:limit]\n",
        "                break\n",
        "            \n",
        "            # Get next batch\n",
        "            response = es.scroll(scroll_id=scroll_id, scroll='5m')\n",
        "            scroll_id = response['_scroll_id']\n",
        "            hits = response['hits']['hits']\n",
        "        \n",
        "        print(f\"‚úÖ Successfully fetched {len(speeches):,} speeches\")\n",
        "        return speeches\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error fetching speeches: {e}\")\n",
        "        return []\n",
        "        \n",
        "    finally:\n",
        "        if scroll_id:\n",
        "            try:\n",
        "                es.clear_scroll(scroll_id=scroll_id)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# Fetch speeches (use limit=10 for testing, remove for full run)\n",
        "# skip_processed=True means it will resume from where it left off\n",
        "speeches = fetch_all_speeches(es, limit=None, skip_processed=True)  # Change to limit=10 for testing\n",
        "print(f\"\\nTotal speeches to process: {len(speeches):,}\")\n",
        "\n",
        "if len(speeches) == 0:\n",
        "    print(\"‚úÖ All speeches already have keywords! Nothing to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Keyword Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_keywords_from_text(gen_text: str) -> str:\n",
        "    \"\"\"Helper to extract keywords from generated text and clean special tokens.\"\"\"\n",
        "    try:\n",
        "        # List of special tokens to remove\n",
        "        special_tokens = [\n",
        "            '<|START_OF_TURN_TOKEN|>',\n",
        "            '<|END_OF_TURN_TOKEN|>',\n",
        "            '<|CHATBOT_TOKEN|>',\n",
        "            '<|USER_TOKEN|>',\n",
        "            '<|SYSTEM_TOKEN|>',\n",
        "            '<BOS_TOKEN>',\n",
        "            '<EOS_TOKEN>',\n",
        "            '<s>',\n",
        "            '</s>',\n",
        "        ]\n",
        "        \n",
        "        # Find where keywords start\n",
        "        keywords_start_phrase = \"Anahtar kelimeler:\"\n",
        "        if keywords_start_phrase in gen_text:\n",
        "            keywords_start = gen_text.find(keywords_start_phrase) + len(keywords_start_phrase)\n",
        "            keywords = gen_text[keywords_start:].strip()\n",
        "        else:\n",
        "            # If phrase not found, try to extract from the end of generation\n",
        "            keywords = gen_text.strip()\n",
        "        \n",
        "        # Take only first line\n",
        "        keywords = keywords.split('\\\\n')[0].strip()\n",
        "        \n",
        "        # Remove all special tokens\n",
        "        for token in special_tokens:\n",
        "            keywords = keywords.replace(token, '')\n",
        "        \n",
        "        # Clean up extra whitespace and commas\n",
        "        keywords = keywords.strip()\n",
        "        keywords = ', '.join([k.strip() for k in keywords.split(',') if k.strip()])\n",
        "        \n",
        "        # Validate that we have actual content (not just empty or single character)\n",
        "        if not keywords or len(keywords) < 3 or keywords.count(',') == 0:\n",
        "            return \"ERROR: No valid keywords generated\"\n",
        "        \n",
        "        return keywords\n",
        "        \n",
        "    except Exception as e:\n",
        "        return f\"ERROR: Could not extract keywords - {str(e)}\"\n",
        "\n",
        "def extract_keywords_batch(speeches_batch: List[Dict], batch_size: int = 8) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extract keywords from multiple speeches at once (batch processing for speed).\n",
        "    \n",
        "    Args:\n",
        "        speeches_batch: List of speech dictionaries\n",
        "        batch_size: Number of speeches to process together\n",
        "    \n",
        "    Returns:\n",
        "        List of comma-separated keyword strings\n",
        "    \"\"\"\n",
        "    max_chars = 2000\n",
        "    prompts = []\n",
        "    \n",
        "    for speech in speeches_batch:\n",
        "        speech_content = speech['content'][:max_chars]\n",
        "        topic_context = f\" Konu: '{speech.get('groq_topic_label', '')}'.\" if speech.get('groq_topic_label') else \"\"\n",
        "        \n",
        "        prompt = f\"\"\"A≈üaƒüƒ±daki TBMM konu≈ümasƒ±ndan 10 anahtar kelime √ßƒ±kar. Sadece anahtar kelimeleri virg√ºlle ayrƒ±lmƒ±≈ü olarak listele.{topic_context}\n",
        "\n",
        "Konu≈üma:\n",
        "{speech_content}\n",
        "\n",
        "Anahtar kelimeler:\"\"\"\n",
        "        prompts.append(prompt)\n",
        "    \n",
        "    # Batch tokenization\n",
        "    messages_batch = [[{\"role\": \"user\", \"content\": p}] for p in prompts]\n",
        "    \n",
        "    # Tokenize all messages\n",
        "    tokenized = []\n",
        "    for msg in messages_batch:\n",
        "        ids = tokenizer.apply_chat_template(\n",
        "            msg, \n",
        "            tokenize=True, \n",
        "            add_generation_prompt=True, \n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        tokenized.append(ids.squeeze(0))\n",
        "    \n",
        "    # Pad to same length (left padding for decoder models)\n",
        "    from torch.nn.utils.rnn import pad_sequence\n",
        "    input_ids_batch = pad_sequence(\n",
        "        tokenized, \n",
        "        batch_first=True, \n",
        "        padding_value=tokenizer.pad_token_id\n",
        "    ).to(device)\n",
        "    \n",
        "    attention_mask = (input_ids_batch != tokenizer.pad_token_id).long().to(device)\n",
        "    \n",
        "    # Generate for entire batch (much faster!)\n",
        "    with torch.no_grad():\n",
        "        gen_tokens = model.generate(\n",
        "            input_ids_batch,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=50,  # Reduced from 100\n",
        "            do_sample=False,    # Greedy decoding is faster\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode all results\n",
        "    results = []\n",
        "    for gen_token in gen_tokens:\n",
        "        gen_text = tokenizer.decode(gen_token, skip_special_tokens=True)\n",
        "        keywords = extract_keywords_from_text(gen_text)\n",
        "        results.append(keywords)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def extract_keywords(speech_content: str, topic_label: str = \"\", speech_giver: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Extract 10 keywords from a single speech (single processing, slower).\n",
        "    Use extract_keywords_batch() for better performance.\n",
        "    \"\"\"\n",
        "    speech_dict = {\n",
        "        'content': speech_content,\n",
        "        'groq_topic_label': topic_label,\n",
        "        'speech_giver': speech_giver\n",
        "    }\n",
        "    return extract_keywords_batch([speech_dict], batch_size=1)[0]\n",
        "\n",
        "# Test with a sample speech\n",
        "if len(speeches) > 0:\n",
        "    print(\"\\nüß™ Testing keyword extraction with first speech...\\n\")\n",
        "    sample = speeches[0]\n",
        "    print(f\"Speech ID: {sample['speech_id']}\")\n",
        "    print(f\"Speaker: {sample['speech_giver']}\")\n",
        "    print(f\"Topic: {sample.get('groq_topic_label', 'N/A')}\")\n",
        "    print(f\"Content preview: {sample['content'][:200]}...\\n\")\n",
        "    \n",
        "    keywords = extract_keywords(\n",
        "        sample['content'], \n",
        "        sample.get('groq_topic_label', ''),\n",
        "        sample['speech_giver']\n",
        "    )\n",
        "    print(f\"Extracted keywords: {keywords}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Process All Speeches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_all_speeches(speeches: List[Dict]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Process all speeches and extract keywords.\n",
        "    \n",
        "    Args:\n",
        "        speeches: List of speech dictionaries\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with speech_id and keywords columns\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(f\"\\nüîÑ Processing {len(speeches):,} speeches...\")\n",
        "    print(\"This will take some time...\\n\")\n",
        "    \n",
        "    for speech in tqdm(speeches, desc=\"Extracting keywords\"):\n",
        "        try:\n",
        "            keywords = extract_keywords(\n",
        "                speech['content'],\n",
        "                speech.get('groq_topic_label', ''),\n",
        "                speech['speech_giver']\n",
        "            )\n",
        "            \n",
        "            results.append({\n",
        "                'speech_id': speech['speech_id'],\n",
        "                'keywords': keywords,\n",
        "                'speech_giver': speech['speech_giver'],\n",
        "                'year': speech.get('year', ''),\n",
        "                'topic_label': speech.get('groq_topic_label', '')\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è  Error processing speech {speech['speech_id']}: {e}\")\n",
        "            results.append({\n",
        "                'speech_id': speech['speech_id'],\n",
        "                'keywords': 'ERROR',\n",
        "                'speech_giver': speech['speech_giver'],\n",
        "                'year': speech.get('year', ''),\n",
        "                'topic_label': speech.get('groq_topic_label', '')\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(results)\n",
        "    print(f\"\\n‚úÖ Processed {len(df):,} speeches\")\n",
        "    return df\n",
        "\n",
        "# Process all speeches with batch processing\n",
        "# Adjust batch_size based on your GPU memory:\n",
        "# - 8GB GPU: batch_size=4-8\n",
        "# - 16GB GPU: batch_size=8-16  \n",
        "# - 24GB GPU: batch_size=16-32\n",
        "# - 45GB+ GPU: batch_size=32-64\n",
        "# - CPU: batch_size=1-2\n",
        "batch_size = 32 if device == 'cuda' else 1  # 45GB GPU can handle 32-64\n",
        "\n",
        "# Upload to Elasticsearch every 100 speeches for safety\n",
        "results_df = process_all_speeches(speeches, es, batch_size=batch_size, upload_every=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to CSV\n",
        "results_df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"\\nüíæ Results saved to: {OUTPUT_CSV}\")\n",
        "print(f\"Total rows: {len(results_df):,}\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\nüìä Sample results:\")\n",
        "print(results_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistics and Quality Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for errors\n",
        "error_count = (results_df['keywords'] == 'ERROR').sum()\n",
        "error_or_missing = results_df['keywords'].str.contains('ERROR', na=True).sum()\n",
        "\n",
        "print(f\"\\nüìà Statistics:\")\n",
        "print(f\"Total speeches processed: {len(results_df):,}\")\n",
        "print(f\"Errors: {error_count}\")\n",
        "print(f\"Success rate: {((len(results_df) - error_count) / len(results_df) * 100):.2f}%\")\n",
        "\n",
        "# Sample keywords by topic\n",
        "if 'topic_label' in results_df.columns and results_df['topic_label'].notna().any():\n",
        "    print(\"\\nüìã Sample keywords by topic:\")\n",
        "    for topic in results_df['topic_label'].dropna().unique()[:5]:\n",
        "        topic_df = results_df[results_df['topic_label'] == topic]\n",
        "        if len(topic_df) > 0:\n",
        "            print(f\"\\n{topic}:\")\n",
        "            print(f\"  Sample: {topic_df.iloc[0]['keywords']}\")\n",
        "\n",
        "# Keyword count distribution\n",
        "results_df['keyword_count'] = results_df['keywords'].str.split(',').str.len()\n",
        "print(f\"\\nüî¢ Keyword count distribution:\")\n",
        "print(results_df['keyword_count'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Verification (Keywords Already Uploaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def upload_keywords_to_elasticsearch(es: Elasticsearch, results_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Upload extracted keywords back to Elasticsearch.\n",
        "    \n",
        "    Args:\n",
        "        es: Elasticsearch client\n",
        "        results_df: DataFrame with speech_id and keywords\n",
        "    \"\"\"\n",
        "    print(\"\\nüíæ Uploading keywords to Elasticsearch...\")\n",
        "    \n",
        "    from elasticsearch import helpers\n",
        "    \n",
        "    actions = []\n",
        "    for _, row in results_df.iterrows():\n",
        "        if row['keywords'] != 'ERROR':\n",
        "            # Convert comma-separated string to list\n",
        "            keywords_list = [k.strip() for k in row['keywords'].split(',')]\n",
        "            \n",
        "            actions.append({\n",
        "                '_op_type': 'update',\n",
        "                '_index': ELASTICSEARCH_INDEX,\n",
        "                '_id': row['speech_id'],\n",
        "                'doc': {\n",
        "                    'keywords': keywords_list,\n",
        "                    'keywords_str': row['keywords']\n",
        "                }\n",
        "            })\n",
        "    \n",
        "    # Bulk update\n",
        "    success, failed = helpers.bulk(es, actions, raise_on_error=False)\n",
        "    \n",
        "    print(f\"‚úÖ Successfully updated {success:,} documents\")\n",
        "    if failed:\n",
        "        print(f\"‚ö†Ô∏è  Failed to update {len(failed)} documents\")\n",
        "\n",
        "# Keywords were already uploaded during processing (every 100 speeches)\n",
        "# Let's verify by checking a random sample from Elasticsearch\n",
        "\n",
        "if len(results_df) > 0:\n",
        "    print(\"\\nüîç Verifying keywords in Elasticsearch...\\n\")\n",
        "    \n",
        "    # Check first 3 speeches\n",
        "    for i in range(min(3, len(results_df))):\n",
        "        speech_id = results_df.iloc[i]['speech_id']\n",
        "        \n",
        "        try:\n",
        "            doc = es.get(index=ELASTICSEARCH_INDEX, id=speech_id)\n",
        "            es_keywords = doc['_source'].get('keywords', [])\n",
        "            \n",
        "            print(f\"Speech ID: {speech_id}\")\n",
        "            print(f\"  Keywords in ES: {es_keywords[:5]}...\" if len(es_keywords) > 5 else f\"  Keywords in ES: {es_keywords}\")\n",
        "            print(f\"  CSV keywords: {results_df.iloc[i]['keywords'][:100]}...\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Could not verify speech {speech_id}: {e}\\n\")\n",
        "    \n",
        "    print(\"‚úÖ Keywords have been uploaded to Elasticsearch during processing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook:\n",
        "1. ‚úÖ Loaded the Aya Expanse 8B model with optimized settings\n",
        "2. ‚úÖ Fetched unprocessed speeches from Elasticsearch (resume mode)\n",
        "3. ‚úÖ Extracted 10 keywords using batch processing (10-30x faster)\n",
        "4. ‚úÖ Uploaded keywords to Elasticsearch every 100 speeches\n",
        "5. ‚úÖ Saved results to CSV with speech_id and keywords columns\n",
        "6. ‚úÖ Provided statistics and quality checks\n",
        "\n",
        "**Output file:** `data/speech_keywords.csv`\n",
        "\n",
        "**Columns:**\n",
        "- `speech_id`: Unique identifier for each speech\n",
        "- `keywords`: Comma-separated list of 10 keywords\n",
        "- `speech_giver`: Speaker name (for reference)\n",
        "- `year`: Speech year (for reference)\n",
        "- `topic_label`: Topic label (for reference)\n",
        "\n",
        "**Elasticsearch Fields Created:**\n",
        "- `keywords`: Array of keyword strings\n",
        "- `keywords_str`: Comma-separated keyword string\n",
        "\n",
        "**Performance Notes:**\n",
        "- Uses batch processing (32 speeches at once on 45GB GPU) for 10-30x speedup\n",
        "- Greedy decoding (do_sample=False) for faster generation\n",
        "- Left padding for decoder-only architecture compatibility\n",
        "- Uploads every 100 speeches to prevent data loss\n",
        "- Resume mode: Re-running skips already processed speeches\n",
        "\n",
        "**Model Notes:**\n",
        "- Topic-aware prompting for better keyword relevance\n",
        "- Long speeches truncated to 2000 characters\n",
        "- GPU acceleration (FP16) for speed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Download Results from Colab (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Only run this cell if you're using Google Colab\n",
        "# This will zip the CSV and download it to your computer\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    import zipfile\n",
        "    import os\n",
        "    \n",
        "    # Create zip file\n",
        "    zip_filename = 'speech_keywords.zip'\n",
        "    \n",
        "    print(f\"üì¶ Creating zip file: {zip_filename}...\")\n",
        "    \n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        if os.path.exists(OUTPUT_CSV):\n",
        "            zipf.write(OUTPUT_CSV, os.path.basename(OUTPUT_CSV))\n",
        "            print(f\"   Added: {OUTPUT_CSV}\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  File not found: {OUTPUT_CSV}\")\n",
        "    \n",
        "    # Get file size\n",
        "    file_size = os.path.getsize(zip_filename) / (1024 * 1024)  # MB\n",
        "    print(f\"\\n‚úÖ Zip file created: {zip_filename} ({file_size:.2f} MB)\")\n",
        "    print(f\"üì• Downloading to your computer...\")\n",
        "    \n",
        "    # Download\n",
        "    files.download(zip_filename)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Download complete!\")\n",
        "    print(f\"   Check your Downloads folder for: {zip_filename}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è  This cell only works in Google Colab.\")\n",
        "    print(f\"   If you're running locally, the CSV is already saved at: {OUTPUT_CSV}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
