{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6bfb95eb",
      "metadata": {
        "id": "6bfb95eb",
        "lines_to_next_cell": 2
      },
      "source": [
        "--- WiDiD: Incremental Word Sense Discovery for Parliamentary Speeches ---\n",
        "Term 27, Year 1‚Äì2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "34db69a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34db69a6",
        "outputId": "3223a3d7-869b-46d3-9068-2e9a217ee167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: elasticsearch==8.6.2 in /usr/local/lib/python3.12/dist-packages (8.6.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: elastic-transport<9,>=8 in /usr/local/lib/python3.12/dist-packages (from elasticsearch==8.6.2) (8.17.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from elastic-transport<9,>=8->elasticsearch==8.6.2) (2.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from elastic-transport<9,>=8->elasticsearch==8.6.2) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n"
          ]
        }
      ],
      "source": [
        "# ##¬†Imports\n",
        "%pip install \"elasticsearch==8.6.2\" sentence-transformers scikit-learn pandas matplotlib\n",
        "from elasticsearch import Elasticsearch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d6474a6",
      "metadata": {
        "id": "4d6474a6"
      },
      "outputs": [],
      "source": [
        "# ## Configuration\n",
        "INDEX_NAME = \"parliament_speeches\"\n",
        "ES_URL = \"https://assure-hammer-flooring-appreciate.trycloudflare.com\"   # adjust if different\n",
        "TARGET_WORDS = [\"iklim\",\"anaokulu\",\"tatil\",\"alƒ±≈üveri≈ü\",\"d√∂viz\",\"emekli\",\"bayram\",\"ayasofya\",\"zam\"]\n",
        "BASELINE_MAX_CLUSTERS = 30\n",
        "MAX_CLUSTERS = 50\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "TOP_K_CLUSTERS = 3  # Track top-3 clusters per year\n",
        "OUTPUT_DIR = \"./lorenz_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Create subdirectory for each word (for API compatibility)\n",
        "# The API expects: src/widid_results/{word}/tsne_{word}.csv\n",
        "WIDID_RESULTS_DIR = \"./widid_results\"\n",
        "os.makedirs(WIDID_RESULTS_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "200810f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "200810f4",
        "lines_to_next_cell": 1,
        "outputId": "dd9f92a1-49a9-4ca6-879f-a260069fa024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Elasticsearch \n",
            "8.6.1\n"
          ]
        }
      ],
      "source": [
        "# ## Connect to Elasticsearch\n",
        "es = Elasticsearch(ES_URL)\n",
        "print(\"Connected to Elasticsearch \")\n",
        "print(es.info().body[\"version\"][\"number\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "659b4caa",
      "metadata": {
        "id": "659b4caa"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011a1394",
      "metadata": {
        "id": "011a1394",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def fetch_speeches(term, year, size=10000):\n",
        "    \"\"\"Fetch speeches for a specific term and year with metadata.\"\"\"\n",
        "    query = {\n",
        "        \"size\": size,\n",
        "        \"_source\": [\"content\", \"term\", \"year\", \"file\", \"session_date\"],\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [\n",
        "                    {\"term\": {\"term\": term}},\n",
        "                    {\"term\": {\"year\": year}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    res = es.search(index=INDEX_NAME, body=query)\n",
        "    return [{\n",
        "        \"content\": hit[\"_source\"][\"content\"],\n",
        "        \"term\": hit[\"_source\"].get(\"term\"),\n",
        "        \"year\": hit[\"_source\"].get(\"year\"),\n",
        "        \"file\": hit[\"_source\"].get(\"file\"),\n",
        "        \"session_date\": hit[\"_source\"].get(\"session_date\")\n",
        "    } for hit in res[\"hits\"][\"hits\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYd8Yr3ZYmlV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYd8Yr3ZYmlV",
        "outputId": "d7b3ec48-16ea-42fc-8aab-c985cc4627ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing 55 term-year pairs from (17, 1) to (27, 5)\n"
          ]
        }
      ],
      "source": [
        "def discover_term_year_combinations(es, index_name):\n",
        "    \"\"\"Discover all available term-year combinations from Elasticsearch.\"\"\"\n",
        "    query = {\n",
        "        \"size\": 0,\n",
        "        \"aggs\": {\n",
        "            \"by_term\": {\n",
        "                \"terms\": {\n",
        "                    \"field\": \"term\",\n",
        "                    \"size\": 100,\n",
        "                    \"order\": {\"_key\": \"asc\"}\n",
        "                },\n",
        "                \"aggs\": {\n",
        "                    \"by_year\": {\n",
        "                        \"terms\": {\n",
        "                            \"field\": \"year\",\n",
        "                            \"size\": 10,\n",
        "                            \"order\": {\"_key\": \"asc\"}\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    try:\n",
        "        res = es.search(index=index_name, body=query)\n",
        "        term_year_tuples = []\n",
        "        \n",
        "        for term_bucket in res[\"aggregations\"][\"by_term\"][\"buckets\"]:\n",
        "            term = term_bucket[\"key\"]\n",
        "            for year_bucket in term_bucket[\"by_year\"][\"buckets\"]:\n",
        "                year = year_bucket[\"key\"]\n",
        "                term_year_tuples.append((term, year))\n",
        "        \n",
        "        # Sort by term, then year\n",
        "        term_year_tuples.sort(key=lambda x: (x[0], x[1]))\n",
        "        \n",
        "        return term_year_tuples\n",
        "    except Exception as e:\n",
        "        print(f\"Error discovering term-year combinations: {e}\")\n",
        "        return []\n",
        "\n",
        "TERM_YEAR_TUPLES = discover_term_year_combinations(es, INDEX_NAME)\n",
        "if TERM_YEAR_TUPLES:\n",
        "    print(f\"Discovered {len(TERM_YEAR_TUPLES)} term-year pairs from Elasticsearch\")\n",
        "    print(f\"  Range: {TERM_YEAR_TUPLES[0]} to {TERM_YEAR_TUPLES[-1]}\")\n",
        "    print(f\"  Terms: {sorted(set(t for t, y in TERM_YEAR_TUPLES))}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No term-year combinations found in Elasticsearch!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "342d2e07",
      "metadata": {
        "id": "342d2e07",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def extract_contexts(speeches, target_word, window=10):\n",
        "    \"\"\"Extract short context windows around target word with metadata.\"\"\"\n",
        "    contexts = []\n",
        "    # Create regex pattern to match target word and any Turkish suffixes\n",
        "    pattern = re.compile(rf\"\\b{re.escape(target_word.lower())}\\w*\\b\")\n",
        "\n",
        "    for speech in speeches:\n",
        "        text = speech[\"content\"]\n",
        "        term = speech.get(\"term\")\n",
        "        year = speech.get(\"year\")\n",
        "        file = speech.get(\"file\")\n",
        "        session_date = speech.get(\"session_date\")\n",
        "\n",
        "        tokens = re.findall(r\"\\w+\", text.lower()) # simple tokenization, one or more word characters\n",
        "        for i, tok in enumerate(tokens):\n",
        "            # Use regex to match the word and its variations\n",
        "            if pattern.match(tok):\n",
        "                start = max(0, i - window)\n",
        "                end = min(len(tokens), i + window + 1)\n",
        "                snippet = \" \".join(tokens[start:end])\n",
        "                contexts.append({\n",
        "                    \"context\": snippet,\n",
        "                    \"term\": term,\n",
        "                    \"year\": year,\n",
        "                    \"file\": file,\n",
        "                    \"session_date\": session_date\n",
        "                })\n",
        "    return contexts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "a3aa11a8",
      "metadata": {
        "id": "a3aa11a8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def compute_embeddings(model, contexts):\n",
        "    \"\"\"Compute embeddings for context snippets.\"\"\"\n",
        "    if len(contexts) == 0:\n",
        "        return np.empty((0, model.get_sentence_embedding_dimension()))\n",
        "    return model.encode(contexts, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3cc71813",
      "metadata": {
        "id": "3cc71813",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def get_cluster_prototypes(X, labels, return_label_ids=False):\n",
        "    \"\"\"Compute centroids for each cluster and optionally return their IDs.\"\"\"\n",
        "    clusters = []\n",
        "    label_ids = []\n",
        "    for label in np.unique(labels):\n",
        "        if label == -1:\n",
        "            continue\n",
        "        members = X[labels == label]\n",
        "        if len(members) == 0:\n",
        "            continue\n",
        "        centroid = np.mean(members, axis=0)\n",
        "        clusters.append(centroid)\n",
        "        label_ids.append(label)\n",
        "    clusters = np.array(clusters)\n",
        "    if return_label_ids:\n",
        "        return clusters, label_ids\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "75b2e5a8",
      "metadata": {
        "id": "75b2e5a8",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def show_top_contexts(contexts, labels, n=3):\n",
        "    \"\"\"Print representative contexts for each cluster.\"\"\"\n",
        "    df = pd.DataFrame({\"cluster\": labels, \"context\": contexts})\n",
        "    grouped = df.groupby(\"cluster\")[\"context\"].apply(list)\n",
        "    for cluster, examples in grouped.items():\n",
        "        cluster_name = \"overflow/filtered\" if cluster == -1 else cluster\n",
        "        print(f\"\\nüåÄ Cluster {cluster_name} ({len(examples)} examples):\")\n",
        "        for ex in examples[:n]:\n",
        "            print(\"   ‚Ä¢\", ex[:200].replace(\"\\n\", \" \") + (\"...\" if len(ex) > 200 else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "722c3d66",
      "metadata": {
        "id": "722c3d66"
      },
      "outputs": [],
      "source": [
        "def limit_clusters(labels, max_clusters):\n",
        "    \"\"\"Keep only the largest max_clusters and map the rest to -1.\"\"\"\n",
        "    if max_clusters is None:\n",
        "        return labels, np.unique(labels).tolist()\n",
        "    labels = np.asarray(labels)\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_counts = [\n",
        "        (label, count) for label, count in zip(unique, counts) if label != -1\n",
        "    ]\n",
        "    cluster_counts.sort(key=lambda item: item[1], reverse=True)\n",
        "    keep = [label for label, _ in cluster_counts[:max_clusters]]\n",
        "    if not keep:\n",
        "        return np.full_like(labels, -1), []\n",
        "    filtered = np.array([label if label in keep else -1 for label in labels], dtype=labels.dtype)\n",
        "    return filtered, keep\n",
        "\n",
        "\n",
        "class ClusterAligner:\n",
        "    \"\"\"Keeps global cluster IDs and assigns consistent colors over time.\"\"\"\n",
        "\n",
        "    def __init__(self, max_clusters=100, similarity_threshold=0.8, cmap_name=\"gist_ncar\"):\n",
        "        self.max_clusters = max_clusters\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.centroids = []\n",
        "        self.global_ids = []\n",
        "        self.cmap = plt.cm.get_cmap(cmap_name, max_clusters)\n",
        "        self.palette = [self.cmap(i) for i in range(self.cmap.N)]\n",
        "        self.overflow_color = (0.65, 0.65, 0.65, 1.0)\n",
        "\n",
        "    def _add_centroid(self, centroid):\n",
        "        if len(self.global_ids) >= self.max_clusters:\n",
        "            return -1\n",
        "        new_id = len(self.global_ids)\n",
        "        self.centroids.append(centroid)\n",
        "        self.global_ids.append(new_id)\n",
        "        return new_id\n",
        "\n",
        "    def _match_or_create(self, centroid):\n",
        "        centroid = centroid.reshape(1, -1)\n",
        "        if not self.centroids:\n",
        "            return self._add_centroid(centroid)\n",
        "        stacked = np.vstack(self.centroids)\n",
        "        sims = cosine_similarity(stacked, centroid)[:, 0]\n",
        "        best_idx = int(np.argmax(sims))\n",
        "        if sims[best_idx] >= self.similarity_threshold:\n",
        "            return self.global_ids[best_idx]\n",
        "        return self._add_centroid(centroid)\n",
        "\n",
        "    def align(self, raw_labels, centroid_map):\n",
        "        aligned = np.full_like(raw_labels, -1)\n",
        "        for local_label, centroid in centroid_map.items():\n",
        "            global_id = self._match_or_create(centroid)\n",
        "            if global_id == -1:\n",
        "                continue\n",
        "            aligned[raw_labels == local_label] = global_id\n",
        "        return aligned\n",
        "\n",
        "    def colors_for(self, labels):\n",
        "        return [\n",
        "            self.palette[label]\n",
        "            if 0 <= label < len(self.palette)\n",
        "            else self.overflow_color\n",
        "            for label in labels\n",
        "        ]\n",
        "\n",
        "    def get_color(self, label):\n",
        "        \"\"\"Get color for a single label.\"\"\"\n",
        "        if 0 <= label < len(self.palette):\n",
        "            return self.palette[label]\n",
        "        return self.overflow_color\n",
        "\n",
        "\n",
        "def plot_tsne_with_coords(term, year, word, tsne_coords, aligned_labels, aligner):\n",
        "    if len(tsne_coords) < 2:\n",
        "        print(\"  Skipping t-SNE (insufficient embeddings).\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Plot by cluster using numeric markers (memory efficient)\n",
        "    unique_labels = sorted(set(label for label in aligned_labels if label >= 0))\n",
        "\n",
        "    for cluster_id in unique_labels:\n",
        "        mask = aligned_labels == cluster_id\n",
        "        cluster_coords = tsne_coords[mask]\n",
        "        color = aligner.get_color(cluster_id)\n",
        "\n",
        "        # Use cluster number as marker in scatter plot\n",
        "        plt.scatter(cluster_coords[:, 0], cluster_coords[:, 1],\n",
        "                   c=[color], marker=f'${cluster_id}$', s=200,\n",
        "                   alpha=0.8, edgecolors='black', linewidths=0.5)\n",
        "\n",
        "    # Plot filtered points if any\n",
        "    if -1 in aligned_labels:\n",
        "        mask = aligned_labels == -1\n",
        "        overflow_coords = tsne_coords[mask]\n",
        "        plt.scatter(overflow_coords[:, 0], overflow_coords[:, 1],\n",
        "                   c=[aligner.overflow_color], marker='x', s=30,\n",
        "                   alpha=0.3, linewidths=0.5)\n",
        "\n",
        "    plt.title(f\"'{word}' Term {term} Year {year} (t-SNE - Unified Projection)\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Dim 1\", fontsize=12)\n",
        "    plt.ylabel(\"Dim 2\", fontsize=12)\n",
        "    plt.grid(alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    tsne_path = os.path.join(OUTPUT_DIR, f\"tsne_term{term}_year{year}_{word}.png\")\n",
        "    plt.savefig(tsne_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"  Saved t-SNE plot to {tsne_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "893c2c09",
      "metadata": {
        "id": "893c2c09"
      },
      "outputs": [],
      "source": [
        "def create_cluster_guide(cluster_contexts_map, target_word, output_dir, aligner):\n",
        "    \"\"\"\n",
        "    Create a cluster guide with summary CSV and detailed context file.\n",
        "    Shows what each global cluster represents semantically.\n",
        "    \"\"\"\n",
        "\n",
        "    if not cluster_contexts_map:\n",
        "        print(\"  No clusters to document.\")\n",
        "        return\n",
        "\n",
        "    # Calculate statistics for each cluster\n",
        "    guide_rows = []\n",
        "    for global_id in sorted(cluster_contexts_map.keys()):\n",
        "        contexts = cluster_contexts_map[global_id]\n",
        "        term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "\n",
        "        guide_rows.append({\n",
        "            'global_id': global_id,\n",
        "            'color_index': global_id,\n",
        "            'total_contexts': len(contexts),\n",
        "            'term_year_span': ', '.join(term_years),\n",
        "            'num_appearances': len(term_years)\n",
        "        })\n",
        "\n",
        "    # Create summary CSV\n",
        "    df_summary = pd.DataFrame(guide_rows).sort_values('total_contexts', ascending=False)\n",
        "    summary_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_summary.csv\")\n",
        "    df_summary.to_csv(summary_path, index=False)\n",
        "    print(f\"  Saved cluster summary to {summary_path}\")\n",
        "\n",
        "    # Create detailed context file\n",
        "    context_file_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_contexts.txt\")\n",
        "    with open(context_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"{'='*80}\\n\")\n",
        "        f.write(f\"CLUSTER GUIDE FOR '{target_word.upper()}'\\n\")\n",
        "        f.write(f\"Generated: {pd.Timestamp.now()}\\n\")\n",
        "        f.write(f\"Total clusters: {len(cluster_contexts_map)}\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        # Sort clusters by total contexts (most common first)\n",
        "        for global_id in sorted(cluster_contexts_map.keys(),\n",
        "                               key=lambda x: len(cluster_contexts_map[x]),\n",
        "                               reverse=True):\n",
        "            contexts = cluster_contexts_map[global_id]\n",
        "            term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "\n",
        "            # Get color info\n",
        "            color = aligner.get_color(global_id)\n",
        "            color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "                int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "            )\n",
        "\n",
        "            f.write(f\"\\n{'='*80}\\n\")\n",
        "            f.write(f\"CLUSTER {global_id} (Color: {color_hex})\\n\")\n",
        "            f.write(f\"{'-'*80}\\n\")\n",
        "            f.write(f\"Total contexts: {len(contexts)}\\n\")\n",
        "            f.write(f\"Appearances: {len(term_years)} term-years\\n\")\n",
        "            f.write(f\"Term-year span: {', '.join(term_years)}\\n\")\n",
        "            f.write(f\"\\nREPRESENTATIVE CONTEXTS:\\n\")\n",
        "            f.write(f\"{'-'*80}\\n\")\n",
        "\n",
        "            # Show up to 15 diverse examples\n",
        "            shown = 0\n",
        "            for ctx_item in contexts[:15]:\n",
        "                f.write(f\"\\n[{ctx_item['term']}-{ctx_item['year']}] \")\n",
        "                f.write(ctx_item['context'][:250])\n",
        "                if len(ctx_item['context']) > 250:\n",
        "                    f.write(\"...\")\n",
        "                f.write(\"\\n\")\n",
        "                shown += 1\n",
        "\n",
        "            if len(contexts) > 15:\n",
        "                f.write(f\"\\n... and {len(contexts) - 15} more contexts\\n\")\n",
        "\n",
        "    print(f\"  Saved detailed contexts to {context_file_path}\")\n",
        "    print(f\"  Total clusters documented: {len(cluster_contexts_map)}\")\n",
        "\n",
        "    return df_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "be915a99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be915a99",
        "outputId": "1e374725-a183-4d41-a442-65acaf74fe59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded \n"
          ]
        }
      ],
      "source": [
        "# ## Load Sentence Transformer Model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Model loaded \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "2f8a3697",
      "metadata": {
        "id": "2f8a3697"
      },
      "outputs": [],
      "source": [
        "# Create Color Reference and Mapping\n",
        "def create_color_reference(cluster_contexts_map, target_word, output_dir, aligner):\n",
        "    \"\"\"Create color reference chart and mapping CSV.\"\"\"\n",
        "\n",
        "    if not cluster_contexts_map:\n",
        "        print(\"  No clusters to map.\")\n",
        "        return\n",
        "\n",
        "    # Get all global IDs\n",
        "    used_cluster_ids = sorted(cluster_contexts_map.keys())\n",
        "\n",
        "    # Create color mapping CSV\n",
        "    color_mapping = []\n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "            int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "        )\n",
        "        color_rgb = f\"({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)})\"\n",
        "\n",
        "        color_mapping.append({\n",
        "            'global_id': global_id,\n",
        "            'hex_color': color_hex,\n",
        "            'rgb_color': color_rgb\n",
        "        })\n",
        "\n",
        "    df_colors = pd.DataFrame(color_mapping)\n",
        "    color_csv_path = os.path.join(output_dir, f'cluster_colors_{target_word}.csv')\n",
        "    df_colors.to_csv(color_csv_path, index=False)\n",
        "    print(f\"  Saved color mapping to {color_csv_path}\")\n",
        "\n",
        "    # Print console reference\n",
        "    print(\"\\n=== Color Reference ===\")\n",
        "    for _, row in df_colors.iterrows():\n",
        "        print(f\"Cluster {row['global_id']}: {row['hex_color']}\")\n",
        "\n",
        "    # Create visual color reference chart\n",
        "    import matplotlib.patches as mpatches\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, max(6, len(used_cluster_ids) // 4)))\n",
        "    patches = []\n",
        "\n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        patches.append(mpatches.Patch(color=color, label=f'Cluster {global_id}'))\n",
        "\n",
        "    ax.legend(handles=patches, loc='center', ncol=min(4, len(patches)), fontsize=10)\n",
        "    ax.axis('off')\n",
        "    plt.title(f\"Color Reference for '{target_word}' Clusters\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    chart_path = os.path.join(output_dir, f'color_reference_{target_word}.png')\n",
        "    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"  Saved color reference chart to {chart_path}\")\n",
        "\n",
        "    return df_colors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bb2a0a2",
      "metadata": {
        "id": "6bb2a0a2"
      },
      "source": [
        "## 6Ô∏è‚É£ Main Loop Over Words\n",
        "\n",
        "This loop processes each target word and generates:\n",
        "- **Individual CSVs**: One per term-year in `lorenz_results/` (includes session_date, file)\n",
        "- **t-SNE plots**: PNGs saved to `lorenz_results/` (for visualization)\n",
        "- **Consolidated t-SNE CSV**: Single CSV per word in `widid_results/{word}/tsne_{word}.csv` (for coordinate API endpoint)\n",
        "- **Cluster guides**: Summary and context files in `lorenz_results/`\n",
        "- **Color reference**: Color mapping for clusters\n",
        "\n",
        "The consolidated CSV contains all t-SNE data points with columns:\n",
        "- `target_word`, `term`, `year`, `tsne_x`, `tsne_y`, `cluster_id`, `context`, `session_date`, `file`\n",
        "\n",
        "Each row represents a single point in the t-SNE visualization with its context and session date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f87033",
      "metadata": {
        "id": "e1f87033"
      },
      "outputs": [],
      "source": [
        "for target_word in TARGET_WORDS:\n",
        "    print(f\"\\n\\n===  Analyzing '{target_word}' across {len(TERM_YEAR_TUPLES)} term-year pairs ===\")\n",
        "    aligner = ClusterAligner(max_clusters=MAX_CLUSTERS, similarity_threshold=SIMILARITY_THRESHOLD)\n",
        "    baseline_used = False\n",
        "    cluster_contexts_map = {}  # Store contexts per global_id for cluster guide\n",
        "    term_year_data = []  # Store embeddings and labels for unified t-SNE\n",
        "\n",
        "    for term, year in TERM_YEAR_TUPLES:\n",
        "        print(f\"\\n--- Term {term}, Year {year} ---\")\n",
        "        speeches = fetch_speeches(term, year)\n",
        "        \n",
        "        # Extract actual term/year from Elasticsearch data (use most common if multiple)\n",
        "        if speeches:\n",
        "            actual_terms = [s.get(\"term\") for s in speeches if s.get(\"term\") is not None]\n",
        "            actual_years = [s.get(\"year\") for s in speeches if s.get(\"year\") is not None]\n",
        "            if actual_terms:\n",
        "                actual_term = max(set(actual_terms), key=actual_terms.count)\n",
        "            else:\n",
        "                actual_term = term\n",
        "            if actual_years:\n",
        "                actual_year = max(set(actual_years), key=actual_years.count)\n",
        "            else:\n",
        "                actual_year = year\n",
        "            print(f\"  Actual data: Term {actual_term}, Year {actual_year}\")\n",
        "        else:\n",
        "            actual_term = term\n",
        "            actual_year = year\n",
        "        \n",
        "        contexts = extract_contexts(speeches, target_word)\n",
        "        print(f\"  Contexts: {len(contexts)}\")\n",
        "        if len(contexts) < 10:\n",
        "            print(\"  Not enough contexts, skipping this slice.\")\n",
        "            continue\n",
        "\n",
        "        embeddings = compute_embeddings(model, [c[\"context\"] for c in contexts])\n",
        "        ap = AffinityPropagation(random_state=42)\n",
        "        ap.fit(embeddings)\n",
        "        local_labels = ap.labels_\n",
        "\n",
        "        cap = BASELINE_MAX_CLUSTERS if not baseline_used else MAX_CLUSTERS\n",
        "        limited_labels, kept_clusters = limit_clusters(local_labels, cap)\n",
        "        print(f\"  Raw clusters: {len(np.unique(local_labels))}, kept: {len(kept_clusters)} (cap={cap})\")\n",
        "\n",
        "        prototypes, proto_labels = get_cluster_prototypes(embeddings, limited_labels, return_label_ids=True)\n",
        "        centroid_map = dict(zip(proto_labels, prototypes))\n",
        "        if not centroid_map:\n",
        "            print(\"  No clusters survived filtering, skipping visualization.\")\n",
        "            continue\n",
        "\n",
        "        baseline_used = True\n",
        "        aligned_labels = aligner.align(limited_labels, centroid_map)\n",
        "        global_cluster_count = len(set(label for label in aligned_labels if label >= 0))\n",
        "        print(f\"  Global clusters represented: {global_cluster_count}\")\n",
        "\n",
        "        # Store context examples for cluster guide (up to 10 per global cluster per term-year)\n",
        "        # Use actual term/year from contexts (which come from Elasticsearch)\n",
        "        for global_id in set(label for label in aligned_labels if label >= 0):\n",
        "            cluster_context_examples = [\n",
        "                contexts[i] for i, label in enumerate(aligned_labels) if label == global_id\n",
        "            ]\n",
        "            if global_id not in cluster_contexts_map:\n",
        "                cluster_contexts_map[global_id] = []\n",
        "            cluster_contexts_map[global_id].extend([\n",
        "                {'term': ctx.get(\"term\", actual_term), 'year': ctx.get(\"year\", actual_year), 'context': ctx[\"context\"]}\n",
        "                for ctx in cluster_context_examples[:10]\n",
        "            ])\n",
        "\n",
        "        print(\"\\n=== Representative Contexts ===\")\n",
        "        show_top_contexts([c[\"context\"] for c in contexts], aligned_labels)\n",
        "\n",
        "        # Use actual term/year from Elasticsearch data\n",
        "        df = pd.DataFrame(\n",
        "            {\n",
        "                \"term\": actual_term,\n",
        "                \"year\": actual_year,\n",
        "                \"context\": [c[\"context\"] for c in contexts],\n",
        "                \"session_date\": [c.get(\"session_date\") for c in contexts],\n",
        "                \"file\": [c.get(\"file\") for c in contexts],\n",
        "                \"local_cluster\": limited_labels,\n",
        "                \"global_cluster\": aligned_labels,\n",
        "            }\n",
        "        )\n",
        "        csv_path = os.path.join(OUTPUT_DIR, f\"widid_term{actual_term}_year{actual_year}_{target_word}.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"  Saved clusters to {csv_path}\")\n",
        "\n",
        "        # Store data for unified t-SNE (computed later) - use actual term/year from Elasticsearch\n",
        "        term_year_data.append({\n",
        "            \"term\": actual_term,\n",
        "            \"year\": actual_year,\n",
        "            \"embeddings\": embeddings,\n",
        "            \"aligned_labels\": aligned_labels,\n",
        "            \"contexts\": contexts\n",
        "        })\n",
        "\n",
        "\n",
        "    # Compute unified t-SNE across all term-years\n",
        "    if term_year_data:\n",
        "        print(\"\\n=== Computing Unified t-SNE ===\")\n",
        "        all_embeddings = np.vstack([item[\"embeddings\"] for item in term_year_data])\n",
        "        print(f\"  Total embeddings: {len(all_embeddings)}\")\n",
        "\n",
        "        # Compute optimal perplexity for combined data\n",
        "        desired = max(5, len(all_embeddings) // 3)\n",
        "        max_valid = max(1, len(all_embeddings) - 1)\n",
        "        perplexity = min(desired, max_valid, 30)\n",
        "\n",
        "        tsne_unified = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(all_embeddings)\n",
        "        print(f\"  t-SNE completed with perplexity={perplexity}\")\n",
        "\n",
        "        # Prepare consolidated data for API\n",
        "        consolidated_data = []\n",
        "\n",
        "        # Plot each term-year with its slice of unified coordinates\n",
        "        start_idx = 0\n",
        "        for item in term_year_data:\n",
        "            end_idx = start_idx + len(item[\"embeddings\"])\n",
        "            tsne_coords = tsne_unified[start_idx:end_idx]\n",
        "            #plot_tsne_with_coords(item[\"term\"], item[\"year\"], target_word,\n",
        "                                #tsne_coords, item[\"aligned_labels\"], aligner)\n",
        "\n",
        "            # Collect data for consolidated CSV\n",
        "            for i, (coord, label) in enumerate(zip(tsne_coords, item[\"aligned_labels\"])):\n",
        "                # Get the original context data from the stored data\n",
        "                context_idx = i\n",
        "                if context_idx < len(item.get(\"contexts\", [])):\n",
        "                    context_data = item[\"contexts\"][context_idx]\n",
        "                    context_text = context_data.get(\"context\", \"\")\n",
        "                    # Use term/year from context (Elasticsearch data), fallback to item values\n",
        "                    context_term = context_data.get(\"term\", item[\"term\"])\n",
        "                    context_year = context_data.get(\"year\", item[\"year\"])\n",
        "                    session_date = context_data.get(\"session_date\")\n",
        "                    file = context_data.get(\"file\")\n",
        "                else:\n",
        "                    context_text = \"\"\n",
        "                    context_term = item[\"term\"]\n",
        "                    context_year = item[\"year\"]\n",
        "                    session_date = None\n",
        "                    file = None\n",
        "\n",
        "                consolidated_data.append({\n",
        "                    \"target_word\": target_word,\n",
        "                    \"term\": context_term,\n",
        "                    \"year\": context_year,\n",
        "                    \"tsne_x\": float(coord[0]),\n",
        "                    \"tsne_y\": float(coord[1]),\n",
        "                    \"cluster_id\": int(label),\n",
        "                    \"context\": context_text,\n",
        "                    \"session_date\": session_date,\n",
        "                    \"file\": file\n",
        "                })\n",
        "\n",
        "            start_idx = end_idx\n",
        "\n",
        "        # Save consolidated t-SNE data for API\n",
        "        if consolidated_data:\n",
        "            df_consolidated = pd.DataFrame(consolidated_data)\n",
        "\n",
        "            # Save in word-specific subdirectory for API compatibility\n",
        "            word_dir = os.path.join(WIDID_RESULTS_DIR, target_word)\n",
        "            os.makedirs(word_dir, exist_ok=True)\n",
        "            consolidated_csv_path = os.path.join(word_dir, f\"tsne_{target_word}.csv\")\n",
        "            df_consolidated.to_csv(consolidated_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "            print(f\"\\n=== Saved consolidated t-SNE data ===\")\n",
        "            print(f\"  File: {consolidated_csv_path}\")\n",
        "            print(f\"  Total data points: {len(consolidated_data)}\")\n",
        "            print(f\"  Columns: {', '.join(df_consolidated.columns.tolist())}\")\n",
        "            print(f\"  This file is ready for the interactive t-SNE API!\")\n",
        "    # Generate cluster guide after processing all term-years for this word\n",
        "    print(\"\\n=== Generating Cluster Guide ===\")\n",
        "    create_cluster_guide(cluster_contexts_map, target_word, OUTPUT_DIR, aligner)\n",
        "\n",
        "    # Generate color reference\n",
        "    print(\"\\n=== Generating Color Reference ===\")\n",
        "    create_color_reference(cluster_contexts_map, target_word, OUTPUT_DIR, aligner)\n",
        "    print(\"-----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b4c273",
      "metadata": {
        "id": "b6b4c273"
      },
      "outputs": [],
      "source": [
        "!zip -r lorenz_results.zip lorenz_results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6XU61t2675eY",
      "metadata": {
        "id": "6XU61t2675eY"
      },
      "outputs": [],
      "source": [
        "!zip -r widid_results.zip widid_results/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R8oRD0K3qCS_",
      "metadata": {
        "id": "R8oRD0K3qCS_"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
