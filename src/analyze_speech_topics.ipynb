{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parliament Speech Topic Analysis with BERTopic and Groq LLM\n",
    "\n",
    "This notebook performs topic modeling on Turkish parliament speeches using:\n",
    "1. **BERTopic** - For automatic topic discovery\n",
    "2. **Groq LLM** - For generating human-readable topic names in Turkish\n",
    "3. **Elasticsearch** - For storing and updating speech documents\n",
    "\n",
    "## Workflow:\n",
    "1. Connect to Elasticsearch\n",
    "2. Fetch all speeches\n",
    "3. Train BERTopic model\n",
    "4. Update Elasticsearch with topic assignments\n",
    "5. Generate readable topic names with Groq LLM\n",
    "6. Export results to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation & Setup\n",
    "\n",
    "Install required packages (run this first in Google Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q elasticsearch bertopic groq pandas plotly python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.exceptions import ConnectionError, NotFoundError\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from groq import Groq\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set your environment variables here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearch Configuration\n",
    "ELASTICSEARCH_HOST = \"http://localhost:9200\"  # Change this to your ES host\n",
    "ELASTICSEARCH_INDEX = \"parliament_speeches\"\n",
    "\n",
    "# File paths\n",
    "MODEL_SAVE_PATH = \"./bertopic_model\"\n",
    "TOPIC_SUMMARY_FILE = \"./topic_summary.csv\"\n",
    "TOPIC_DETAILS_FILE = \"./topic_details.csv\"\n",
    "\n",
    "# Processing configuration\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "# LLM Configuration\n",
    "GROQ_API_KEY = \"\"  # Set your Groq API key here\n",
    "GROQ_MODEL = \"llama-3.1-70b-versatile\"  # or your preferred model\n",
    "USE_LLM_NAMING = True  # Set to False to skip LLM naming\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"ELASTICSEARCH_HOST\"] = ELASTICSEARCH_HOST\n",
    "os.environ[\"ELASTICSEARCH_INDEX\"] = ELASTICSEARCH_INDEX\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"GROQ_MODEL\"] = GROQ_MODEL\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")\n",
    "print(f\"   Elasticsearch: {ELASTICSEARCH_HOST}\")\n",
    "print(f\"   Index: {ELASTICSEARCH_INDEX}\")\n",
    "print(f\"   LLM Naming: {'Enabled' if USE_LLM_NAMING and GROQ_API_KEY else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions\n",
    "\n",
    "### 4.1 Elasticsearch Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_elasticsearch() -> Elasticsearch:\n",
    "    \"\"\"\n",
    "    Connect to Elasticsearch and verify connection.\n",
    "    \n",
    "    Returns:\n",
    "        Elasticsearch client instance\n",
    "    \"\"\"\n",
    "    print(f\"üîå Connecting to Elasticsearch at {ELASTICSEARCH_HOST}...\")\n",
    "    \n",
    "    try:\n",
    "        es = Elasticsearch(hosts=[ELASTICSEARCH_HOST])\n",
    "        \n",
    "        if es.ping():\n",
    "            count = es.count(index=ELASTICSEARCH_INDEX)\n",
    "            total_docs = count.get('count', 0)\n",
    "            print(f\"‚úÖ Connected to Elasticsearch\")\n",
    "            print(f\"üìä Index: {ELASTICSEARCH_INDEX}\")\n",
    "            print(f\"üìä Total documents: {total_docs:,}\")\n",
    "            return es\n",
    "        else:\n",
    "            raise Exception(\"Ping failed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to connect to Elasticsearch: {e}\")\n",
    "        print(f\"   Make sure Elasticsearch is running on {ELASTICSEARCH_HOST}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Fetch Speeches from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_speeches(es: Elasticsearch) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch all speeches from Elasticsearch using scroll API for efficient retrieval.\n",
    "    \n",
    "    Args:\n",
    "        es: Elasticsearch client instance\n",
    "        \n",
    "    Returns:\n",
    "        List of speech dictionaries with id, content, and metadata\n",
    "    \"\"\"\n",
    "    print(f\"\\nüì• Fetching speeches from Elasticsearch...\")\n",
    "    \n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"exists\": {\"field\": \"content\"}},\n",
    "                ],\n",
    "                \"must_not\": [\n",
    "                    {\"term\": {\"content\": \"\"}},\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"size\": BATCH_SIZE,\n",
    "        \"_source\": [\n",
    "            \"content\", \"speech_giver\", \"term\", \"year\", \n",
    "            \"session_date\", \"session_id\", \"speech_no\",\n",
    "            \"province\", \"political_party\", \"speech_title\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    speeches = []\n",
    "    scroll_id = None\n",
    "    batch_count = 0\n",
    "    \n",
    "    try:\n",
    "        response = es.search(\n",
    "            index=ELASTICSEARCH_INDEX,\n",
    "            body=query,\n",
    "            scroll='5m'\n",
    "        )\n",
    "        \n",
    "        scroll_id = response['_scroll_id']\n",
    "        hits = response['hits']['hits']\n",
    "        \n",
    "        while hits:\n",
    "            batch_count += 1\n",
    "            print(f\"Batch {batch_count}: Processing {len(hits)} speeches...\")\n",
    "            \n",
    "            for hit in hits:\n",
    "                source = hit['_source']\n",
    "                \n",
    "                if source.get('content') and source['content'].strip():\n",
    "                    speeches.append({\n",
    "                        'id': hit['_id'],\n",
    "                        'content': source['content'],\n",
    "                        'speech_giver': source.get('speech_giver', ''),\n",
    "                        'term': source.get('term'),\n",
    "                        'year': source.get('year'),\n",
    "                        'session_date': source.get('session_date'),\n",
    "                        'session_id': source.get('session_id'),\n",
    "                        'speech_no': source.get('speech_no'),\n",
    "                        'province': source.get('province'),\n",
    "                        'political_party': source.get('political_party'),\n",
    "                        'speech_title': source.get('speech_title')\n",
    "                    })\n",
    "            \n",
    "            response = es.scroll(scroll_id=scroll_id, scroll='5m')\n",
    "            scroll_id = response['_scroll_id']\n",
    "            hits = response['hits']['hits']\n",
    "        \n",
    "        print(f\"‚úÖ Successfully fetched {len(speeches):,} speeches with valid content\")\n",
    "        return speeches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error fetching speeches: {e}\")\n",
    "        return []\n",
    "        \n",
    "    finally:\n",
    "        if scroll_id:\n",
    "            try:\n",
    "                es.clear_scroll(scroll_id=scroll_id)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Run BERTopic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(speeches: List[Dict]) -> Tuple[List[int], BERTopic]:\n",
    "    \"\"\"\n",
    "    Run BERTopic modeling on speech content.\n",
    "    \n",
    "    Args:\n",
    "        speeches: List of speech dictionaries containing 'content' field\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (topics, topic_model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n‚öôÔ∏è  Training BERTopic model on {len(speeches):,} speeches...\")\n",
    "    print(\"   This may take several minutes depending on your hardware...\")\n",
    "    \n",
    "    contents = [speech['content'] for speech in speeches]\n",
    "    \n",
    "    topic_model = BERTopic(\n",
    "        language=\"turkish\",\n",
    "        nr_topics=250,\n",
    "        verbose=True,\n",
    "        calculate_probabilities=False,\n",
    "        min_topic_size=3,\n",
    "    )\n",
    "    \n",
    "    topics, _ = topic_model.fit_transform(contents)\n",
    "    \n",
    "    topic_model.save(MODEL_SAVE_PATH)\n",
    "    print(f\"‚úÖ Model trained and saved to {MODEL_SAVE_PATH}\")\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    num_topics = len(topic_info[topic_info['Topic'] != -1])\n",
    "    outlier_count = (topics == -1).sum()\n",
    "    print(f\"üìä Discovered {num_topics} topics (excluding outliers)\")\n",
    "    print(f\"üìä Outliers: {outlier_count} speeches\")\n",
    "    \n",
    "    return topics, topic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Update Elasticsearch with Topic Assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elasticsearch_with_topics(\n",
    "    es: Elasticsearch, \n",
    "    speeches: List[Dict], \n",
    "    topics: List[int],\n",
    "    topic_model: BERTopic\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Bulk update Elasticsearch documents with topic assignments.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success_count, failure_count)\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ Updating Elasticsearch with topic assignments...\")\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_labels = {\n",
    "        int(row['Topic']): row['Name'] \n",
    "        for _, row in topic_info.iterrows()\n",
    "    }\n",
    "    \n",
    "    actions = []\n",
    "    for speech, topic_id in zip(speeches, topics):\n",
    "        topic_label = topic_labels.get(topic_id, f\"Topic_{topic_id}\")\n",
    "        \n",
    "        actions.append({\n",
    "            '_op_type': 'update',\n",
    "            '_index': ELASTICSEARCH_INDEX,\n",
    "            '_id': speech['id'],\n",
    "            'doc': {\n",
    "                'topic_id': int(topic_id),\n",
    "                'topic_label': topic_label,\n",
    "                'topic_analyzed': True\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    print(f\"   Updating {len(actions):,} documents...\")\n",
    "    \n",
    "    try:\n",
    "        success, errors = helpers.bulk(\n",
    "            es, \n",
    "            actions, \n",
    "            raise_on_error=False,\n",
    "            chunk_size=500\n",
    "        )\n",
    "        \n",
    "        failed_count = len(errors) if errors else 0\n",
    "        \n",
    "        print(f\"‚úÖ Successfully updated {success:,} documents\")\n",
    "        if failed_count > 0:\n",
    "            print(f\"‚ö†Ô∏è  Failed to update {failed_count} documents\")\n",
    "            \n",
    "        return success, failed_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during bulk update: {e}\")\n",
    "        return 0, len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Save Topic Details for LLM Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topic_details(topic_model: BERTopic, output_file: str = TOPIC_DETAILS_FILE):\n",
    "    \"\"\"\n",
    "    Save detailed topic information to CSV including keywords and representative docs.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Saving topic details for LLM processing...\")\n",
    "    \n",
    "    try:\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        \n",
    "        detailed_keywords = []\n",
    "        representative_docs_list = []\n",
    "        \n",
    "        for topic_id in topic_info['Topic']:\n",
    "            if topic_id == -1:\n",
    "                detailed_keywords.append(\"Outliers\")\n",
    "                representative_docs_list.append(\"[]\")\n",
    "            else:\n",
    "                words = topic_model.get_topic(topic_id)\n",
    "                keywords = ', '.join([word for word, _ in words[:10]])\n",
    "                detailed_keywords.append(keywords)\n",
    "                \n",
    "                try:\n",
    "                    rep_docs = topic_model.get_representative_docs(topic_id)\n",
    "                    representative_docs_list.append(str(rep_docs))\n",
    "                except:\n",
    "                    representative_docs_list.append(\"[]\")\n",
    "        \n",
    "        topic_info['Keywords'] = detailed_keywords\n",
    "        topic_info['Representative_Docs'] = representative_docs_list\n",
    "        \n",
    "        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "        \n",
    "        topic_info.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Topic details saved to {output_file}\")\n",
    "        print(f\"   Total topics: {len(topic_info)} (including outliers)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error saving topic details: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Groq LLM Topic Namer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqTopicNamer:\n",
    "    \"\"\"Service for generating readable topic names using Groq LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str, model: str = \"llama-3.1-70b-versatile\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "    \n",
    "    def _build_prompt(self, keywords: str, representative_docs: List[str]) -> str:\n",
    "        docs_text = \"\"\n",
    "        for i, doc in enumerate(representative_docs[:3], 1):\n",
    "            truncated = doc[:200] if len(doc) > 200 else doc\n",
    "            docs_text += f\"{i}. {truncated}...\\n\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"Sen T√ºrkiye B√ºy√ºk Millet Meclisi konu≈ümalarƒ±nƒ± analiz eden bir uzmansƒ±n.\n",
    "A≈üaƒüƒ±daki anahtar kelimeler ve √∂rnek konu≈üma metinlerinden yola √ßƒ±karak,\n",
    "bu konuyu en iyi tanƒ±mlayan kƒ±sa ve anlamlƒ± bir ba≈ülƒ±k olu≈ütur. ba≈ülƒ±ƒüƒ± doƒürudan cevap olarak ver a√ßƒ±klama yapma. √ñrnek ba≈ülƒ±klar : Ekonomi ve B√ºt√ße Politikalarƒ±, Eƒüitim Sistemi ve √ñƒüretmenlik, Saƒülƒ±k Hizmetleri ve Tedavi, G√ºvenlik ve Ter√∂rle M√ºcadele, vb.\n",
    "\n",
    "Anahtar Kelimeler: {keywords}\n",
    "\n",
    "√ñrnek Konu≈ümalar:\n",
    "{docs_text}\n",
    "\n",
    "Sadece ba≈ülƒ±k ver, a√ßƒ±klama yapma. Ba≈ülƒ±k T√ºrk√ße olmalƒ± ve en fazla 5 kelime olmalƒ±.\n",
    "Ba≈ülƒ±k:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def _clean_topic_name(self, name: str) -> str:\n",
    "        name = re.sub(r'^(Ba≈ülƒ±k:|Konu:|Topic:)\\s*', '', name, flags=re.IGNORECASE)\n",
    "        name = re.sub(r'[\"\\']', '', name)\n",
    "        name = name.strip()\n",
    "        \n",
    "        words = name.split()\n",
    "        cleaned_words = []\n",
    "        for word in words:\n",
    "            if word and len(word) > 1:\n",
    "                cleaned_words.append(word[0].upper() + word[1:].lower())\n",
    "            elif word:\n",
    "                cleaned_words.append(word.upper())\n",
    "        \n",
    "        return ' '.join(cleaned_words)\n",
    "    \n",
    "    def generate_topic_name(\n",
    "        self, \n",
    "        topic_id: int, \n",
    "        keywords: str, \n",
    "        representative_docs: List[str],\n",
    "        max_retries: int = 3\n",
    "    ) -> str:\n",
    "        prompt = self._build_prompt(keywords, representative_docs)\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"Sen T√ºrk√ße konu≈üan bir meclis uzmanƒ±sƒ±n. Kƒ±sa, a√ßƒ±k ve anlamlƒ± ba≈ülƒ±klar olu≈üturursun.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    max_tokens=50,\n",
    "                    top_p=1,\n",
    "                    stream=False\n",
    "                )\n",
    "                \n",
    "                generated_name = response.choices[0].message.content.strip()\n",
    "                generated_name = self._clean_topic_name(generated_name)\n",
    "                \n",
    "                if generated_name and len(generated_name) > 5:\n",
    "                    return generated_name\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  Generated name too short for topic {topic_id}, retrying...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Attempt {attempt + 1} failed for topic {topic_id}: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                    continue\n",
    "        \n",
    "        words = [w.strip().capitalize() for w in keywords.split(',')[:4]]\n",
    "        return ' ve '.join(words) if len(words) <= 3 else ' '.join(words[:3])\n",
    "    \n",
    "    def process_topic_details_csv(self, csv_path: str) -> Dict[int, str]:\n",
    "        print(f\"\\nü§ñ Generating readable topic names with Groq LLM...\")\n",
    "        print(f\"   Model: {self.model}\")\n",
    "        \n",
    "        topic_mapping = {}\n",
    "        \n",
    "        try:\n",
    "            maxInt = sys.maxsize\n",
    "            while True:\n",
    "                try:\n",
    "                    csv.field_size_limit(maxInt)\n",
    "                    break\n",
    "                except OverflowError:\n",
    "                    maxInt = int(maxInt / 10)\n",
    "            \n",
    "            with open(csv_path, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                topics = list(reader)\n",
    "            \n",
    "            total = len(topics)\n",
    "            print(f\"   Processing {total} topics...\")\n",
    "            \n",
    "            for idx, row in enumerate(topics, 1):\n",
    "                topic_id = int(row['Topic'])\n",
    "                \n",
    "                if topic_id == -1:\n",
    "                    print(f\"   [{idx}/{total}] Skipping outlier topic -1\")\n",
    "                    continue\n",
    "                \n",
    "                keywords = row.get('Keywords', '')\n",
    "                rep_docs_str = row.get('Representative_Docs', '')\n",
    "                \n",
    "                if len(rep_docs_str) > 3000:\n",
    "                    rep_docs_str = rep_docs_str[:3000]\n",
    "                \n",
    "                try:\n",
    "                    rep_docs_str = rep_docs_str.strip('[]')\n",
    "                    if rep_docs_str:\n",
    "                        rep_docs = [doc.strip(' \"\\\"') for doc in rep_docs_str.split('\", \"')]\n",
    "                        rep_docs = [doc for doc in rep_docs if doc]\n",
    "                    else:\n",
    "                        rep_docs = []\n",
    "                except:\n",
    "                    rep_docs = []\n",
    "                \n",
    "                print(f\"   [{idx}/{total}] Topic {topic_id}: Generating name...\")\n",
    "                readable_name = self.generate_topic_name(topic_id, keywords, rep_docs)\n",
    "                topic_mapping[topic_id] = readable_name\n",
    "                \n",
    "                print(f\"   ‚úÖ Topic {topic_id}: \\\"{readable_name}\\\"\")\n",
    "                \n",
    "                time.sleep(0.1)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Successfully generated {len(topic_mapping)} topic names\")\n",
    "            return topic_mapping\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ùå Error: topic_details.csv not found at {csv_path}\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing topics: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Update Elasticsearch with Groq-Generated Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elasticsearch_topic_labels(\n",
    "    es: Elasticsearch, \n",
    "    topic_mapping: Dict[int, str],\n",
    "    index: str = ELASTICSEARCH_INDEX\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Bulk update Elasticsearch documents with readable topic names.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüíæ Updating Elasticsearch with readable topic names...\")\n",
    "    \n",
    "    total_updated = 0\n",
    "    \n",
    "    for topic_id, readable_name in topic_mapping.items():\n",
    "        try:\n",
    "            response = es.update_by_query(\n",
    "                index=index,\n",
    "                body={\n",
    "                    \"script\": {\n",
    "                        \"source\": \"ctx._source.topic_label = params.new_label\",\n",
    "                        \"lang\": \"painless\",\n",
    "                        \"params\": {\n",
    "                            \"new_label\": readable_name\n",
    "                        }\n",
    "                    },\n",
    "                    \"query\": {\n",
    "                        \"term\": {\"topic_id\": topic_id}\n",
    "                    }\n",
    "                },\n",
    "                conflicts='proceed',\n",
    "                refresh=True\n",
    "            )\n",
    "            \n",
    "            updated = response.get('updated', 0)\n",
    "            total_updated += updated\n",
    "            \n",
    "            if updated > 0:\n",
    "                print(f\"   ‚úÖ Topic {topic_id}: Updated {updated:,} documents to \\\"{readable_name}\\\"\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error updating topic {topic_id}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total documents updated: {total_updated:,}\")\n",
    "    return total_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Export Topic Summary to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_topic_summary(\n",
    "    speeches: List[Dict],\n",
    "    topics: List[int],\n",
    "    topic_model: BERTopic,\n",
    "    exclude_outliers: bool = True,\n",
    "    groq_topic_mapping: Optional[Dict[int, str]] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create and export topic summary CSV for backup/analysis.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Creating topic summary...\")\n",
    "    \n",
    "    df = pd.DataFrame(speeches)\n",
    "    df['topic_id'] = topics\n",
    "    \n",
    "    if exclude_outliers:\n",
    "        original_count = len(df)\n",
    "        df = df[df['topic_id'] != -1].copy()\n",
    "        excluded_count = original_count - len(df)\n",
    "        if excluded_count > 0:\n",
    "            print(f\"   Excluding {excluded_count:,} outlier speeches (topic_id -1)\")\n",
    "    \n",
    "    topic_info = topic_model.get_topic_info()\n",
    "    topic_labels = {\n",
    "        int(row['Topic']): row['Name'] \n",
    "        for _, row in topic_info.iterrows()\n",
    "    }\n",
    "    df['topic_label'] = df['topic_id'].map(topic_labels)\n",
    "    \n",
    "    # Add Groq-generated topic names if available\n",
    "    if groq_topic_mapping:\n",
    "        df['groq_topic_label'] = df['topic_id'].map(groq_topic_mapping)\n",
    "        print(f\"   Added Groq-generated topic names for {df['groq_topic_label'].notna().sum():,} speeches\")\n",
    "    \n",
    "    # Create summary by MP and topic\n",
    "    groupby_cols = ['speech_giver', 'topic_id', 'topic_label']\n",
    "    if groq_topic_mapping:\n",
    "        groupby_cols.append('groq_topic_label')\n",
    "    \n",
    "    summary = df.groupby(groupby_cols).agg({\n",
    "        'id': 'count',\n",
    "        'term': lambda x: list(set(x.dropna())),\n",
    "        'year': lambda x: list(set(x.dropna()))\n",
    "    }).reset_index()\n",
    "    \n",
    "    summary.rename(columns={'id': 'speech_count'}, inplace=True)\n",
    "    summary = summary.sort_values('speech_count', ascending=False)\n",
    "    \n",
    "    summary.to_csv(TOPIC_SUMMARY_FILE, index=False)\n",
    "    print(f\"‚úÖ Topic summary saved to {TOPIC_SUMMARY_FILE}\")\n",
    "    print(f\"   Total rows: {len(summary):,}\")\n",
    "    print(f\"   Unique topics: {summary['topic_id'].nunique()}\")\n",
    "    print(f\"   Unique MPs: {summary['speech_giver'].nunique()}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Visualize Top Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_topics(topic_model: BERTopic, n_topics: int = 10):\n",
    "    \"\"\"\n",
    "    Visualize the top topics discovered.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà Generating topic visualization...\")\n",
    "    \n",
    "    try:\n",
    "        topic_info = topic_model.get_topic_info()\n",
    "        topic_info = topic_info[topic_info['Topic'] != -1]\n",
    "        top_topics = topic_info.nlargest(n_topics, 'Count')\n",
    "        \n",
    "        print(f\"\\nüèÜ Top {n_topics} Topics:\")\n",
    "        print(\"=\" * 80)\n",
    "        for idx, row in top_topics.iterrows():\n",
    "            print(f\"Topic {row['Topic']}: {row['Name']}\")\n",
    "            print(f\"   Count: {row['Count']:,} speeches\")\n",
    "            print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not generate visualization: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Main Execution\n",
    "\n",
    "Run the complete pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution flow.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"PARLIAMENT SPEECH TOPIC ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Connect to Elasticsearch\n",
    "    es = connect_to_elasticsearch()\n",
    "    \n",
    "    # Step 2: Fetch all speeches\n",
    "    speeches = fetch_all_speeches(es)\n",
    "    \n",
    "    if not speeches:\n",
    "        print(\"‚ùå No speeches found. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Run topic modeling\n",
    "    topics, topic_model = run_topic_modeling(speeches)\n",
    "    \n",
    "    # Step 4: Update Elasticsearch with results\n",
    "    success, failed = update_elasticsearch_with_topics(\n",
    "        es, speeches, topics, topic_model\n",
    "    )\n",
    "    \n",
    "    # Step 5: Save detailed topic information for LLM\n",
    "    save_topic_details(topic_model, TOPIC_DETAILS_FILE)\n",
    "    \n",
    "    # Step 6: Generate readable topic names with LLM (optional)\n",
    "    topic_mapping = None\n",
    "    if USE_LLM_NAMING and GROQ_API_KEY:\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"ü§ñ LLM TOPIC NAME GENERATION\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            namer = GroqTopicNamer(api_key=GROQ_API_KEY, model=GROQ_MODEL)\n",
    "            topic_mapping = namer.process_topic_details_csv(TOPIC_DETAILS_FILE)\n",
    "            \n",
    "            if topic_mapping:\n",
    "                updated_count = update_elasticsearch_topic_labels(es, topic_mapping, ELASTICSEARCH_INDEX)\n",
    "                \n",
    "                print(\"\\n‚úÖ LLM naming complete!\")\n",
    "                print(f\"üìä Generated names for {len(topic_mapping)} topics\")\n",
    "                print(f\"üìä Updated {updated_count:,} documents in Elasticsearch\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No topic mappings generated, skipping ES update\")\n",
    "                topic_mapping = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  LLM naming failed: {e}\")\n",
    "            print(\"   Continuing with keyword-based labels\")\n",
    "            topic_mapping = None\n",
    "    elif not GROQ_API_KEY:\n",
    "        print(\"\\n‚ö†Ô∏è  Skipping LLM naming: GROQ_API_KEY not set\")\n",
    "        print(\"   Set GROQ_API_KEY variable to enable\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Skipping LLM naming: USE_LLM_NAMING=False\")\n",
    "    \n",
    "    # Step 7: Export summary to CSV (backup) - after LLM naming if enabled\n",
    "    summary = export_topic_summary(speeches, topics, topic_model, groq_topic_mapping=topic_mapping)\n",
    "    \n",
    "    # Step 8: Show top topics\n",
    "    visualize_top_topics(topic_model, n_topics=10)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ TOPIC ANALYSIS COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìä Total speeches analyzed: {len(speeches):,}\")\n",
    "    print(f\"üìä Documents updated in ES: {success:,}\")\n",
    "    print(f\"üìä Model saved to: {MODEL_SAVE_PATH}\")\n",
    "    print(f\"üìä Topic details saved to: {TOPIC_DETAILS_FILE}\")\n",
    "    print(f\"üìä Summary saved to: {TOPIC_SUMMARY_FILE}\")\n",
    "    if USE_LLM_NAMING and GROQ_API_KEY:\n",
    "        print(f\"ü§ñ LLM-generated names: Enabled\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run the main function\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Results\n",
    "\n",
    "Display the topic summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the topic summary\n",
    "summary_df = pd.read_csv(TOPIC_SUMMARY_FILE)\n",
    "print(f\"\\nTotal rows in summary: {len(summary_df):,}\")\n",
    "print(f\"Columns: {list(summary_df.columns)}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "summary_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Files (for Google Colab)\n",
    "\n",
    "Download the generated files to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download files in Google Colab\n",
    "# from google.colab import files\n",
    "# files.download(TOPIC_SUMMARY_FILE)\n",
    "# files.download(TOPIC_DETAILS_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
