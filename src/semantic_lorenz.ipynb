{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Shift Visualization via Lorenz Attractor\n",
    "Track how word senses evolve across parliamentary terms using a Lorenz attractor-driven trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Imports\n",
    "%pip install \"elasticsearch==8.6.2\" sentence-transformers scikit-learn pandas matplotlib scipy plotly\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.integrate import odeint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import re, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Connect to Elasticsearch\n",
    "es = Elasticsearch(ES_URL)\n",
    "print(\"Connected to Elasticsearch\")\n",
    "print(es.info().body[\"version\"][\"number\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_speeches(term, year, size=10000):\n",
    "    \"\"\"Fetch speeches for a specific term and year.\"\"\"\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"_source\": [\"content\", \"term\", \"year\"],\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": [\n",
    "                    {\"term\": {\"term\": term}},\n",
    "                    {\"term\": {\"year\": year}}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    res = es.search(index=INDEX_NAME, body=query)\n",
    "    return [hit[\"_source\"][\"content\"] for hit in res[\"hits\"][\"hits\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contexts(texts, target_word, window=10):\n",
    "    \"\"\"Extract short context windows around target word and its morphological variations.\"\"\"\n",
    "    contexts = []\n",
    "    pattern = re.compile(rf\"\\b{re.escape(target_word.lower())}\\w*\\b\")\n",
    "    \n",
    "    for t in texts:\n",
    "        tokens = re.findall(r\"\\w+\", t.lower())\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if pattern.match(tok):\n",
    "                start = max(0, i - window)\n",
    "                end = min(len(tokens), i + window + 1)\n",
    "                snippet = \" \".join(tokens[start:end])\n",
    "                contexts.append(snippet)\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, contexts):\n",
    "    \"\"\"Compute embeddings for context snippets.\"\"\"\n",
    "    if len(contexts) == 0:\n",
    "        return np.empty((0, model.get_sentence_embedding_dimension()))\n",
    "    return model.encode(contexts, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_prototypes(X, labels, return_label_ids=False):\n",
    "    \"\"\"Compute centroids for each cluster and optionally return their IDs.\"\"\"\n",
    "    clusters = []\n",
    "    label_ids = []\n",
    "    for label in np.unique(labels):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        members = X[labels == label]\n",
    "        if len(members) == 0:\n",
    "            continue\n",
    "        centroid = np.mean(members, axis=0)\n",
    "        clusters.append(centroid)\n",
    "        label_ids.append(label)\n",
    "    clusters = np.array(clusters)\n",
    "    if return_label_ids:\n",
    "        return clusters, label_ids\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_clusters(labels, max_clusters):\n",
    "    \"\"\"Keep only the largest max_clusters and map the rest to -1.\"\"\"\n",
    "    if max_clusters is None:\n",
    "        return labels, np.unique(labels).tolist()\n",
    "    labels = np.asarray(labels)\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    cluster_counts = [\n",
    "        (label, count) for label, count in zip(unique, counts) if label != -1\n",
    "    ]\n",
    "    cluster_counts.sort(key=lambda item: item[1], reverse=True)\n",
    "    keep = [label for label, _ in cluster_counts[:max_clusters]]\n",
    "    if not keep:\n",
    "        return np.full_like(labels, -1), []\n",
    "    filtered = np.array([label if label in keep else -1 for label in labels], dtype=labels.dtype)\n",
    "    return filtered, keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterAligner:\n",
    "    \"\"\"Keeps global cluster IDs and assigns consistent colors over time.\"\"\"\n",
    "\n",
    "    def __init__(self, max_clusters=100, similarity_threshold=0.8, cmap_name=\"gist_ncar\"):\n",
    "        self.max_clusters = max_clusters\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.centroids = []\n",
    "        self.global_ids = []\n",
    "        self.cmap = plt.cm.get_cmap(cmap_name, max_clusters)\n",
    "        self.palette = [self.cmap(i) for i in range(self.cmap.N)]\n",
    "        self.overflow_color = (0.65, 0.65, 0.65, 1.0)\n",
    "\n",
    "    def _add_centroid(self, centroid):\n",
    "        if len(self.global_ids) >= self.max_clusters:\n",
    "            return -1\n",
    "        new_id = len(self.global_ids)\n",
    "        self.centroids.append(centroid)\n",
    "        self.global_ids.append(new_id)\n",
    "        return new_id\n",
    "\n",
    "    def _match_or_create(self, centroid):\n",
    "        centroid = centroid.reshape(1, -1)\n",
    "        if not self.centroids:\n",
    "            return self._add_centroid(centroid)\n",
    "        stacked = np.vstack(self.centroids)\n",
    "        sims = cosine_similarity(stacked, centroid)[:, 0]\n",
    "        best_idx = int(np.argmax(sims))\n",
    "        if sims[best_idx] >= self.similarity_threshold:\n",
    "            return self.global_ids[best_idx]\n",
    "        return self._add_centroid(centroid)\n",
    "\n",
    "    def align(self, raw_labels, centroid_map):\n",
    "        aligned = np.full_like(raw_labels, -1)\n",
    "        for local_label, centroid in centroid_map.items():\n",
    "            global_id = self._match_or_create(centroid)\n",
    "            if global_id == -1:\n",
    "                continue\n",
    "            aligned[raw_labels == local_label] = global_id\n",
    "        return aligned\n",
    "\n",
    "    def get_color(self, label):\n",
    "        if 0 <= label < len(self.palette):\n",
    "            return self.palette[label]\n",
    "        return self.overflow_color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Sentence Transformer Model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Model loaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure to track cluster evolution\n",
    "cluster_timeline = []  # List of dicts: {term, year, global_id, centroid, size, contexts}\n",
    "\n",
    "aligner = ClusterAligner(max_clusters=MAX_CLUSTERS, similarity_threshold=SIMILARITY_THRESHOLD)\n",
    "baseline_used = False\n",
    "\n",
    "print(f\"\\n=== Analyzing '{TARGET_WORD}' across {len(TERM_YEAR_TUPLES)} term-year pairs ===\")\n",
    "\n",
    "for term, year in TERM_YEAR_TUPLES:\n",
    "    print(f\"\\n--- Term {term}, Year {year} ---\")\n",
    "    texts = fetch_speeches(term, year)\n",
    "    contexts = extract_contexts(texts, TARGET_WORD)\n",
    "    print(f\"  Contexts: {len(contexts)}\")\n",
    "    \n",
    "    if len(contexts) < 10:\n",
    "        print(\"  Not enough contexts, skipping this slice.\")\n",
    "        continue\n",
    "\n",
    "    embeddings = compute_embeddings(model, contexts)\n",
    "    ap = AffinityPropagation(random_state=42)\n",
    "    ap.fit(embeddings)\n",
    "    local_labels = ap.labels_\n",
    "\n",
    "    cap = BASELINE_MAX_CLUSTERS if not baseline_used else MAX_CLUSTERS\n",
    "    limited_labels, kept_clusters = limit_clusters(local_labels, cap)\n",
    "    print(f\"  Raw clusters: {len(np.unique(local_labels))}, kept: {len(kept_clusters)} (cap={cap})\")\n",
    "\n",
    "    prototypes, proto_labels = get_cluster_prototypes(embeddings, limited_labels, return_label_ids=True)\n",
    "    centroid_map = dict(zip(proto_labels, prototypes))\n",
    "    if not centroid_map:\n",
    "        print(\"  No clusters survived filtering, skipping.\")\n",
    "        continue\n",
    "\n",
    "    baseline_used = True\n",
    "    aligned_labels = aligner.align(limited_labels, centroid_map)\n",
    "    \n",
    "    # Count cluster sizes\n",
    "    cluster_sizes = {}\n",
    "    for label in aligned_labels:\n",
    "        if label >= 0:\n",
    "            cluster_sizes[label] = cluster_sizes.get(label, 0) + 1\n",
    "    \n",
    "    # Get top-K clusters by size\n",
    "    top_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)[:TOP_K_CLUSTERS]\n",
    "    print(f\"  Top {len(top_clusters)} clusters: {top_clusters}\")\n",
    "    \n",
    "    # Store cluster info\n",
    "    for global_id, size in top_clusters:\n",
    "        # Find the centroid for this global_id\n",
    "        centroid_idx = aligner.global_ids.index(global_id)\n",
    "        centroid = aligner.centroids[centroid_idx]\n",
    "        \n",
    "        cluster_timeline.append({\n",
    "            'term': term,\n",
    "            'year': year,\n",
    "            'global_id': global_id,\n",
    "            'centroid': centroid,\n",
    "            'size': size,\n",
    "            'total_contexts': len(contexts)\n",
    "        })\n",
    "\n",
    "print(f\"\\n=== Collected {len(cluster_timeline)} cluster snapshots ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier manipulation\n",
    "df_timeline = pd.DataFrame(cluster_timeline)\n",
    "df_timeline['size_share'] = df_timeline['size'] / df_timeline['total_contexts']\n",
    "df_timeline['time_idx'] = df_timeline.groupby('global_id').cumcount()\n",
    "print(df_timeline.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Lorenz Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lorenz_coords(df_timeline):\n",
    "    \"\"\"Compute Lorenz coordinates (x, y, z) for each cluster snapshot.\"\"\"\n",
    "    lorenz_data = []\n",
    "    \n",
    "    for global_id in df_timeline['global_id'].unique():\n",
    "        cluster_df = df_timeline[df_timeline['global_id'] == global_id].sort_values(['term', 'year'])\n",
    "        \n",
    "        if len(cluster_df) == 0:\n",
    "            continue\n",
    "        \n",
    "        baseline_centroid = cluster_df.iloc[0]['centroid']\n",
    "        prev_centroid = None\n",
    "        \n",
    "        for idx, row in cluster_df.iterrows():\n",
    "            current_centroid = row['centroid']\n",
    "            \n",
    "            # x: distance from baseline\n",
    "            baseline_sim = cosine_similarity(\n",
    "                current_centroid.reshape(1, -1), \n",
    "                baseline_centroid.reshape(1, -1)\n",
    "            )[0, 0]\n",
    "            x = 1.0 - baseline_sim\n",
    "            \n",
    "            # y: local drift (distance from previous)\n",
    "            if prev_centroid is not None:\n",
    "                local_sim = cosine_similarity(\n",
    "                    current_centroid.reshape(1, -1),\n",
    "                    prev_centroid.reshape(1, -1)\n",
    "                )[0, 0]\n",
    "                y = 1.0 - local_sim\n",
    "            else:\n",
    "                y = 0.0\n",
    "            \n",
    "            # z: size share\n",
    "            z = row['size_share']\n",
    "            \n",
    "            lorenz_data.append({\n",
    "                'term': row['term'],\n",
    "                'year': row['year'],\n",
    "                'global_id': global_id,\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'z': z,\n",
    "                'size': row['size']\n",
    "            })\n",
    "            \n",
    "            prev_centroid = current_centroid\n",
    "    \n",
    "    return pd.DataFrame(lorenz_data)\n",
    "\n",
    "df_lorenz = compute_lorenz_coords(df_timeline)\n",
    "print(df_lorenz.head(10))\n",
    "print(f\"\\nLorenz coordinates computed for {len(df_lorenz)} snapshots\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lorenz System Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_system(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "    \"\"\"Classic Lorenz attractor equations.\"\"\"\n",
    "    x, y, z = state\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (rho - z) - y\n",
    "    dz = x * y - beta * z\n",
    "    return [dx, dy, dz]\n",
    "\n",
    "def integrate_cluster_trajectory(cluster_data, dt=0.01, steps_per_point=50):\n",
    "    \"\"\"Integrate Lorenz system driven by cluster data.\"\"\"\n",
    "    trajectory = []\n",
    "    \n",
    "    if len(cluster_data) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Initialize with first data point (scaled up for visibility)\n",
    "    x0 = cluster_data.iloc[0]['x'] * 10\n",
    "    y0 = cluster_data.iloc[0]['y'] * 10\n",
    "    z0 = cluster_data.iloc[0]['z'] * 30\n",
    "    state = [x0, y0, z0]\n",
    "    \n",
    "    for idx, row in cluster_data.iterrows():\n",
    "        # Use data as perturbation/forcing\n",
    "        target_x = row['x'] * 10\n",
    "        target_y = row['y'] * 10\n",
    "        target_z = row['z'] * 30\n",
    "        \n",
    "        # Integrate towards target with Lorenz dynamics\n",
    "        t_span = np.linspace(0, dt * steps_per_point, steps_per_point)\n",
    "        \n",
    "        for step in range(steps_per_point):\n",
    "            # Add attraction towards data point\n",
    "            force_x = (target_x - state[0]) * 0.1\n",
    "            force_y = (target_y - state[1]) * 0.1\n",
    "            force_z = (target_z - state[2]) * 0.1\n",
    "            \n",
    "            # Lorenz dynamics\n",
    "            d_state = lorenz_system(state, 0)\n",
    "            \n",
    "            # Combine\n",
    "            state[0] += (d_state[0] + force_x) * dt\n",
    "            state[1] += (d_state[1] + force_y) * dt\n",
    "            state[2] += (d_state[2] + force_z) * dt\n",
    "            \n",
    "            trajectory.append([\n",
    "                state[0], state[1], state[2],\n",
    "                row['term'], row['year'], row['global_id'], row['size']\n",
    "            ])\n",
    "    \n",
    "    return np.array(trajectory)\n",
    "\n",
    "# Generate trajectories for each cluster\n",
    "all_trajectories = {}\n",
    "for global_id in df_lorenz['global_id'].unique():\n",
    "    cluster_data = df_lorenz[df_lorenz['global_id'] == global_id].sort_values(['term', 'year'])\n",
    "    traj = integrate_cluster_trajectory(cluster_data)\n",
    "    if len(traj) > 0:\n",
    "        all_trajectories[global_id] = traj\n",
    "        print(f\"Cluster {global_id}: {len(traj)} trajectory points\")\n",
    "\n",
    "print(f\"\\n=== Generated {len(all_trajectories)} cluster trajectories ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Configuration\n",
    "INDEX_NAME = \"parliament_speeches\"\n",
    "ES_URL = \"http://localhost:9200\"\n",
    "TARGET_WORD = \"sağlık\"\n",
    "START_TERM = 17\n",
    "END_TERM = 27\n",
    "YEARS_PER_TERM = 5\n",
    "BASELINE_MAX_CLUSTERS = 50\n",
    "MAX_CLUSTERS = 100\n",
    "SIMILARITY_THRESHOLD = 0.8\n",
    "TOP_K_CLUSTERS = 3  # Track top-3 clusters per year\n",
    "OUTPUT_DIR = \"./lorenz_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Generate term-year tuples\n",
    "TERM_YEAR_TUPLES = [\n",
    "    (term, year) \n",
    "    for term in range(START_TERM, END_TERM + 1) \n",
    "    for year in range(1, YEARS_PER_TERM + 1)\n",
    "]\n",
    "print(f\"Processing {len(TERM_YEAR_TUPLES)} term-year pairs from {TERM_YEAR_TUPLES[0]} to {TERM_YEAR_TUPLES[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib 3D plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for global_id, traj in all_trajectories.items():\n",
    "    color = aligner.get_color(global_id)\n",
    "    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2], \n",
    "            color=color, alpha=0.7, linewidth=2, label=f'Cluster {global_id}')\n",
    "    # Mark start and end\n",
    "    ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2], \n",
    "              color=color, s=100, marker='o', edgecolors='black', linewidths=2)\n",
    "    ax.scatter(traj[-1, 0], traj[-1, 1], traj[-1, 2], \n",
    "              color=color, s=100, marker='s', edgecolors='black', linewidths=2)\n",
    "\n",
    "ax.set_xlabel('X (Baseline Drift)', fontsize=12)\n",
    "ax.set_ylabel('Y (Local Drift)', fontsize=12)\n",
    "ax.set_zlabel('Z (Size Share)', fontsize=12)\n",
    "ax.set_title(f'Lorenz Attractor: Semantic Evolution of \"{TARGET_WORD}\"\\n({START_TERM},{1}) to ({END_TERM},{YEARS_PER_TERM})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=8)\n",
    "ax.view_init(elev=20, azim=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(OUTPUT_DIR, f'lorenz_{TARGET_WORD}.png')\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Saved plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plotly 3D plot\n",
    "fig_plotly = go.Figure()\n",
    "\n",
    "for global_id, traj in all_trajectories.items():\n",
    "    color_tuple = aligner.get_color(global_id)\n",
    "    color_str = f'rgba({int(color_tuple[0]*255)},{int(color_tuple[1]*255)},{int(color_tuple[2]*255)},{color_tuple[3]})'\n",
    "    \n",
    "    # Trajectory line\n",
    "    fig_plotly.add_trace(go.Scatter3d(\n",
    "        x=traj[:, 0], y=traj[:, 1], z=traj[:, 2],\n",
    "        mode='lines',\n",
    "        line=dict(color=color_str, width=4),\n",
    "        name=f'Cluster {global_id}',\n",
    "        hovertext=[f'T{int(t)},Y{int(y)}' for t, y in zip(traj[:, 3], traj[:, 4])],\n",
    "        hoverinfo='text'\n",
    "    ))\n",
    "    \n",
    "    # Start marker\n",
    "    fig_plotly.add_trace(go.Scatter3d(\n",
    "        x=[traj[0, 0]], y=[traj[0, 1]], z=[traj[0, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color=color_str, symbol='circle', line=dict(color='black', width=2)),\n",
    "        name=f'Start {global_id}',\n",
    "        showlegend=False\n",
    "    ))\n",
    "    \n",
    "    # End marker\n",
    "    fig_plotly.add_trace(go.Scatter3d(\n",
    "        x=[traj[-1, 0]], y=[traj[-1, 1]], z=[traj[-1, 2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=8, color=color_str, symbol='square', line=dict(color='black', width=2)),\n",
    "        name=f'End {global_id}',\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "fig_plotly.update_layout(\n",
    "    title=f'Lorenz Attractor: Semantic Evolution of \"{TARGET_WORD}\"',\n",
    "    scene=dict(\n",
    "        xaxis_title='X (Baseline Drift)',\n",
    "        yaxis_title='Y (Local Drift)',\n",
    "        zaxis_title='Z (Size Share)'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "html_path = os.path.join(OUTPUT_DIR, f'lorenz_{TARGET_WORD}_interactive.html')\n",
    "fig_plotly.write_html(html_path)\n",
    "print(f\"Saved interactive plot to {html_path}\")\n",
    "fig_plotly.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Plotly Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animated Plotly visualization with frame-by-frame progression\n",
    "def create_animated_lorenz(all_trajectories, aligner, target_word, output_dir, frame_step=5):\n",
    "    \"\"\"\n",
    "    Create an animated 3D plot where trajectories progressively reveal over time.\n",
    "    frame_step: how many integration steps to advance per frame (lower = smoother but more frames)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine max trajectory length for frame count\n",
    "    max_len = max(len(traj) for traj in all_trajectories.values())\n",
    "    num_frames = max_len // frame_step\n",
    "    \n",
    "    frames = []\n",
    "    \n",
    "    for frame_idx in range(0, max_len, frame_step):\n",
    "        frame_data = []\n",
    "        \n",
    "        for global_id, traj in all_trajectories.items():\n",
    "            color_tuple = aligner.get_color(global_id)\n",
    "            color_str = f'rgba({int(color_tuple[0]*255)},{int(color_tuple[1]*255)},{int(color_tuple[2]*255)},{color_tuple[3]})'\n",
    "            \n",
    "            # Get trajectory up to current frame\n",
    "            end_idx = min(frame_idx + 1, len(traj))\n",
    "            if end_idx == 0:\n",
    "                continue\n",
    "            \n",
    "            current_traj = traj[:end_idx]\n",
    "            \n",
    "            # Trajectory line (growing)\n",
    "            frame_data.append(go.Scatter3d(\n",
    "                x=current_traj[:, 0], \n",
    "                y=current_traj[:, 1], \n",
    "                z=current_traj[:, 2],\n",
    "                mode='lines',\n",
    "                line=dict(color=color_str, width=4),\n",
    "                name=f'Cluster {global_id}',\n",
    "                hovertext=[f'T{int(t)},Y{int(y)}' for t, y in zip(current_traj[:, 3], current_traj[:, 4])],\n",
    "                hoverinfo='text',\n",
    "                showlegend=(frame_idx == 0)\n",
    "            ))\n",
    "            \n",
    "            # Current position marker (moving point)\n",
    "            if end_idx > 0:\n",
    "                frame_data.append(go.Scatter3d(\n",
    "                    x=[current_traj[-1, 0]], \n",
    "                    y=[current_traj[-1, 1]], \n",
    "                    z=[current_traj[-1, 2]],\n",
    "                    mode='markers',\n",
    "                    marker=dict(size=10, color=color_str, symbol='diamond', \n",
    "                               line=dict(color='white', width=2)),\n",
    "                    name=f'Current {global_id}',\n",
    "                    showlegend=False\n",
    "                ))\n",
    "        \n",
    "        frames.append(go.Frame(data=frame_data, name=str(frame_idx)))\n",
    "    \n",
    "    # Initial frame (empty or first frame)\n",
    "    initial_data = frames[0].data if frames else []\n",
    "    \n",
    "    # Create figure\n",
    "    fig_anim = go.Figure(\n",
    "        data=initial_data,\n",
    "        frames=frames\n",
    "    )\n",
    "    \n",
    "    # Add play/pause buttons and slider\n",
    "    fig_anim.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Lorenz Attractor: Semantic Evolution of \"{target_word}\" (Animated)',\n",
    "            font=dict(size=18)\n",
    "        ),\n",
    "        scene=dict(\n",
    "            xaxis_title='X (Baseline Drift)',\n",
    "            yaxis_title='Y (Local Drift)',\n",
    "            zaxis_title='Z (Size Share)',\n",
    "            camera=dict(\n",
    "                eye=dict(x=1.5, y=1.5, z=1.2)\n",
    "            )\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=900,\n",
    "        updatemenus=[\n",
    "            dict(\n",
    "                type='buttons',\n",
    "                showactive=False,\n",
    "                buttons=[\n",
    "                    dict(\n",
    "                        label='Play',\n",
    "                        method='animate',\n",
    "                        args=[None, dict(\n",
    "                            frame=dict(duration=50, redraw=True),\n",
    "                            fromcurrent=True,\n",
    "                            mode='immediate',\n",
    "                            transition=dict(duration=0)\n",
    "                        )]\n",
    "                    ),\n",
    "                    dict(\n",
    "                        label='Pause',\n",
    "                        method='animate',\n",
    "                        args=[[None], dict(\n",
    "                            frame=dict(duration=0, redraw=False),\n",
    "                            mode='immediate',\n",
    "                            transition=dict(duration=0)\n",
    "                        )]\n",
    "                    )\n",
    "                ],\n",
    "                x=0.1, y=0.0, xanchor='left', yanchor='bottom'\n",
    "            )\n",
    "        ],\n",
    "        sliders=[\n",
    "            dict(\n",
    "                active=0,\n",
    "                steps=[\n",
    "                    dict(\n",
    "                        args=[[f.name], dict(\n",
    "                            frame=dict(duration=0, redraw=True),\n",
    "                            mode='immediate',\n",
    "                            transition=dict(duration=0)\n",
    "                        )],\n",
    "                        label=str(i),\n",
    "                        method='animate'\n",
    "                    ) for i, f in enumerate(frames)\n",
    "                ],\n",
    "                x=0.1, y=0.0, len=0.9, xanchor='left', yanchor='top',\n",
    "                currentvalue=dict(\n",
    "                    visible=True,\n",
    "                    prefix='Frame: ',\n",
    "                    xanchor='right'\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save and display\n",
    "    html_path = os.path.join(output_dir, f'lorenz_{target_word}_animated.html')\n",
    "    fig_anim.write_html(html_path)\n",
    "    print(f\"Saved animated plot to {html_path}\")\n",
    "    \n",
    "    return fig_anim\n",
    "\n",
    "# Generate animation\n",
    "fig_animated = create_animated_lorenz(all_trajectories, aligner, TARGET_WORD, OUTPUT_DIR, frame_step=10)\n",
    "fig_animated.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "csv_path = os.path.join(OUTPUT_DIR, f'lorenz_data_{TARGET_WORD}.csv')\n",
    "df_lorenz.to_csv(csv_path, index=False)\n",
    "print(f\"Saved Lorenz coordinates to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
