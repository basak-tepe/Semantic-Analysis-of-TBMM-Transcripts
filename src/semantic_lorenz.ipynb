{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPsQVeHIdgAs"
      },
      "source": [
        "# Semantic Shift Visualization via Lorenz Attractor\n",
        "Track how word senses evolve across parliamentary terms using a Lorenz attractor-driven trajectory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgnFqnuudgAt"
      },
      "outputs": [],
      "source": [
        "# ## Imports\n",
        "%pip install \"elasticsearch==8.6.2\" sentence-transformers scikit-learn pandas matplotlib scipy plotly\n",
        "from elasticsearch import Elasticsearch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.integrate import odeint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import plotly.graph_objects as go\n",
        "import re, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiwR53RCdgAu"
      },
      "outputs": [],
      "source": [
        "# ## Connect to Elasticsearch\n",
        "es = Elasticsearch(ES_URL)\n",
        "print(\"Connected to Elasticsearch\")\n",
        "print(es.info().body[\"version\"][\"number\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Configuration\n",
        "INDEX_NAME = \"parliament_speeches\"\n",
        "ES_URL = \"https://analog-advisory-many-specialists.trycloudflare.com\"   # adjust if different\n",
        "TARGET_WORD = \"vergi\"\n",
        "START_TERM = 17\n",
        "END_TERM = 27\n",
        "YEARS_PER_TERM = 5\n",
        "BASELINE_MAX_CLUSTERS = 50\n",
        "MAX_CLUSTERS = 100\n",
        "SIMILARITY_THRESHOLD = 0.8\n",
        "TOP_K_CLUSTERS = 3  # Track top-3 clusters per year\n",
        "OUTPUT_DIR = \"./lorenz_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "UpicSoccdrbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YOqnpvwdgAu"
      },
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoTNblmldgAu"
      },
      "outputs": [],
      "source": [
        "def fetch_speeches(term, year, size=10000):\n",
        "    \"\"\"Fetch speeches for a specific term and year.\"\"\"\n",
        "    query = {\n",
        "        \"size\": size,\n",
        "        \"_source\": [\"content\", \"term\", \"year\"],\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [\n",
        "                    {\"term\": {\"term\": term}},\n",
        "                    {\"term\": {\"year\": year}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    res = es.search(index=INDEX_NAME, body=query)\n",
        "    return [hit[\"_source\"][\"content\"] for hit in res[\"hits\"][\"hits\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_term_year_tuples(start_term, end_term):\n",
        "    result = []\n",
        "    for term in range(start_term, end_term + 1):\n",
        "        for year in range(1, 6):\n",
        "            result.append((term, year))\n",
        "    return result\n",
        "\n",
        "TERM_YEAR_TUPLES=make_term_year_tuples(START_TERM,END_TERM)\n",
        "print(f\"Processing {len(TERM_YEAR_TUPLES)} term-year pairs from {TERM_YEAR_TUPLES[0]} to {TERM_YEAR_TUPLES[-1]}\")"
      ],
      "metadata": {
        "id": "RBZTMzyldxTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRzvTQN-dgAu"
      },
      "outputs": [],
      "source": [
        "def extract_contexts(texts, target_word, window=10):\n",
        "    \"\"\"Extract short context windows around target word and its morphological variations.\"\"\"\n",
        "    contexts = []\n",
        "    pattern = re.compile(rf\"\\b{re.escape(target_word.lower())}\\w*\\b\")\n",
        "\n",
        "    for t in texts:\n",
        "        tokens = re.findall(r\"\\w+\", t.lower())\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if pattern.match(tok):\n",
        "                start = max(0, i - window)\n",
        "                end = min(len(tokens), i + window + 1)\n",
        "                snippet = \" \".join(tokens[start:end])\n",
        "                contexts.append(snippet)\n",
        "    return contexts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQkKi1lydgAu"
      },
      "outputs": [],
      "source": [
        "def compute_embeddings(model, contexts):\n",
        "    \"\"\"Compute embeddings for context snippets.\"\"\"\n",
        "    if len(contexts) == 0:\n",
        "        return np.empty((0, model.get_sentence_embedding_dimension()))\n",
        "    return model.encode(contexts, show_progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCuMrCHpdgAu"
      },
      "outputs": [],
      "source": [
        "def get_cluster_prototypes(X, labels, return_label_ids=False):\n",
        "    \"\"\"Compute centroids for each cluster and optionally return their IDs.\"\"\"\n",
        "    clusters = []\n",
        "    label_ids = []\n",
        "    for label in np.unique(labels):\n",
        "        if label == -1:\n",
        "            continue\n",
        "        members = X[labels == label]\n",
        "        if len(members) == 0:\n",
        "            continue\n",
        "        centroid = np.mean(members, axis=0)\n",
        "        clusters.append(centroid)\n",
        "        label_ids.append(label)\n",
        "    clusters = np.array(clusters)\n",
        "    if return_label_ids:\n",
        "        return clusters, label_ids\n",
        "    return clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INZiP-Q9dgAv"
      },
      "outputs": [],
      "source": [
        "def limit_clusters(labels, max_clusters):\n",
        "    \"\"\"Keep only the largest max_clusters and map the rest to -1.\"\"\"\n",
        "    if max_clusters is None:\n",
        "        return labels, np.unique(labels).tolist()\n",
        "    labels = np.asarray(labels)\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_counts = [\n",
        "        (label, count) for label, count in zip(unique, counts) if label != -1\n",
        "    ]\n",
        "    cluster_counts.sort(key=lambda item: item[1], reverse=True)\n",
        "    keep = [label for label, _ in cluster_counts[:max_clusters]]\n",
        "    if not keep:\n",
        "        return np.full_like(labels, -1), []\n",
        "    filtered = np.array([label if label in keep else -1 for label in labels], dtype=labels.dtype)\n",
        "    return filtered, keep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx0sni5RdgAv"
      },
      "outputs": [],
      "source": [
        "class ClusterAligner:\n",
        "    \"\"\"Keeps global cluster IDs and assigns consistent colors over time.\"\"\"\n",
        "\n",
        "    def __init__(self, max_clusters=100, similarity_threshold=0.8, cmap_name=\"gist_ncar\"):\n",
        "        self.max_clusters = max_clusters\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.centroids = []\n",
        "        self.global_ids = []\n",
        "        self.cmap = plt.cm.get_cmap(cmap_name, max_clusters)\n",
        "        self.palette = [self.cmap(i) for i in range(self.cmap.N)]\n",
        "        self.overflow_color = (0.65, 0.65, 0.65, 1.0)\n",
        "\n",
        "    def _add_centroid(self, centroid):\n",
        "        if len(self.global_ids) >= self.max_clusters:\n",
        "            return -1\n",
        "        new_id = len(self.global_ids)\n",
        "        self.centroids.append(centroid)\n",
        "        self.global_ids.append(new_id)\n",
        "        return new_id\n",
        "\n",
        "    def _match_or_create(self, centroid):\n",
        "        centroid = centroid.reshape(1, -1)\n",
        "        if not self.centroids:\n",
        "            return self._add_centroid(centroid)\n",
        "        stacked = np.vstack(self.centroids)\n",
        "        sims = cosine_similarity(stacked, centroid)[:, 0]\n",
        "        best_idx = int(np.argmax(sims))\n",
        "        if sims[best_idx] >= self.similarity_threshold:\n",
        "            return self.global_ids[best_idx]\n",
        "        return self._add_centroid(centroid)\n",
        "\n",
        "    def align(self, raw_labels, centroid_map):\n",
        "        aligned = np.full_like(raw_labels, -1)\n",
        "        for local_label, centroid in centroid_map.items():\n",
        "            global_id = self._match_or_create(centroid)\n",
        "            if global_id == -1:\n",
        "                continue\n",
        "            aligned[raw_labels == local_label] = global_id\n",
        "        return aligned\n",
        "\n",
        "    def get_color(self, label):\n",
        "        if 0 <= label < len(self.palette):\n",
        "            return self.palette[label]\n",
        "        return self.overflow_color\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHm_FQGcdgAv"
      },
      "outputs": [],
      "source": [
        "def create_cluster_guide(cluster_contexts_map, target_word, output_dir, aligner):\n",
        "    \"\"\"\n",
        "    Create a cluster guide with summary CSV and detailed context file.\n",
        "    Shows what each global cluster represents semantically.\n",
        "    \"\"\"\n",
        "\n",
        "    if not cluster_contexts_map:\n",
        "        print(\"  No clusters to document.\")\n",
        "        return\n",
        "\n",
        "    # Calculate statistics for each cluster\n",
        "    guide_rows = []\n",
        "    for global_id in sorted(cluster_contexts_map.keys()):\n",
        "        contexts = cluster_contexts_map[global_id]\n",
        "        term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "\n",
        "        guide_rows.append({\n",
        "            'global_id': global_id,\n",
        "            'color_index': global_id,\n",
        "            'total_contexts': len(contexts),\n",
        "            'term_year_span': ', '.join(term_years),\n",
        "            'num_appearances': len(term_years)\n",
        "        })\n",
        "\n",
        "    # Create summary CSV\n",
        "    df_summary = pd.DataFrame(guide_rows).sort_values('total_contexts', ascending=False)\n",
        "    summary_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_summary.csv\")\n",
        "    df_summary.to_csv(summary_path, index=False)\n",
        "    print(f\"  Saved cluster summary to {summary_path}\")\n",
        "\n",
        "    # Create detailed context file\n",
        "    context_file_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_contexts.txt\")\n",
        "    with open(context_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"{'='*80}\\n\")\n",
        "        f.write(f\"CLUSTER GUIDE FOR '{target_word.upper()}'\\n\")\n",
        "        f.write(f\"Generated: {pd.Timestamp.now()}\\n\")\n",
        "        f.write(f\"Total clusters: {len(cluster_contexts_map)}\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        # Sort clusters by total contexts (most common first)\n",
        "        for global_id in sorted(cluster_contexts_map.keys(),\n",
        "                               key=lambda x: len(cluster_contexts_map[x]),\n",
        "                               reverse=True):\n",
        "            contexts = cluster_contexts_map[global_id]\n",
        "            term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "\n",
        "            # Get color info\n",
        "            color = aligner.get_color(global_id)\n",
        "            color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "                int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "            )\n",
        "\n",
        "            f.write(f\"\\n{'='*80}\\n\")\n",
        "            f.write(f\"CLUSTER {global_id} (Color: {color_hex})\\n\")\n",
        "            f.write(f\"{'-'*80}\\n\")\n",
        "            f.write(f\"Total contexts: {len(contexts)}\\n\")\n",
        "            f.write(f\"Appearances: {len(term_years)} term-years\\n\")\n",
        "            f.write(f\"Term-year span: {', '.join(term_years)}\\n\")\n",
        "            f.write(f\"\\nREPRESENTATIVE CONTEXTS:\\n\")\n",
        "            f.write(f\"{'-'*80}\\n\")\n",
        "\n",
        "            # Show up to 15 diverse examples\n",
        "            shown = 0\n",
        "            for ctx_item in contexts[:15]:\n",
        "                f.write(f\"\\n[{ctx_item['term']}-{ctx_item['year']}] \")\n",
        "                f.write(ctx_item['context'][:250])\n",
        "                if len(ctx_item['context']) > 250:\n",
        "                    f.write(\"...\")\n",
        "                f.write(\"\\n\")\n",
        "                shown += 1\n",
        "\n",
        "            if len(contexts) > 15:\n",
        "                f.write(f\"\\n... and {len(contexts) - 15} more contexts\\n\")\n",
        "\n",
        "    print(f\"  Saved detailed contexts to {context_file_path}\")\n",
        "    print(f\"  Total clusters documented: {len(cluster_contexts_map)}\")\n",
        "\n",
        "    return df_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrdZONBSdgAv"
      },
      "outputs": [],
      "source": [
        "# Create Color Reference and Mapping\n",
        "def create_color_reference(df_timeline, target_word, output_dir, aligner):\n",
        "    \"\"\"Create color reference chart and mapping CSV.\"\"\"\n",
        "\n",
        "    if df_timeline is None or len(df_timeline) == 0:\n",
        "        print(\"  No timeline data to map.\")\n",
        "        return\n",
        "\n",
        "    # Get all global IDs from timeline\n",
        "    used_cluster_ids = sorted(df_timeline['global_id'].unique())\n",
        "\n",
        "    # Create color mapping CSV\n",
        "    color_mapping = []\n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "            int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "        )\n",
        "        color_rgb = f\"({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)})\"\n",
        "\n",
        "        color_mapping.append({\n",
        "            'global_id': global_id,\n",
        "            'hex_color': color_hex,\n",
        "            'rgb_color': color_rgb\n",
        "        })\n",
        "\n",
        "    df_colors = pd.DataFrame(color_mapping)\n",
        "    color_csv_path = os.path.join(output_dir, f'cluster_colors_{target_word}.csv')\n",
        "    df_colors.to_csv(color_csv_path, index=False)\n",
        "    print(f\"  Saved color mapping to {color_csv_path}\")\n",
        "\n",
        "    # Print console reference\n",
        "    print(\"\\n=== Color Reference ===\")\n",
        "    for _, row in df_colors.iterrows():\n",
        "        print(f\"Cluster {row['global_id']}: {row['hex_color']}\")\n",
        "\n",
        "    # Create visual color reference chart\n",
        "    import matplotlib.patches as mpatches\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, max(6, len(used_cluster_ids) // 4)))\n",
        "    patches = []\n",
        "\n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        patches.append(mpatches.Patch(color=color, label=f'Cluster {global_id}'))\n",
        "\n",
        "    ax.legend(handles=patches, loc='center', ncol=min(4, len(patches)), fontsize=10)\n",
        "    ax.axis('off')\n",
        "    plt.title(f\"Color Reference for '{target_word}' Clusters\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    chart_path = os.path.join(output_dir, f'color_reference_{target_word}.png')\n",
        "    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"  Saved color reference chart to {chart_path}\")\n",
        "\n",
        "    return df_colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5aFa_xbdgAv"
      },
      "outputs": [],
      "source": [
        "# ## Load Sentence Transformer Model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqYkfXgtdgAv"
      },
      "source": [
        "## Clustering and Tracking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRJFCb2edgAw"
      },
      "outputs": [],
      "source": [
        "# Data structure to track cluster evolution\n",
        "cluster_timeline = []  # List of dicts: {term, year, global_id, centroid, size, contexts}\n",
        "cluster_contexts_map = {}  # Store contexts per global_id for cluster guide\n",
        "\n",
        "aligner = ClusterAligner(max_clusters=MAX_CLUSTERS, similarity_threshold=SIMILARITY_THRESHOLD)\n",
        "baseline_used = False\n",
        "\n",
        "print(f\"\\n=== Analyzing '{TARGET_WORD}' across {len(TERM_YEAR_TUPLES)} term-year pairs ===\")\n",
        "\n",
        "for term, year in TERM_YEAR_TUPLES:\n",
        "    print(f\"\\n--- Term {term}, Year {year} ---\")\n",
        "    texts = fetch_speeches(term, year)\n",
        "    contexts = extract_contexts(texts, TARGET_WORD)\n",
        "    print(f\"  Contexts: {len(contexts)}\")\n",
        "\n",
        "    if len(contexts) < 10:\n",
        "        print(\"  Not enough contexts, skipping this slice.\")\n",
        "        continue\n",
        "\n",
        "    embeddings = compute_embeddings(model, contexts)\n",
        "    ap = AffinityPropagation(random_state=42)\n",
        "    ap.fit(embeddings)\n",
        "    local_labels = ap.labels_\n",
        "\n",
        "    cap = BASELINE_MAX_CLUSTERS if not baseline_used else MAX_CLUSTERS\n",
        "    limited_labels, kept_clusters = limit_clusters(local_labels, cap)\n",
        "    print(f\"  Raw clusters: {len(np.unique(local_labels))}, kept: {len(kept_clusters)} (cap={cap})\")\n",
        "\n",
        "    prototypes, proto_labels = get_cluster_prototypes(embeddings, limited_labels, return_label_ids=True)\n",
        "    centroid_map = dict(zip(proto_labels, prototypes))\n",
        "    if not centroid_map:\n",
        "        print(\"  No clusters survived filtering, skipping.\")\n",
        "        continue\n",
        "\n",
        "    baseline_used = True\n",
        "    aligned_labels = aligner.align(limited_labels, centroid_map)\n",
        "\n",
        "    # Count cluster sizes\n",
        "    cluster_sizes = {}\n",
        "    for label in aligned_labels:\n",
        "        if label >= 0:\n",
        "            cluster_sizes[label] = cluster_sizes.get(label, 0) + 1\n",
        "\n",
        "    # Get top-K clusters by size\n",
        "    top_clusters = sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True)[:TOP_K_CLUSTERS]\n",
        "    print(f\"  Top {len(top_clusters)} clusters: {top_clusters}\")\n",
        "\n",
        "    # Store cluster info and contexts\n",
        "    for global_id, size in top_clusters:\n",
        "        # Find the centroid for this global_id\n",
        "        centroid_idx = aligner.global_ids.index(global_id)\n",
        "        centroid = aligner.centroids[centroid_idx]\n",
        "\n",
        "        # Get contexts belonging to this global cluster (up to 10 examples)\n",
        "        cluster_context_examples = [\n",
        "            contexts[i] for i, label in enumerate(aligned_labels) if label == global_id\n",
        "        ]\n",
        "\n",
        "        # Store contexts for cluster guide\n",
        "        if global_id not in cluster_contexts_map:\n",
        "            cluster_contexts_map[global_id] = []\n",
        "        cluster_contexts_map[global_id].extend([\n",
        "            {'term': term, 'year': year, 'context': ctx}\n",
        "            for ctx in cluster_context_examples[:10]\n",
        "        ])\n",
        "\n",
        "        cluster_timeline.append({\n",
        "            'term': term,\n",
        "            'year': year,\n",
        "            'global_id': global_id,\n",
        "            'centroid': centroid,\n",
        "            'size': size,\n",
        "            'total_contexts': len(contexts)\n",
        "        })\n",
        "\n",
        "print(f\"\\n=== Collected {len(cluster_timeline)} cluster snapshots ===\")\n",
        "print(f\"=== Stored contexts for {len(cluster_contexts_map)} unique clusters ===\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENkvu3QpdgAw"
      },
      "outputs": [],
      "source": [
        "# Convert to DataFrame for easier manipulation\n",
        "df_timeline = pd.DataFrame(cluster_timeline)\n",
        "df_timeline['size_share'] = df_timeline['size'] / df_timeline['total_contexts']\n",
        "df_timeline['time_idx'] = df_timeline.groupby('global_id').cumcount()\n",
        "print(df_timeline.head(10))\n",
        "\n",
        "# Generate cluster guide\n",
        "print(\"\\n=== Generating Cluster Guide ===\")\n",
        "create_cluster_guide(cluster_contexts_map, TARGET_WORD, OUTPUT_DIR, aligner)\n",
        "\n",
        "# Generate color reference\n",
        "print(\"\\n=== Generating Color Reference ===\")\n",
        "create_color_reference(df_timeline, TARGET_WORD, OUTPUT_DIR, aligner)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnKg4n76dgAw"
      },
      "source": [
        "## Compute Lorenz Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_kPjH-VdgAw"
      },
      "outputs": [],
      "source": [
        "def compute_lorenz_coords(df_timeline):\n",
        "    \"\"\"Compute Lorenz coordinates (x, y, z) for each cluster snapshot.\"\"\"\n",
        "    lorenz_data = []\n",
        "\n",
        "    for global_id in df_timeline['global_id'].unique():\n",
        "        cluster_df = df_timeline[df_timeline['global_id'] == global_id].sort_values(['term', 'year'])\n",
        "\n",
        "        if len(cluster_df) == 0:\n",
        "            continue\n",
        "\n",
        "        baseline_centroid = cluster_df.iloc[0]['centroid']\n",
        "        prev_centroid = None\n",
        "\n",
        "        for idx, row in cluster_df.iterrows():\n",
        "            current_centroid = row['centroid']\n",
        "\n",
        "            # x: distance from baseline\n",
        "            baseline_sim = cosine_similarity(\n",
        "                current_centroid.reshape(1, -1),\n",
        "                baseline_centroid.reshape(1, -1)\n",
        "            )[0, 0]\n",
        "            x = 1.0 - baseline_sim\n",
        "\n",
        "            # y: local drift (distance from previous)\n",
        "            if prev_centroid is not None:\n",
        "                local_sim = cosine_similarity(\n",
        "                    current_centroid.reshape(1, -1),\n",
        "                    prev_centroid.reshape(1, -1)\n",
        "                )[0, 0]\n",
        "                y = 1.0 - local_sim\n",
        "            else:\n",
        "                y = 0.0\n",
        "\n",
        "            # z: size share\n",
        "            z = row['size_share']\n",
        "\n",
        "            lorenz_data.append({\n",
        "                'term': row['term'],\n",
        "                'year': row['year'],\n",
        "                'global_id': global_id,\n",
        "                'x': x,\n",
        "                'y': y,\n",
        "                'z': z,\n",
        "                'size': row['size']\n",
        "            })\n",
        "\n",
        "            prev_centroid = current_centroid\n",
        "\n",
        "    return pd.DataFrame(lorenz_data)\n",
        "\n",
        "df_lorenz = compute_lorenz_coords(df_timeline)\n",
        "print(df_lorenz.head(10))\n",
        "print(f\"\\nLorenz coordinates computed for {len(df_lorenz)} snapshots\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ0pMxdRdgAw"
      },
      "source": [
        "## Lorenz System Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hybZrn_dgAw"
      },
      "outputs": [],
      "source": [
        "def lorenz_system(state, t, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
        "    \"\"\"Classic Lorenz attractor equations.\"\"\"\n",
        "    x, y, z = state\n",
        "    dx = sigma * (y - x)\n",
        "    dy = x * (rho - z) - y\n",
        "    dz = x * y - beta * z\n",
        "    return [dx, dy, dz]\n",
        "\n",
        "def integrate_cluster_trajectory(cluster_data, dt=0.01, steps_per_point=50):\n",
        "    \"\"\"Integrate Lorenz system driven by cluster data.\"\"\"\n",
        "    trajectory = []\n",
        "\n",
        "    if len(cluster_data) == 0:\n",
        "        return np.array([])\n",
        "\n",
        "    # Initialize with first data point (scaled up for visibility)\n",
        "    x0 = cluster_data.iloc[0]['x'] * 10\n",
        "    y0 = cluster_data.iloc[0]['y'] * 10\n",
        "    z0 = cluster_data.iloc[0]['z'] * 30\n",
        "    state = [x0, y0, z0]\n",
        "\n",
        "    for idx, row in cluster_data.iterrows():\n",
        "        # Use data as perturbation/forcing\n",
        "        target_x = row['x'] * 10\n",
        "        target_y = row['y'] * 10\n",
        "        target_z = row['z'] * 30\n",
        "\n",
        "        # Integrate towards target with Lorenz dynamics\n",
        "        t_span = np.linspace(0, dt * steps_per_point, steps_per_point)\n",
        "\n",
        "        for step in range(steps_per_point):\n",
        "            # Add attraction towards data point\n",
        "            force_x = (target_x - state[0]) * 0.1\n",
        "            force_y = (target_y - state[1]) * 0.1\n",
        "            force_z = (target_z - state[2]) * 0.1\n",
        "\n",
        "            # Lorenz dynamics\n",
        "            d_state = lorenz_system(state, 0)\n",
        "\n",
        "            # Combine\n",
        "            state[0] += (d_state[0] + force_x) * dt\n",
        "            state[1] += (d_state[1] + force_y) * dt\n",
        "            state[2] += (d_state[2] + force_z) * dt\n",
        "\n",
        "            trajectory.append([\n",
        "                state[0], state[1], state[2],\n",
        "                row['term'], row['year'], row['global_id'], row['size']\n",
        "            ])\n",
        "\n",
        "    return np.array(trajectory)\n",
        "\n",
        "# Generate trajectories for each cluster\n",
        "all_trajectories = {}\n",
        "for global_id in df_lorenz['global_id'].unique():\n",
        "    cluster_data = df_lorenz[df_lorenz['global_id'] == global_id].sort_values(['term', 'year'])\n",
        "    traj = integrate_cluster_trajectory(cluster_data)\n",
        "    if len(traj) > 0:\n",
        "        all_trajectories[global_id] = traj\n",
        "        print(f\"Cluster {global_id}: {len(traj)} trajectory points\")\n",
        "\n",
        "print(f\"\\n=== Generated {len(all_trajectories)} cluster trajectories ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMzaUcs0dgAw"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9vMqBIIdgAw"
      },
      "source": [
        "## 3D Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-uTD2-ddgAw"
      },
      "outputs": [],
      "source": [
        "# Matplotlib 3D plot\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "for global_id, traj in all_trajectories.items():\n",
        "    color = aligner.get_color(global_id)\n",
        "    ax.plot(traj[:, 0], traj[:, 1], traj[:, 2],\n",
        "            color=color, alpha=0.7, linewidth=2, label=f'Cluster {global_id}')\n",
        "    # Mark start and end\n",
        "    ax.scatter(traj[0, 0], traj[0, 1], traj[0, 2],\n",
        "              color=color, s=100, marker='o', edgecolors='black', linewidths=2)\n",
        "    ax.scatter(traj[-1, 0], traj[-1, 1], traj[-1, 2],\n",
        "              color=color, s=100, marker='s', edgecolors='black', linewidths=2)\n",
        "\n",
        "ax.set_xlabel('X (Baseline Drift)', fontsize=12)\n",
        "ax.set_ylabel('Y (Local Drift)', fontsize=12)\n",
        "ax.set_zlabel('Z (Size Share)', fontsize=12)\n",
        "ax.set_title(f'Lorenz Attractor: Semantic Evolution of \"{TARGET_WORD}\"\\n({START_TERM},{1}) to ({END_TERM},{YEARS_PER_TERM})',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax.legend(loc='upper right', fontsize=8)\n",
        "ax.view_init(elev=20, azim=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(OUTPUT_DIR, f'lorenz_{TARGET_WORD}.png')\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved plot to {plot_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WEpNgH5dgAx"
      },
      "outputs": [],
      "source": [
        "# Interactive Plotly 3D plot\n",
        "fig_plotly = go.Figure()\n",
        "\n",
        "for global_id, traj in all_trajectories.items():\n",
        "    color_tuple = aligner.get_color(global_id)\n",
        "    color_str = f'rgba({int(color_tuple[0]*255)},{int(color_tuple[1]*255)},{int(color_tuple[2]*255)},{color_tuple[3]})'\n",
        "\n",
        "    # Trajectory line\n",
        "    fig_plotly.add_trace(go.Scatter3d(\n",
        "        x=traj[:, 0], y=traj[:, 1], z=traj[:, 2],\n",
        "        mode='lines',\n",
        "        line=dict(color=color_str, width=4),\n",
        "        name=f'Cluster {global_id}',\n",
        "        hovertext=[f'T{int(t)},Y{int(y)}' for t, y in zip(traj[:, 3], traj[:, 4])],\n",
        "        hoverinfo='text'\n",
        "    ))\n",
        "\n",
        "    # Start marker\n",
        "    fig_plotly.add_trace(go.Scatter3d(\n",
        "        x=[traj[0, 0]], y=[traj[0, 1]], z=[traj[0, 2]],\n",
        "        mode='markers',\n",
        "        marker=dict(size=8, color=color_str, symbol='circle', line=dict(color='black', width=2)),\n",
        "        name=f'Start {global_id}',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "    # End marker\n",
        "    fig_plotly.add_trace(go.Scatter3d(\n",
        "        x=[traj[-1, 0]], y=[traj[-1, 1]], z=[traj[-1, 2]],\n",
        "        mode='markers',\n",
        "        marker=dict(size=8, color=color_str, symbol='square', line=dict(color='black', width=2)),\n",
        "        name=f'End {global_id}',\n",
        "        showlegend=False\n",
        "    ))\n",
        "\n",
        "fig_plotly.update_layout(\n",
        "    title=f'Lorenz Attractor: Semantic Evolution of \"{TARGET_WORD}\"',\n",
        "    scene=dict(\n",
        "        xaxis_title='X (Baseline Drift)',\n",
        "        yaxis_title='Y (Local Drift)',\n",
        "        zaxis_title='Z (Size Share)'\n",
        "    ),\n",
        "    width=1000,\n",
        "    height=800\n",
        ")\n",
        "\n",
        "html_path = os.path.join(OUTPUT_DIR, f'lorenz_{TARGET_WORD}_interactive.html')\n",
        "fig_plotly.write_html(html_path)\n",
        "print(f\"Saved interactive plot to {html_path}\")\n",
        "fig_plotly.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCKueHOgdgAx"
      },
      "source": [
        "## Animated Plotly Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwR5jF4fdgAx"
      },
      "outputs": [],
      "source": [
        "# Create animated Plotly visualization with frame-by-frame progression\n",
        "def create_animated_lorenz(all_trajectories, aligner, target_word, output_dir, frame_step=5):\n",
        "    \"\"\"\n",
        "    Create an animated 3D plot where trajectories progressively reveal over time.\n",
        "    frame_step: how many integration steps to advance per frame (lower = smoother but more frames)\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine max trajectory length for frame count\n",
        "    max_len = max(len(traj) for traj in all_trajectories.values())\n",
        "    num_frames = max_len // frame_step\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    for frame_idx in range(0, max_len, frame_step):\n",
        "        frame_data = []\n",
        "\n",
        "        for global_id, traj in all_trajectories.items():\n",
        "            color_tuple = aligner.get_color(global_id)\n",
        "            color_str = f'rgba({int(color_tuple[0]*255)},{int(color_tuple[1]*255)},{int(color_tuple[2]*255)},{color_tuple[3]})'\n",
        "\n",
        "            # Get trajectory up to current frame\n",
        "            end_idx = min(frame_idx + 1, len(traj))\n",
        "            if end_idx == 0:\n",
        "                continue\n",
        "\n",
        "            current_traj = traj[:end_idx]\n",
        "\n",
        "            # Trajectory line (growing)\n",
        "            frame_data.append(go.Scatter3d(\n",
        "                x=current_traj[:, 0],\n",
        "                y=current_traj[:, 1],\n",
        "                z=current_traj[:, 2],\n",
        "                mode='lines',\n",
        "                line=dict(color=color_str, width=4),\n",
        "                name=f'Cluster {global_id}',\n",
        "                hovertext=[f'T{int(t)},Y{int(y)}' for t, y in zip(current_traj[:, 3], current_traj[:, 4])],\n",
        "                hoverinfo='text',\n",
        "                showlegend=(frame_idx == 0)\n",
        "            ))\n",
        "\n",
        "            # Current position marker (moving point)\n",
        "            if end_idx > 0:\n",
        "                frame_data.append(go.Scatter3d(\n",
        "                    x=[current_traj[-1, 0]],\n",
        "                    y=[current_traj[-1, 1]],\n",
        "                    z=[current_traj[-1, 2]],\n",
        "                    mode='markers',\n",
        "                    marker=dict(size=10, color=color_str, symbol='diamond',\n",
        "                               line=dict(color='white', width=2)),\n",
        "                    name=f'Current {global_id}',\n",
        "                    showlegend=False\n",
        "                ))\n",
        "\n",
        "        frames.append(go.Frame(data=frame_data, name=str(frame_idx)))\n",
        "\n",
        "    # Initial frame (empty or first frame)\n",
        "    initial_data = frames[0].data if frames else []\n",
        "\n",
        "    # Create figure\n",
        "    fig_anim = go.Figure(\n",
        "        data=initial_data,\n",
        "        frames=frames\n",
        "    )\n",
        "\n",
        "    # Add play/pause buttons and slider\n",
        "    fig_anim.update_layout(\n",
        "        title=dict(\n",
        "            text=f'Lorenz Attractor: Semantic Evolution of \"{target_word}\" (Animated)',\n",
        "            font=dict(size=18)\n",
        "        ),\n",
        "        scene=dict(\n",
        "            xaxis_title='X (Baseline Drift)',\n",
        "            yaxis_title='Y (Local Drift)',\n",
        "            zaxis_title='Z (Size Share)',\n",
        "            camera=dict(\n",
        "                eye=dict(x=1.5, y=1.5, z=1.2)\n",
        "            )\n",
        "        ),\n",
        "        width=1200,\n",
        "        height=900,\n",
        "        updatemenus=[\n",
        "            dict(\n",
        "                type='buttons',\n",
        "                showactive=False,\n",
        "                buttons=[\n",
        "                    dict(\n",
        "                        label='Play',\n",
        "                        method='animate',\n",
        "                        args=[None, dict(\n",
        "                            frame=dict(duration=50, redraw=True),\n",
        "                            fromcurrent=True,\n",
        "                            mode='immediate',\n",
        "                            transition=dict(duration=0)\n",
        "                        )]\n",
        "                    ),\n",
        "                    dict(\n",
        "                        label='Pause',\n",
        "                        method='animate',\n",
        "                        args=[[None], dict(\n",
        "                            frame=dict(duration=0, redraw=False),\n",
        "                            mode='immediate',\n",
        "                            transition=dict(duration=0)\n",
        "                        )]\n",
        "                    )\n",
        "                ],\n",
        "                x=0.1, y=0.0, xanchor='left', yanchor='bottom'\n",
        "            )\n",
        "        ],\n",
        "        sliders=[\n",
        "            dict(\n",
        "                active=0,\n",
        "                steps=[\n",
        "                    dict(\n",
        "                        args=[[f.name], dict(\n",
        "                            frame=dict(duration=0, redraw=True),\n",
        "                            mode='immediate',\n",
        "                            transition=dict(duration=0)\n",
        "                        )],\n",
        "                        label=str(i),\n",
        "                        method='animate'\n",
        "                    ) for i, f in enumerate(frames)\n",
        "                ],\n",
        "                x=0.1, y=0.0, len=0.9, xanchor='left', yanchor='top',\n",
        "                currentvalue=dict(\n",
        "                    visible=True,\n",
        "                    prefix='Frame: ',\n",
        "                    xanchor='right'\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Save and display\n",
        "    html_path = os.path.join(output_dir, f'lorenz_{target_word}_animated.html')\n",
        "    fig_anim.write_html(html_path)\n",
        "    print(f\"Saved animated plot to {html_path}\")\n",
        "\n",
        "    return fig_anim\n",
        "\n",
        "# Generate animation\n",
        "fig_animated = create_animated_lorenz(all_trajectories, aligner, TARGET_WORD, OUTPUT_DIR, frame_step=10)\n",
        "fig_animated.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DH2FTNJdgAx"
      },
      "outputs": [],
      "source": [
        "# Save data to CSV\n",
        "csv_path = os.path.join(OUTPUT_DIR, f'lorenz_data_{TARGET_WORD}.csv')\n",
        "df_lorenz.to_csv(csv_path, index=False)\n",
        "print(f\"Saved Lorenz coordinates to {csv_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}