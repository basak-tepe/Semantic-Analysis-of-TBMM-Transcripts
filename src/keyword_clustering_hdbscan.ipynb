{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Embedding & HDBSCAN Topic Clustering\n",
    "\n",
    "This notebook:\n",
    "1. Embeds speech keywords using Turkish sentence-transformers model\n",
    "2. Clusters speeches using HDBSCAN algorithm\n",
    "3. Generates topic labels based on representative keywords\n",
    "4. Uploads new topic assignments to Elasticsearch\n",
    "5. Visualizes clusters with UMAP\n",
    "\n",
    "**Model**: `trmteb/turkish-embedding-model-fine-tuned` (768-dimensional embeddings)\n",
    "\n",
    "**HDBSCAN**: Hierarchical density-based clustering (no need to specify number of clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Tuple\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Sentence transformers for embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Clustering\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "import umap\n",
    "\n",
    "# Elasticsearch\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "KEYWORDS_CSV = \"../data/speech_keywords.csv\"\n",
    "EMBEDDINGS_FILE = \"../data/keyword_embeddings.npy\"\n",
    "OUTPUT_CSV = \"../data/speech_keywords_with_topics.csv\"\n",
    "MODEL_NAME = \"trmteb/turkish-embedding-model-fine-tuned\"\n",
    "ELASTICSEARCH_HOST = os.getenv(\"ELASTICSEARCH_HOST\", \"http://localhost:9200\")\n",
    "ELASTICSEARCH_INDEX = os.getenv(\"ELASTICSEARCH_INDEX\", \"parliament_speeches\")\n",
    "\n",
    "print(\"âœ… Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Keywords Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KEYWORDS_CSV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the CSV with keywords\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¥ Loading keywords from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mKEYWORDS_CSV\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m df = pd.read_csv(KEYWORDS_CSV)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“Š Dataset info:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'KEYWORDS_CSV' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the CSV with keywords\n",
    "print(f\"ðŸ“¥ Loading keywords from {KEYWORDS_CSV}...\")\n",
    "df = pd.read_csv(KEYWORDS_CSV)\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset info:\")\n",
    "print(f\"   Total speeches: {len(df):,}\")\n",
    "print(f\"   Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nðŸ“‹ Sample data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Turkish Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸ”„ Loading model: {MODEL_NAME}...\")\n",
    "print(\"   This may take a few minutes on first run...\\n\")\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Check model details\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"âœ… Model loaded successfully!\")\n",
    "print(f\"   Embedding dimension: {embedding_dim}\")\n",
    "print(f\"   Max sequence length: {model.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Keyword Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(keywords_list: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of keyword strings.\n",
    "    \n",
    "    Args:\n",
    "        keywords_list: List of comma-separated keyword strings\n",
    "        batch_size: Batch size for encoding\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (n_samples, embedding_dim)\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”„ Generating embeddings for {len(keywords_list):,} speeches...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    # Encode in batches with progress bar\n",
    "    embeddings = model.encode(\n",
    "        keywords_list,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Embeddings generated!\")\n",
    "    print(f\"   Shape: {embeddings.shape}\")\n",
    "    print(f\"   Memory: {embeddings.nbytes / 1e6:.2f} MB\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if os.path.exists(EMBEDDINGS_FILE):\n",
    "    print(f\"\\nðŸ“‚ Found existing embeddings at {EMBEDDINGS_FILE}\")\n",
    "    response = input(\"Load existing embeddings? (y/n): \")\n",
    "    \n",
    "    if response.lower() == 'y':\n",
    "        embeddings = np.load(EMBEDDINGS_FILE)\n",
    "        print(f\"âœ… Loaded embeddings: {embeddings.shape}\")\n",
    "    else:\n",
    "        embeddings = generate_embeddings(df['keywords'].tolist())\n",
    "        np.save(EMBEDDINGS_FILE, embeddings)\n",
    "        print(f\"ðŸ’¾ Saved embeddings to {EMBEDDINGS_FILE}\")\n",
    "else:\n",
    "    embeddings = generate_embeddings(df['keywords'].tolist())\n",
    "    np.save(EMBEDDINGS_FILE, embeddings)\n",
    "    print(f\"ðŸ’¾ Saved embeddings to {EMBEDDINGS_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_hdbscan(\n",
    "    embeddings: np.ndarray,\n",
    "    min_cluster_size: int = 50,\n",
    "    min_samples: int = 10,\n",
    "    metric: str = 'euclidean',\n",
    "    cluster_selection_epsilon: float = 0.0\n",
    ") -> Tuple[np.ndarray, hdbscan.HDBSCAN]:\n",
    "    \"\"\"\n",
    "    Cluster embeddings using HDBSCAN.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: numpy array of embeddings\n",
    "        min_cluster_size: minimum size of clusters\n",
    "        min_samples: number of samples in a neighborhood\n",
    "        metric: distance metric to use\n",
    "        cluster_selection_epsilon: distance threshold for merging clusters\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (cluster_labels, clusterer_object)\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”„ Running HDBSCAN clustering...\")\n",
    "    print(f\"   Parameters:\")\n",
    "    print(f\"   - min_cluster_size: {min_cluster_size}\")\n",
    "    print(f\"   - min_samples: {min_samples}\")\n",
    "    print(f\"   - metric: {metric}\")\n",
    "    print(f\"   - cluster_selection_epsilon: {cluster_selection_epsilon}\")\n",
    "    \n",
    "    # Create and fit clusterer\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon,\n",
    "        prediction_data=True\n",
    "    )\n",
    "    \n",
    "    labels = clusterer.fit_predict(embeddings)\n",
    "    \n",
    "    # Statistics\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_outliers = np.sum(labels == -1)\n",
    "    \n",
    "    print(f\"\\nâœ… Clustering complete!\")\n",
    "    print(f\"   Number of clusters: {n_clusters}\")\n",
    "    print(f\"   Outliers: {n_outliers:,} ({n_outliers/len(labels)*100:.1f}%)\")\n",
    "    print(f\"   Clustered speeches: {len(labels) - n_outliers:,}\")\n",
    "    \n",
    "    return labels, clusterer\n",
    "\n",
    "# Run clustering with default parameters\n",
    "# Adjust these parameters based on your data\n",
    "cluster_labels, clusterer = cluster_with_hdbscan(\n",
    "    embeddings,\n",
    "    min_cluster_size=50,  # Minimum 50 speeches per cluster\n",
    "    min_samples=15,       # At least 15 neighbors to form core point\n",
    "    metric='euclidean'    # Euclidean distance in embedding space\n",
    ")\n",
    "\n",
    "# Add to dataframe\n",
    "df['hdbscan_topic_id'] = cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cluster Statistics & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster size distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "\n",
    "print(\"ðŸ“Š Cluster Size Distribution:\\n\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"   Cluster {cluster_id:3d} (Outliers): {count:6,} speeches\")\n",
    "    else:\n",
    "        print(f\"   Cluster {cluster_id:3d}: {count:6,} speeches\")\n",
    "\n",
    "# Plot cluster distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "cluster_counts_no_outliers = cluster_counts[cluster_counts.index != -1]\n",
    "plt.bar(range(len(cluster_counts_no_outliers)), cluster_counts_no_outliers.values)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Speeches')\n",
    "plt.title('Cluster Size Distribution (excluding outliers)')\n",
    "plt.xticks(range(0, len(cluster_counts_no_outliers), max(1, len(cluster_counts_no_outliers)//20)))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cluster_counts_no_outliers.values, bins=30, edgecolor='black')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cluster Sizes')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nðŸ“ˆ Summary Statistics (excluding outliers):\")\n",
    "print(f\"   Mean cluster size: {cluster_counts_no_outliers.mean():.1f}\")\n",
    "print(f\"   Median cluster size: {cluster_counts_no_outliers.median():.1f}\")\n",
    "print(f\"   Largest cluster: {cluster_counts_no_outliers.max():,} speeches\")\n",
    "print(f\"   Smallest cluster: {cluster_counts_no_outliers.min():,} speeches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Topic Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_labels(df: pd.DataFrame, n_keywords: int = 5) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Generate topic labels based on most common keywords in each cluster.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with 'hdbscan_topic_id' and 'keywords' columns\n",
    "        n_keywords: Number of top keywords to use for label\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping cluster_id to topic label\n",
    "    \"\"\"\n",
    "    topic_labels = {}\n",
    "    \n",
    "    for cluster_id in sorted(df['hdbscan_topic_id'].unique()):\n",
    "        # Get all speeches in this cluster\n",
    "        cluster_speeches = df[df['hdbscan_topic_id'] == cluster_id]\n",
    "        \n",
    "        # Extract all keywords from this cluster\n",
    "        all_keywords = []\n",
    "        for keywords_str in cluster_speeches['keywords']:\n",
    "            if pd.notna(keywords_str):\n",
    "                keywords = [k.strip() for k in str(keywords_str).split(',')]\n",
    "                all_keywords.extend(keywords)\n",
    "        \n",
    "        # Count keyword frequencies\n",
    "        keyword_counts = Counter(all_keywords)\n",
    "        \n",
    "        # Get top N keywords\n",
    "        top_keywords = [kw for kw, count in keyword_counts.most_common(n_keywords)]\n",
    "        \n",
    "        # Create label\n",
    "        if cluster_id == -1:\n",
    "            topic_labels[cluster_id] = \"Outliers\"\n",
    "        else:\n",
    "            topic_labels[cluster_id] = \", \".join(top_keywords)\n",
    "    \n",
    "    return topic_labels\n",
    "\n",
    "print(\"ðŸ·ï¸  Generating topic labels...\\n\")\n",
    "topic_labels = generate_topic_labels(df, n_keywords=5)\n",
    "\n",
    "# Add labels to dataframe\n",
    "df['hdbscan_topic_label'] = df['hdbscan_topic_id'].map(topic_labels)\n",
    "\n",
    "# Display topics\n",
    "print(\"ðŸ“‹ Generated Topics:\\n\")\n",
    "for cluster_id, label in sorted(topic_labels.items()):\n",
    "    count = len(df[df['hdbscan_topic_id'] == cluster_id])\n",
    "    if cluster_id == -1:\n",
    "        print(f\"   Topic {cluster_id:3d} ({count:6,} speeches): {label}\")\n",
    "    else:\n",
    "        print(f\"   Topic {cluster_id:3d} ({count:6,} speeches): {label[:80]}...\" if len(label) > 80 else f\"   Topic {cluster_id:3d} ({count:6,} speeches): {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Clusters with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”„ Reducing dimensions with UMAP for visualization...\")\n",
    "print(\"   This may take a few minutes...\\n\")\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "embeddings_2d = reducer.fit_transform(embeddings)\n",
    "\n",
    "print(\"âœ… Dimensionality reduction complete!\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Get unique clusters (excluding outliers for color assignment)\n",
    "unique_clusters = sorted([c for c in df['hdbscan_topic_id'].unique() if c != -1])\n",
    "n_clusters = len(unique_clusters)\n",
    "\n",
    "# Create color map\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, min(n_clusters, 20)))\n",
    "if n_clusters > 20:\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "# Plot each cluster\n",
    "for idx, cluster_id in enumerate(unique_clusters):\n",
    "    mask = df['hdbscan_topic_id'] == cluster_id\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0],\n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[colors[idx % len(colors)]],\n",
    "        label=f'Topic {cluster_id}' if idx < 20 else '',\n",
    "        alpha=0.6,\n",
    "        s=10\n",
    "    )\n",
    "\n",
    "# Plot outliers\n",
    "outlier_mask = df['hdbscan_topic_id'] == -1\n",
    "if outlier_mask.sum() > 0:\n",
    "    plt.scatter(\n",
    "        embeddings_2d[outlier_mask, 0],\n",
    "        embeddings_2d[outlier_mask, 1],\n",
    "        c='gray',\n",
    "        label='Outliers',\n",
    "        alpha=0.3,\n",
    "        s=5\n",
    "    )\n",
    "\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.title(f'HDBSCAN Clusters Visualized with UMAP ({n_clusters} topics)')\n",
    "if n_clusters <= 20:\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', markerscale=2)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample speeches from largest clusters\n",
    "print(\"ðŸ“‹ Sample Speeches from Top 5 Largest Clusters:\\n\")\n",
    "\n",
    "top_clusters = df['hdbscan_topic_id'].value_counts().head(6)\n",
    "top_clusters = top_clusters[top_clusters.index != -1].head(5)\n",
    "\n",
    "for cluster_id in top_clusters.index:\n",
    "    cluster_df = df[df['hdbscan_topic_id'] == cluster_id]\n",
    "    topic_label = topic_labels[cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Topic {cluster_id}: {topic_label}\")\n",
    "    print(f\"Size: {len(cluster_df):,} speeches\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Show 3 sample speeches\n",
    "    samples = cluster_df.sample(min(3, len(cluster_df)))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"\\n{idx}. Speech ID: {row['speech_id']}\")\n",
    "        print(f\"   Speaker: {row.get('speech_giver', 'N/A')}\")\n",
    "        print(f\"   Keywords: {row['keywords'][:100]}...\" if len(str(row['keywords'])) > 100 else f\"   Keywords: {row['keywords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated dataframe with topic assignments\n",
    "print(f\"ðŸ’¾ Saving results to {OUTPUT_CSV}...\")\n",
    "\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Saved! CSV contains:\")\n",
    "print(f\"   - speech_id\")\n",
    "print(f\"   - keywords\")\n",
    "print(f\"   - hdbscan_topic_id\")\n",
    "print(f\"   - hdbscan_topic_label\")\n",
    "print(f\"   - Other metadata columns\")\n",
    "\n",
    "print(f\"\\nðŸ“Š File info:\")\n",
    "print(f\"   Rows: {len(df):,}\")\n",
    "print(f\"   Columns: {len(df.columns)}\")\n",
    "print(f\"   Size: {os.path.getsize(OUTPUT_CSV) / 1e6:.2f} MB\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nðŸ“‹ Sample output:\")\n",
    "df[['speech_id', 'hdbscan_topic_id', 'hdbscan_topic_label', 'keywords']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Upload to Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_topics_to_elasticsearch(\n",
    "    df: pd.DataFrame,\n",
    "    es_host: str,\n",
    "    es_index: str,\n",
    "    batch_size: int = 100\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Upload HDBSCAN topic assignments to Elasticsearch.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with speech_id, hdbscan_topic_id, hdbscan_topic_label\n",
    "        es_host: Elasticsearch host URL\n",
    "        es_index: Index name\n",
    "        batch_size: Number of documents to upload at once\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success_count, failure_count)\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”Œ Connecting to Elasticsearch at {es_host}...\")\n",
    "    \n",
    "    try:\n",
    "        es = Elasticsearch(hosts=[es_host])\n",
    "        \n",
    "        if not es.ping():\n",
    "            print(\"âŒ Could not connect to Elasticsearch\")\n",
    "            return 0, 0\n",
    "        \n",
    "        print(f\"âœ… Connected to Elasticsearch\")\n",
    "        print(f\"\\nðŸ’¾ Uploading {len(df):,} topic assignments...\")\n",
    "        \n",
    "        # Prepare bulk update actions\n",
    "        actions = []\n",
    "        for _, row in df.iterrows():\n",
    "            actions.append({\n",
    "                '_op_type': 'update',\n",
    "                '_index': es_index,\n",
    "                '_id': row['speech_id'],\n",
    "                'doc': {\n",
    "                    'hdbscan_topic_id': int(row['hdbscan_topic_id']),\n",
    "                    'hdbscan_topic_label': str(row['hdbscan_topic_label'])\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        # Upload in batches\n",
    "        success_count = 0\n",
    "        failure_count = 0\n",
    "        \n",
    "        for i in tqdm(range(0, len(actions), batch_size), desc=\"Uploading\"):\n",
    "            batch = actions[i:i+batch_size]\n",
    "            success, failed = helpers.bulk(es, batch, raise_on_error=False)\n",
    "            success_count += success\n",
    "            failure_count += len(failed) if failed else 0\n",
    "        \n",
    "        print(f\"\\nâœ… Upload complete!\")\n",
    "        print(f\"   Success: {success_count:,}\")\n",
    "        print(f\"   Failures: {failure_count:,}\")\n",
    "        \n",
    "        return success_count, failure_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        return 0, 0\n",
    "\n",
    "# Upload to Elasticsearch\n",
    "success, failures = upload_topics_to_elasticsearch(\n",
    "    df,\n",
    "    ELASTICSEARCH_HOST,\n",
    "    ELASTICSEARCH_INDEX,\n",
    "    batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HDBSCAN CLUSTERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "n_clusters = len(df['hdbscan_topic_id'].unique()) - (1 if -1 in df['hdbscan_topic_id'].values else 0)\n",
    "n_outliers = (df['hdbscan_topic_id'] == -1).sum()\n",
    "n_clustered = len(df) - n_outliers\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Statistics:\")\n",
    "print(f\"   Total speeches: {len(df):,}\")\n",
    "print(f\"   Number of topics: {n_clusters}\")\n",
    "print(f\"   Clustered speeches: {n_clustered:,} ({n_clustered/len(df)*100:.1f}%)\")\n",
    "print(f\"   Outliers: {n_outliers:,} ({n_outliers/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ Output Files:\")\n",
    "print(f\"   Embeddings: {EMBEDDINGS_FILE}\")\n",
    "print(f\"   Results CSV: {OUTPUT_CSV}\")\n",
    "\n",
    "print(f\"\\nðŸ” Elasticsearch:\")\n",
    "print(f\"   Successfully uploaded: {success:,} speeches\")\n",
    "print(f\"   New fields added: hdbscan_topic_id, hdbscan_topic_label\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top 10 Largest Topics:\")\n",
    "top_10 = df['hdbscan_topic_id'].value_counts().head(10)\n",
    "for cluster_id, count in top_10.items():\n",
    "    label = topic_labels.get(cluster_id, 'Unknown')\n",
    "    if cluster_id == -1:\n",
    "        print(f\"   Topic {cluster_id:3d}: {count:6,} speeches - {label}\")\n",
    "    else:\n",
    "        print(f\"   Topic {cluster_id:3d}: {count:6,} speeches - {label[:60]}...\" if len(label) > 60 else f\"   Topic {cluster_id:3d}: {count:6,} speeches - {label}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… CLUSTERING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv12",
   "language": "python",
   "name": "myenv12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
