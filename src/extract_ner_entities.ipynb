{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyZIzZt0FCuZ"
      },
      "source": [
        "# NER Entity Extraction from Parliamentary Speeches\n",
        "\n",
        "This notebook extracts Named Entity Recognition (NER) entities from Turkish parliamentary speeches and stores them in Elasticsearch.\n",
        "\n",
        "## ‚ö° PERFORMANCE OPTIMIZATIONS:\n",
        "- **Batch Processing**: Processes multiple texts simultaneously (64-128 texts per batch)\n",
        "- **GPU Acceleration**: Automatically uses GPU if available (CUDA)\n",
        "- **Large ES Batches**: Bulk updates 500-1000 documents at once\n",
        "- **Memory Efficient**: Processes in chunks to handle large datasets\n",
        "- **Parallel Processing**: Ready for Wikipedia linking parallelization\n",
        "\n",
        "## Workflow:\n",
        "1. Connect to Elasticsearch (local or GCP VM)\n",
        "2. Load TerminatorPower/nerT Turkish NER model (with GPU support)\n",
        "3. Process all speeches in batches and extract entities (PERSON, LOCATION, ORGANIZATION)\n",
        "4. Optionally link entities to Wikipedia via Wikidata API\n",
        "5. Update Elasticsearch documents with `ner_entities` field using bulk operations\n",
        "\n",
        "## Features:\n",
        "- **NER Model**: TerminatorPower/nerT (Turkish language model)\n",
        "- **Entity Types**: PERSON, LOCATION, ORGANIZATION\n",
        "- **Wikipedia Linking**: Optional Wikidata API integration\n",
        "- **Caching**: Wikipedia lookups are cached to avoid redundant API calls\n",
        "- **Batch Processing**: Processes 64+ texts simultaneously for 10-50x speedup\n",
        "- **GPU Support**: Automatic GPU detection and usage for faster inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-mFTnORFCub"
      },
      "source": [
        "## 1. Installation & Setup\n",
        "\n",
        "Install required packages (run this first):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VakK8DDYFCub"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q transformers \"elasticsearch==8.6.2\" requests tqdm torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjoR8IQyFCub"
      },
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Set your Elasticsearch connection details. For local development, use `localhost:9200`. For GCP VM, use the VM's IP address.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB4s1q5tFCuc",
        "outputId": "813f2ae3-1e1e-45a8-eb96-cadd1e9ef991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Elasticsearch Host: https://cab-teach-src-oven.trycloudflare.com\n",
            "üìä Index Name: parliament_speeches\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Add parent directory to path for imports\n",
        "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
        "\n",
        "# Configuration\n",
        "# For local: use \"http://localhost:9200\"\n",
        "# For GCP VM: use \"http://VM_IP:9200\"\n",
        "ELASTICSEARCH_HOST = os.getenv(\"ELASTICSEARCH_HOST\", \"https://cab-teach-src-oven.trycloudflare.com\")\n",
        "ELASTICSEARCH_INDEX = os.getenv(\"ELASTICSEARCH_INDEX\", \"parliament_speeches\")\n",
        "\n",
        "print(f\"üì° Elasticsearch Host: {ELASTICSEARCH_HOST}\")\n",
        "print(f\"üìä Index Name: {ELASTICSEARCH_INDEX}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hix3FSItFCuc"
      },
      "source": [
        "## 3. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOROO-JMFCuc",
        "outputId": "fe14a441-70a4-4837-ba6b-6956628fc24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n",
            "üîß Device: GPU (CUDA)\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "import time\n",
        "import warnings\n",
        "from typing import List, Dict, Any, Optional\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch.helpers import scan, bulk\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Suppress NumPy 2.0 deprecation warnings from transformers library\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\".*np.float_.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\".*np.int_.*\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\".*np.complex_.*\")\n",
        "\n",
        "# Wikidata API headers\n",
        "HEADERS = {\n",
        "    # Optional: Add User-Agent header\n",
        "     \"User-Agent\": \"Turkish-NEL-Research/1.0\"\n",
        "}\n",
        "\n",
        "# Check for GPU availability\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üîß Device: {'GPU (CUDA)' if device == 0 else 'CPU'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saiG-kxnFCuc"
      },
      "source": [
        "## 4. Helper Functions\n",
        "\n",
        "Define functions for Wikidata search, entity extraction, and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9CxwVDkFCud",
        "outputId": "d676ce4b-7f21-4879-8d22-9953f80cc7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Helper functions defined!\n"
          ]
        }
      ],
      "source": [
        "def wikidata_search(entity: str, lang: str = \"tr\", limit: int = 1, sleep: float = 2.0, max_retries: int = 3) -> List[Dict]:\n",
        "    \"\"\"Search for entity in Wikidata with rate limiting and retries.\"\"\"\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": entity,\n",
        "        \"language\": lang,\n",
        "        \"format\": \"json\",\n",
        "        \"limit\": limit\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(url, params=params, headers=HEADERS, timeout=10)\n",
        "\n",
        "            # Handle rate limiting\n",
        "            if r.status_code == 429:\n",
        "                wait_time = (2 ** attempt) * 5  # Exponential backoff: 5s, 10s, 20s\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"[Wikidata 429] Rate limited. Waiting {wait_time}s before retry {attempt + 1}/{max_retries}...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"[Wikidata 429] Entity: {entity} | Max retries reached. Skipping.\")\n",
        "                    return []\n",
        "\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            time.sleep(sleep)  # Standard delay between requests\n",
        "            return data.get(\"search\", [])\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 403:\n",
        "                print(f\"[Wikidata 403] Entity: {entity} | Add User-Agent header\")\n",
        "            elif e.response.status_code == 429:\n",
        "                wait_time = (2 ** attempt) * 5\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"[Wikidata 429] Rate limited. Waiting {wait_time}s...\")\n",
        "                    time.sleep(wait_time)\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"[Wikidata 429] Entity: {entity} | Max retries reached. Skipping.\")\n",
        "            else:\n",
        "                print(f\"[Wikidata error] Entity: {entity} | {e}\")\n",
        "            return []\n",
        "        except Exception as e:\n",
        "            print(f\"[Wikidata error] Entity: {entity} | {e}\")\n",
        "            return []\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "def wikidata_to_wikipedia(qid: str, lang: str = \"tr\") -> Optional[str]:\n",
        "    \"\"\"Convert Wikidata QID to Wikipedia URL.\"\"\"\n",
        "    url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbgetentities\",\n",
        "        \"ids\": qid,\n",
        "        \"props\": \"sitelinks\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        entity = data[\"entities\"][qid]\n",
        "        key = f\"{lang}wiki\"\n",
        "        return entity[\"sitelinks\"][key][\"url\"] if key in entity[\"sitelinks\"] else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def aggregate_tokens(entities: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Aggregate subword tokens (handle ## prefixes from BERT tokenization).\"\"\"\n",
        "    if not entities:\n",
        "        return []\n",
        "\n",
        "    merged = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(entities):\n",
        "        token = entities[i]\n",
        "        word = token[\"word\"]\n",
        "        entity_group = token[\"entity_group\"]\n",
        "        score = token[\"score\"]\n",
        "        start = token[\"start\"]\n",
        "        end = token[\"end\"]\n",
        "\n",
        "        # Keep merging consecutive ## tokens of the same entity type\n",
        "        j = i + 1\n",
        "        while j < len(entities):\n",
        "            next_token = entities[j]\n",
        "            # Check if next token is a subword (starts with ##) and same entity group\n",
        "            if next_token[\"word\"].startswith(\"##\") and next_token[\"entity_group\"] == entity_group:\n",
        "                word += next_token[\"word\"][2:]  # remove ##\n",
        "                end = next_token[\"end\"]\n",
        "                # Average the scores\n",
        "                score = (score + next_token[\"score\"]) / 2\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        merged.append({\n",
        "            \"entity_group\": entity_group,\n",
        "            \"word\": word,\n",
        "            \"score\": score,\n",
        "            \"start\": start,\n",
        "            \"end\": end\n",
        "        })\n",
        "\n",
        "        i = j if j > i + 1 else i + 1\n",
        "\n",
        "    return merged\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG4SbuN3FCud"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9ni9Tk9FCud",
        "outputId": "13ac8ecd-9457-4873-db57-c725eead331e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Entity extraction functions defined!\n"
          ]
        }
      ],
      "source": [
        "def extract_entities(text: str, ner_pipeline) -> List[Dict]:\n",
        "    \"\"\"Extract entities from text using NER pipeline (single text).\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        # Suppress warnings for this specific call\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "            raw_entities = ner_pipeline(text)\n",
        "\n",
        "        if not raw_entities:\n",
        "            return []\n",
        "\n",
        "        # Aggregate subword tokens\n",
        "        entities = aggregate_tokens(raw_entities)\n",
        "\n",
        "        # Count entity frequencies\n",
        "        entity_counter = collections.Counter(\n",
        "            e[\"word\"] for e in entities\n",
        "        )\n",
        "\n",
        "        # Build entity list with metadata\n",
        "        entity_list = []\n",
        "        for entity_name, freq in entity_counter.items():\n",
        "            # Find the first occurrence for metadata\n",
        "            first_occurrence = next(\n",
        "                (e for e in entities if e[\"word\"] == entity_name),\n",
        "                None\n",
        "            )\n",
        "\n",
        "            if first_occurrence:\n",
        "                entity_list.append({\n",
        "                    \"entity\": entity_name,\n",
        "                    \"entity_group\": first_occurrence[\"entity_group\"],\n",
        "                    \"frequency\": freq,\n",
        "                    \"confidence\": float(first_occurrence[\"score\"])  # Ensure float type\n",
        "                })\n",
        "\n",
        "        return entity_list\n",
        "    except Exception as e:\n",
        "        # Only log actual errors, not deprecation warnings\n",
        "        if \"np.float_\" not in str(e) and \"np.int_\" not in str(e):\n",
        "            print(f\"[NER error] {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def extract_entities_batch(texts: List[str], ner_pipeline) -> List[List[Dict]]:\n",
        "    \"\"\"Extract entities from multiple texts using batch processing (MUCH FASTER).\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "\n",
        "    # Filter out empty texts and track indices\n",
        "    valid_indices = []\n",
        "    valid_texts = []\n",
        "    for i, text in enumerate(texts):\n",
        "        if text and text.strip():\n",
        "            valid_indices.append(i)\n",
        "            valid_texts.append(text)\n",
        "\n",
        "    if not valid_texts:\n",
        "        return [[] for _ in texts]\n",
        "\n",
        "    try:\n",
        "        # Suppress warnings for this specific call\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "            # Process batch - pipeline handles batching automatically\n",
        "            # The pipeline returns a list of lists, one per input text\n",
        "            raw_entities_batch = ner_pipeline(valid_texts)\n",
        "\n",
        "        # Initialize results list\n",
        "        results = [[] for _ in texts]\n",
        "\n",
        "        # Process each text's entities\n",
        "        for batch_idx, orig_idx in enumerate(valid_indices):\n",
        "            if batch_idx < len(raw_entities_batch):\n",
        "                raw_entities = raw_entities_batch[batch_idx]\n",
        "\n",
        "                if not raw_entities:\n",
        "                    continue\n",
        "\n",
        "                # Handle case where pipeline returns single list vs nested list\n",
        "                if isinstance(raw_entities, list) and len(raw_entities) > 0 and isinstance(raw_entities[0], dict):\n",
        "                    # Aggregate subword tokens\n",
        "                    entities = aggregate_tokens(raw_entities)\n",
        "\n",
        "                    # Count entity frequencies\n",
        "                    entity_counter = collections.Counter(\n",
        "                        e[\"word\"] for e in entities\n",
        "                    )\n",
        "\n",
        "                    # Build entity list with metadata\n",
        "                    entity_list = []\n",
        "                    for entity_name, freq in entity_counter.items():\n",
        "                        # Find the first occurrence for metadata\n",
        "                        first_occurrence = next(\n",
        "                            (e for e in entities if e[\"word\"] == entity_name),\n",
        "                            None\n",
        "                        )\n",
        "\n",
        "                        if first_occurrence:\n",
        "                            entity_list.append({\n",
        "                                \"entity\": entity_name,\n",
        "                                \"entity_group\": first_occurrence[\"entity_group\"],\n",
        "                                \"frequency\": freq,\n",
        "                                \"confidence\": float(first_occurrence[\"score\"])\n",
        "                            })\n",
        "\n",
        "                    results[orig_idx] = entity_list\n",
        "\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        # Fallback to individual processing if batch fails\n",
        "        if \"np.float_\" not in str(e) and \"np.int_\" not in str(e):\n",
        "            print(f\"[NER batch error, falling back to individual processing] {e}\")\n",
        "        return [extract_entities(text, ner_pipeline) for text in texts]\n",
        "\n",
        "\n",
        "def link_entities_to_wikipedia(entities: List[Dict], cache: Dict[str, Optional[str]] = None) -> List[Dict]:\n",
        "    \"\"\"Link entities to Wikipedia via Wikidata API with caching.\"\"\"\n",
        "    if cache is None:\n",
        "        cache = {}\n",
        "\n",
        "    linked_entities = []\n",
        "\n",
        "    for entity_data in entities:\n",
        "        entity_name = entity_data[\"entity\"]\n",
        "\n",
        "        # Check cache first\n",
        "        if entity_name in cache:\n",
        "            wiki_url = cache[entity_name]\n",
        "        else:\n",
        "            # Search Wikidata\n",
        "            candidates = wikidata_search(entity_name)\n",
        "\n",
        "            if candidates:\n",
        "                qid = candidates[0][\"id\"]\n",
        "                wiki_url = wikidata_to_wikipedia(qid)\n",
        "            else:\n",
        "                wiki_url = None\n",
        "\n",
        "            # Cache the result (even if None to avoid retrying)\n",
        "            cache[entity_name] = wiki_url\n",
        "\n",
        "        # Add Wikipedia URL if found\n",
        "        entity_data[\"wikipedia_url\"] = wiki_url\n",
        "\n",
        "        linked_entities.append(entity_data)\n",
        "\n",
        "    return linked_entities\n",
        "\n",
        "\n",
        "def process_speech_document(doc: Dict[str, Any], ner_pipeline, link_wikipedia: bool = True, cache: Dict[str, Optional[str]] = None) -> Optional[Dict]:\n",
        "    \"\"\"Process a single speech document and extract entities.\"\"\"\n",
        "    source = doc.get(\"_source\", {})\n",
        "    content = source.get(\"content\", \"\")\n",
        "\n",
        "    if not content:\n",
        "        return None\n",
        "\n",
        "    # Extract entities\n",
        "    entities = extract_entities(content, ner_pipeline)\n",
        "\n",
        "    if not entities:\n",
        "        return None\n",
        "\n",
        "    # Link to Wikipedia if requested\n",
        "    if link_wikipedia:\n",
        "        entities = link_entities_to_wikipedia(entities, cache=cache)\n",
        "\n",
        "    # Return update document\n",
        "    return {\n",
        "        \"_id\": doc[\"_id\"],\n",
        "        \"_source\": {\n",
        "            \"ner_entities\": entities\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"‚úÖ Entity extraction functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YohUPkCjFCue"
      },
      "source": [
        "## 6. Connect to Elasticsearch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1mzt5biFCue",
        "outputId": "f71ead6c-4764-46d5-dfe1-12163a62cdcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° Connecting to Elasticsearch at https://cab-teach-src-oven.trycloudflare.com...\n",
            "‚úÖ Connected to Elasticsearch\n",
            "   Version: 8.6.1\n"
          ]
        }
      ],
      "source": [
        "# Connect to Elasticsearch\n",
        "print(f\"üì° Connecting to Elasticsearch at {ELASTICSEARCH_HOST}...\")\n",
        "es = Elasticsearch(hosts=[ELASTICSEARCH_HOST])\n",
        "\n",
        "try:\n",
        "    if not es.ping():\n",
        "        raise Exception(\"Failed to ping Elasticsearch\")\n",
        "    print(\"‚úÖ Connected to Elasticsearch\")\n",
        "    print(f\"   Version: {es.info()['version']['number']}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection error: {e}\")\n",
        "    print(f\"   Make sure Elasticsearch is running at {ELASTICSEARCH_HOST}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CORhPcORFCue"
      },
      "source": [
        "## 7. Update Elasticsearch Mapping\n",
        "\n",
        "Ensure the index has the correct mapping for `ner_entities` field.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXiwciywFCue",
        "outputId": "473fb884-fa2d-4314-8d0c-17dd3e020dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Updating Elasticsearch mapping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3724821049.py:19: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
            "  es.indices.put_mapping(index=index_name, body=mapping)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated mapping for 'parliament_speeches' with ner_entities field\n"
          ]
        }
      ],
      "source": [
        "def update_elasticsearch_mapping(es: Elasticsearch, index_name: str):\n",
        "    \"\"\"Update Elasticsearch mapping to include ner_entities field.\"\"\"\n",
        "    mapping = {\n",
        "        \"properties\": {\n",
        "            \"ner_entities\": {\n",
        "                \"type\": \"nested\",\n",
        "                \"properties\": {\n",
        "                    \"entity\": {\"type\": \"keyword\"},\n",
        "                    \"entity_group\": {\"type\": \"keyword\"},\n",
        "                    \"frequency\": {\"type\": \"integer\"},\n",
        "                    \"wikipedia_url\": {\"type\": \"keyword\"},\n",
        "                    \"confidence\": {\"type\": \"float\"}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        es.indices.put_mapping(index=index_name, body=mapping)\n",
        "        print(f\"‚úÖ Updated mapping for '{index_name}' with ner_entities field\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Warning: Could not update mapping: {e}\")\n",
        "        print(\"   The field will be added dynamically, but explicit mapping is recommended.\")\n",
        "\n",
        "\n",
        "# Update mapping\n",
        "print(f\"\\nüîß Updating Elasticsearch mapping...\")\n",
        "update_elasticsearch_mapping(es, ELASTICSEARCH_INDEX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qowP-0JmFCue"
      },
      "source": [
        "## 8. Load NER Model\n",
        "\n",
        "Load the TerminatorPower/nerT Turkish NER model. This may take a few minutes on first run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1ej2bPnFCue",
        "outputId": "b9e7e1b5-62cd-4a67-fb2c-e66fddf6a7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ü§ñ Loading NER model (TerminatorPower/nerT)...\n",
            "   Device: GPU (CUDA)\n",
            "   This may take a few minutes on first run...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ NER model loaded successfully\n",
            "   üöÄ Using GPU acceleration for faster processing!\n"
          ]
        }
      ],
      "source": [
        "# Load NER model with GPU support and batch processing\n",
        "print(f\"\\nü§ñ Loading NER model (TerminatorPower/nerT)...\")\n",
        "print(f\"   Device: {'GPU (CUDA)' if device == 0 else 'CPU'}\")\n",
        "print(\"   This may take a few minutes on first run...\")\n",
        "\n",
        "try:\n",
        "    ner_pipeline = pipeline(\n",
        "        \"token-classification\",\n",
        "        model=\"TerminatorPower/nerT\",\n",
        "        aggregation_strategy=\"simple\",\n",
        "        device=device,  # Use GPU if available\n",
        "        batch_size=32  # Process 32 texts at once (adjust based on GPU memory)\n",
        "    )\n",
        "    print(\"‚úÖ NER model loaded successfully\")\n",
        "    if device == 0:\n",
        "        print(\"   üöÄ Using GPU acceleration for faster processing!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to load NER model: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zcUAPKeFCue"
      },
      "source": [
        "## 9. Configuration - OPTIMIZED FOR SPEED\n",
        "\n",
        "Set your processing preferences. With sufficient RAM/GPU, you can significantly increase batch sizes:\n",
        "- **NER Batch Size**: Number of texts processed simultaneously (64-128 recommended for GPU, 16-32 for CPU)\n",
        "- **ES Batch Size**: Documents per Elasticsearch bulk update (500-1000 recommended)\n",
        "- **Wikipedia Linking**: Adds ~0.5s per unique entity (disable for maximum speed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44tvd4U3FCue",
        "outputId": "57f02aed-64e3-4e47-b656-8dbb31e5709f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Configuration (OPTIMIZED FOR SPEED):\n",
            "================================================================================\n",
            "   Wikipedia Linking: False\n",
            "   NER Batch Size: 1024 texts per batch\n",
            "   ES Batch Size: 2000 documents per bulk update\n",
            "   Parallel Workers: 4\n",
            "   Max Text Length: Unlimited\n",
            "   ‚úÖ Wikipedia linking disabled - faster processing\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Configuration - OPTIMIZED FOR SPEED\n",
        "# Set these variables to control processing behavior\n",
        "\n",
        "# Link entities to Wikipedia? (True/False)\n",
        "# WARNING: Wikipedia linking adds ~0.5s per unique entity\n",
        "# Without caching, this can take 30+ hours for 8,930 documents\n",
        "# With caching (recommended), it's much faster after initial lookups\n",
        "LINK_WIKIPEDIA = False  # Set to True to enable Wikipedia linking\n",
        "\n",
        "# Batch processing settings\n",
        "NER_BATCH_SIZE = 1024  # Number of texts to process at once (increase if you have GPU/RAM)\n",
        "ES_BATCH_SIZE = 2000  # Number of documents to bulk update at once (increase for faster updates)\n",
        "PARALLEL_WORKERS = 4  # Number of parallel workers for Wikipedia linking (if enabled)\n",
        "\n",
        "# Memory optimization\n",
        "MAX_TEXT_LENGTH = 0  # Truncate very long texts to avoid memory issues (0 = no limit)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Configuration (OPTIMIZED FOR SPEED):\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"   Wikipedia Linking: {LINK_WIKIPEDIA}\")\n",
        "print(f\"   NER Batch Size: {NER_BATCH_SIZE} texts per batch\")\n",
        "print(f\"   ES Batch Size: {ES_BATCH_SIZE} documents per bulk update\")\n",
        "print(f\"   Parallel Workers: {PARALLEL_WORKERS}\")\n",
        "print(f\"   Max Text Length: {MAX_TEXT_LENGTH if MAX_TEXT_LENGTH > 0 else 'Unlimited'}\")\n",
        "if LINK_WIKIPEDIA:\n",
        "    print(\"   ‚úÖ Wikipedia linking enabled (with caching)\")\n",
        "    print(\"   First pass will be slower, but subsequent entities will be instant\")\n",
        "else:\n",
        "    print(\"   ‚úÖ Wikipedia linking disabled - faster processing\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkys6uGnFCue"
      },
      "source": [
        "## 10. Get Document Count\n",
        "\n",
        "Check how many documents need to be processed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3WyqQfPFCue",
        "outputId": "65e3beb9-c611-4504-d1bf-6fb271fd60b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Counting documents in 'parliament_speeches'...\n",
            "   Found 27,662 documents\n"
          ]
        }
      ],
      "source": [
        "# Get total document count\n",
        "print(f\"\\nüìä Counting documents in '{ELASTICSEARCH_INDEX}'...\")\n",
        "try:\n",
        "    count_response = es.count(index=ELASTICSEARCH_INDEX)\n",
        "    total_docs = count_response[\"count\"]\n",
        "    print(f\"   Found {total_docs:,} documents\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error counting documents: {e}\")\n",
        "    raise\n",
        "\n",
        "if total_docs == 0:\n",
        "    print(\"‚ö†Ô∏è  No documents found in index. Exiting.\")\n",
        "    raise ValueError(\"No documents in index\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWdXryr8FCue"
      },
      "source": [
        "## 11. Process Documents\n",
        "\n",
        "Process all speeches and extract NER entities. This may take a while depending on the number of documents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnXQYNRzFCuf",
        "outputId": "253ef40f-ec36-4ed2-9752-1f368470499b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "Processing speeches with BATCH PROCESSING (OPTIMIZED)...\n",
            "================================================================================\n",
            "üì• Loading documents from Elasticsearch...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27662/27662 [02:56<00:00, 156.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 27,662 documents into memory\n",
            "\n",
            "üöÄ Processing 27,662 documents in batches of 2048...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [03:45<06:56, 46.24s/it]WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(https://cab-teach-src-oven.trycloudflare.com:443)> has failed for 1 times in a row, putting on 1 second timeout\n",
            "WARNING:elastic_transport.transport:Retrying request after failure (attempt 0 of 3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 565, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1430, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 331, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "                              ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 300, in _read_status\n",
            "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
            "http.client.RemoteDisconnected: Remote end closed connection without response\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/elastic_transport/_node/_http_urllib3.py\", line 167, in perform_request\n",
            "    response = self.pool.urlopen(\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/retry.py\", line 449, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/util/util.py\", line 38, in reraise\n",
            "    raise value.with_traceback(tb)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\", line 534, in _make_request\n",
            "    response = conn.getresponse()\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\", line 565, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 1430, in getresponse\n",
            "    response.begin()\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 331, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "                              ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/http/client.py\", line 300, in _read_status\n",
            "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
            "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/elastic_transport/_transport.py\", line 342, in perform_request\n",
            "    resp = node.perform_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/elastic_transport/_node/_http_urllib3.py\", line 202, in perform_request\n",
            "    raise err from e\n",
            "elastic_transport.ConnectionError: Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n",
            "WARNING:elastic_transport.node_pool:Node <Urllib3HttpNode(https://cab-teach-src-oven.trycloudflare.com:443)> has been marked alive after a successful request\n",
            "Batches:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [08:16<03:40, 55.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: 20,480 | Updated: 19,604 | Errors: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [12:13<00:00, 52.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Batch processing complete!\n"
          ]
        }
      ],
      "source": [
        "# Initialize cache for Wikipedia lookups\n",
        "wiki_cache: Dict[str, Optional[str]] = {}\n",
        "NER_BATCH_SIZE = 2048\n",
        "HEADERS = {\n",
        "    # Optional: Add User-Agent header\n",
        "     \"User-Agent\": \"Turkish-NEL-Research/1.0\"\n",
        "}\n",
        "\n",
        "# Process documents with BATCH PROCESSING for maximum speed\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Processing speeches with BATCH PROCESSING (OPTIMIZED)...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Collect all documents first (or process in chunks)\n",
        "print(\"üì• Loading documents from Elasticsearch...\")\n",
        "all_docs = []\n",
        "for doc in tqdm(scan(es, query={\"query\": {\"match_all\": {}}, \"_source\": [\"content\"]},\n",
        "                     index=ELASTICSEARCH_INDEX), total=total_docs, desc=\"Loading\"):\n",
        "    all_docs.append(doc)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(all_docs):,} documents into memory\")\n",
        "\n",
        "# Process in batches\n",
        "processed = 0\n",
        "updated = 0\n",
        "errors = 0\n",
        "es_batch = []\n",
        "\n",
        "# Process documents in batches\n",
        "print(f\"\\nüöÄ Processing {len(all_docs):,} documents in batches of {NER_BATCH_SIZE}...\")\n",
        "\n",
        "for batch_start in tqdm(range(0, len(all_docs), NER_BATCH_SIZE), desc=\"Batches\"):\n",
        "    batch_end = min(batch_start + NER_BATCH_SIZE, len(all_docs))\n",
        "    doc_batch = all_docs[batch_start:batch_end]\n",
        "\n",
        "    try:\n",
        "        # Extract texts and IDs\n",
        "        texts = []\n",
        "        doc_ids = []\n",
        "        doc_indices = []\n",
        "\n",
        "        for idx, doc in enumerate(doc_batch):\n",
        "            content = doc.get(\"_source\", {}).get(\"content\", \"\")\n",
        "            if MAX_TEXT_LENGTH > 0 and len(content) > MAX_TEXT_LENGTH:\n",
        "                content = content[:MAX_TEXT_LENGTH]  # Truncate if needed\n",
        "\n",
        "            texts.append(content)\n",
        "            doc_ids.append(doc[\"_id\"])\n",
        "            doc_indices.append(batch_start + idx)\n",
        "\n",
        "        # Process batch with NER model (MUCH FASTER than individual processing)\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "            entities_batch = extract_entities_batch(texts, ner_pipeline)\n",
        "\n",
        "        # Process each document's entities\n",
        "        for doc_idx, (doc_id, entities) in enumerate(zip(doc_ids, entities_batch)):\n",
        "            processed += 1\n",
        "\n",
        "            if not entities:\n",
        "                continue\n",
        "\n",
        "            # Link to Wikipedia if requested\n",
        "            if LINK_WIKIPEDIA:\n",
        "                entities = link_entities_to_wikipedia(entities, cache=wiki_cache)\n",
        "\n",
        "            # Add to ES batch\n",
        "            es_batch.append({\n",
        "                \"_op_type\": \"update\",\n",
        "                \"_index\": ELASTICSEARCH_INDEX,\n",
        "                \"_id\": doc_id,\n",
        "                \"doc\": {\"ner_entities\": entities}\n",
        "            })\n",
        "            updated += 1\n",
        "\n",
        "            # Bulk update when ES batch is full\n",
        "            if len(es_batch) >= ES_BATCH_SIZE:\n",
        "                try:\n",
        "                    success, failed = bulk(es, es_batch, stats_only=False, raise_on_error=False)\n",
        "                    if failed:\n",
        "                        tqdm.write(f\"\\n‚ö†Ô∏è  Warning: {len(failed)} bulk update failures\")\n",
        "                        errors += len(failed)\n",
        "                except Exception as bulk_err:\n",
        "                    tqdm.write(f\"\\n‚ùå Bulk update error: {bulk_err}\")\n",
        "                    errors += len(es_batch)\n",
        "                es_batch = []\n",
        "\n",
        "        # Progress update\n",
        "        if processed % (NER_BATCH_SIZE * 10) == 0:\n",
        "            cache_info = \"\"\n",
        "            if LINK_WIKIPEDIA:\n",
        "                cache_hits = len([v for v in wiki_cache.values() if v is not None])\n",
        "                cache_total = len(wiki_cache)\n",
        "                cache_info = f\" | Cache: {cache_total} entities ({cache_hits} linked)\"\n",
        "            tqdm.write(f\"Processed: {processed:,} | Updated: {updated:,} | Errors: {errors}{cache_info}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = str(e)\n",
        "        if \"np.float_\" not in error_msg and \"np.int_\" not in error_msg and \"np.complex_\" not in error_msg:\n",
        "            errors += len(doc_batch)\n",
        "            if errors <= 10:\n",
        "                tqdm.write(f\"\\n‚ö†Ô∏è  Error processing batch {batch_start}-{batch_end}: {e}\")\n",
        "\n",
        "# Process remaining ES batch\n",
        "if es_batch:\n",
        "    try:\n",
        "        success, failed = bulk(es, es_batch, stats_only=False, raise_on_error=False)\n",
        "        if failed:\n",
        "            print(f\"\\n‚ö†Ô∏è  Warning: {len(failed)} failures in final batch\")\n",
        "            errors += len(failed)\n",
        "    except Exception as bulk_err:\n",
        "        print(f\"\\n‚ùå Final batch error: {bulk_err}\")\n",
        "        errors += len(es_batch)\n",
        "\n",
        "print(\"\\n‚úÖ Batch processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdyxYhvzFCuf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "DyPfynsjFCuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c400c43-e56e-4f9a-9681-0c36463dec24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ôªÔ∏è  Refreshing index to ensure updates are committed...\n",
            "‚úÖ Index refreshed\n",
            "\n",
            "================================================================================\n",
            "‚úÖ Processing complete!\n",
            "================================================================================\n",
            "   Total processed: 27,662\n",
            "   Documents updated: 26,769\n",
            "   Errors: 0\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Force refresh to ensure all updates are visible\n",
        "print(\"\\n‚ôªÔ∏è  Refreshing index to ensure updates are committed...\")\n",
        "try:\n",
        "    es.indices.refresh(index=ELASTICSEARCH_INDEX)\n",
        "    print(\"‚úÖ Index refreshed\")\n",
        "except Exception as refresh_err:\n",
        "    print(f\"‚ö†Ô∏è  Refresh warning: {refresh_err}\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Processing complete!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"   Total processed: {processed:,}\")\n",
        "print(f\"   Documents updated: {updated:,}\")\n",
        "print(f\"   Errors: {errors}\")\n",
        "if LINK_WIKIPEDIA:\n",
        "    cache_total = len(wiki_cache)\n",
        "    cache_linked = len([v for v in wiki_cache.values() if v is not None])\n",
        "    print(f\"   Unique entities cached: {cache_total:,}\")\n",
        "    print(f\"   Entities with Wikipedia links: {cache_linked:,}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa_M7rLLFCuf"
      },
      "source": [
        "## 13. Verify Results\n",
        "\n",
        "Check a sample document to verify that NER entities were extracted correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WyatQS4_FCuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22929324-9643-48ad-d27c-4cc2df5b228f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking for documents with NER entities...\n",
            "   Total documents in index: 27,662\n",
            "   Documents with NER entities: 10,000\n",
            "\n",
            "‚úÖ Sample document with NER entities:\n",
            "\n",
            "   Document ID: term26-year3-session51-26\n",
            "   Speech Giver: Ahmet Yƒ±ldƒ±rƒ±m\n",
            "   Term: 26, Year: 3\n",
            "\n",
            "   Found 5 entities:\n",
            "\n",
            "   PER:\n",
            "      - ahmet yildirim (freq: 3, conf: 0.999)\n",
            "\n",
            "   LOC:\n",
            "      - mus (freq: 3, conf: 0.970)\n",
            "      - ege (freq: 1, conf: 0.995)\n",
            "\n",
            "   ORG:\n",
            "      - tbmm (freq: 1, conf: 0.997)\n",
            "      - turkiye buyuk millet meclisinin (freq: 1, conf: 0.988)\n"
          ]
        }
      ],
      "source": [
        "# Get a sample document with NER entities\n",
        "# FIXED: Use nested query for nested fields + updated Elasticsearch 8.x API\n",
        "\n",
        "print(\"üîç Checking for documents with NER entities...\")\n",
        "\n",
        "try:\n",
        "    # First, check total count\n",
        "    total_count = es.count(index=ELASTICSEARCH_INDEX)['count']\n",
        "    print(f\"   Total documents in index: {total_count:,}\")\n",
        "\n",
        "    # Use nested query to check for ner_entities (required for nested fields)\n",
        "    count_query = {\n",
        "        \"query\": {\n",
        "            \"nested\": {\n",
        "                \"path\": \"ner_entities\",\n",
        "                \"query\": {\n",
        "                    \"exists\": {\"field\": \"ner_entities.entity\"}\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"size\": 0  # Just get count\n",
        "    }\n",
        "\n",
        "    # Updated for Elasticsearch 8.x API (no 'body' parameter)\n",
        "    response = es.search(index=ELASTICSEARCH_INDEX, **count_query)\n",
        "    docs_with_ner = response['hits']['total']['value']\n",
        "\n",
        "    print(f\"   Documents with NER entities: {docs_with_ner:,}\")\n",
        "\n",
        "    if docs_with_ner > 0:\n",
        "        # Get a sample document\n",
        "        sample_query = {\n",
        "            \"query\": {\n",
        "                \"nested\": {\n",
        "                    \"path\": \"ner_entities\",\n",
        "                    \"query\": {\n",
        "                        \"exists\": {\"field\": \"ner_entities.entity\"}\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"size\": 1\n",
        "        }\n",
        "\n",
        "        # Updated for Elasticsearch 8.x API (no 'body' parameter)\n",
        "        response = es.search(index=ELASTICSEARCH_INDEX, **sample_query)\n",
        "        sample_doc = response['hits']['hits'][0]['_source']\n",
        "\n",
        "        print(\"\\n‚úÖ Sample document with NER entities:\")\n",
        "        print(f\"\\n   Document ID: {response['hits']['hits'][0]['_id']}\")\n",
        "        print(f\"   Speech Giver: {sample_doc.get('speech_giver', 'N/A')}\")\n",
        "        print(f\"   Term: {sample_doc.get('term', 'N/A')}, Year: {sample_doc.get('year', 'N/A')}\")\n",
        "        print(f\"\\n   Found {len(sample_doc.get('ner_entities', []))} entities:\")\n",
        "\n",
        "        # Group entities by type\n",
        "        entities_by_type = {}\n",
        "        for entity in sample_doc.get('ner_entities', []):\n",
        "            entity_type = entity.get('entity_group', 'UNKNOWN')\n",
        "            if entity_type not in entities_by_type:\n",
        "                entities_by_type[entity_type] = []\n",
        "            entities_by_type[entity_type].append(entity)\n",
        "\n",
        "        for entity_type, entities in entities_by_type.items():\n",
        "            print(f\"\\n   {entity_type}:\")\n",
        "            for entity in entities[:5]:  # Show first 5 of each type\n",
        "                wiki_link = entity.get('wikipedia_url', '')\n",
        "                wiki_info = f\" [Wikipedia: {wiki_link}]\" if wiki_link else \"\"\n",
        "                print(f\"      - {entity.get('entity')} (freq: {entity.get('frequency', 0)}, conf: {entity.get('confidence', 0):.3f}){wiki_info}\")\n",
        "            if len(entities) > 5:\n",
        "                print(f\"      ... and {len(entities) - 5} more\")\n",
        "    else:\n",
        "        print(\"\\n‚ö†Ô∏è  No documents with NER entities found yet\")\n",
        "        print(\"\\n   Possible reasons:\")\n",
        "        print(\"   1. Processing is still running (check the progress bar)\")\n",
        "        print(\"   2. Processing completed but no entities were found in documents\")\n",
        "        print(\"   3. Bulk updates haven't been committed yet\")\n",
        "        print(\"\\n   Checking a random document to see its structure...\")\n",
        "\n",
        "        # Check a random document\n",
        "        random_query = {\"query\": {\"match_all\": {}}, \"size\": 1}\n",
        "        random_response = es.search(index=ELASTICSEARCH_INDEX, **random_query)\n",
        "        if random_response['hits']['total']['value'] > 0:\n",
        "            random_doc = random_response['hits']['hits'][0]['_source']\n",
        "            has_content = 'content' in random_doc and len(random_doc.get('content', '')) > 0\n",
        "            has_ner = 'ner_entities' in random_doc\n",
        "            print(f\"   Random document has 'content' field: {has_content}\")\n",
        "            print(f\"   Random document has 'ner_entities' field: {has_ner}\")\n",
        "            if has_ner:\n",
        "                print(f\"   NER entities count: {len(random_doc.get('ner_entities', []))}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error verifying results: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj6KXCbkFCuf"
      },
      "source": [
        "## 14. Next Steps\n",
        "\n",
        "After running this notebook locally:\n",
        "\n",
        "1. **Verify Results**: Check that documents have `ner_entities` populated\n",
        "2. **Sync to GCP VM**: Copy the enriched data to your GCP VM Elasticsearch instance\n",
        "3. **Test API**: Verify that API endpoints return NER entities correctly\n",
        "\n",
        "### Syncing to GCP VM\n",
        "\n",
        "You can sync the data using Elasticsearch reindex API or by:\n",
        "- Exporting documents with NER entities from local ES\n",
        "- Importing them into GCP VM ES\n",
        "\n",
        "Alternatively, you can run this notebook on a GCP VM or Vertex AI Workbench instance connected to your GCP VM Elasticsearch.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}