{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WiDiD with DBSCAN: Incremental Word Sense Discovery\n",
        "## Using DBSCAN Clustering for Parliamentary Speeches\n",
        "\n",
        "This notebook implements the same WiDiD approach as `widid.ipynb` but uses **DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) instead of Affinity Propagation.\n",
        "\n",
        "### Key Differences:\n",
        "- **DBSCAN** automatically determines the number of clusters based on density\n",
        "- Better at handling noise and outliers\n",
        "- Requires tuning `eps` (neighborhood radius) and `min_samples` parameters\n",
        "- Does not assume clusters are convex or have similar sizes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install \"elasticsearch==8.6.2\" sentence-transformers scikit-learn pandas matplotlib\n",
        "from elasticsearch import Elasticsearch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN  # Changed from AffinityPropagation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re, os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "INDEX_NAME = \"parliament_speeches\"\n",
        "ES_URL = \"http://localhost:9200\"   # adjust if different\n",
        "TARGET_WORDS = [\"vergi\"]\n",
        "START_TERM = 17\n",
        "END_TERM = 27\n",
        "YEARS_PER_TERM = 5\n",
        "\n",
        "# DBSCAN specific parameters\n",
        "DBSCAN_EPS = 0.3  # Maximum distance between samples (lower = tighter clusters)\n",
        "DBSCAN_MIN_SAMPLES = 3  # Minimum samples in neighborhood to form core point\n",
        "DBSCAN_METRIC = 'cosine'  # Use cosine distance for semantic similarity\n",
        "\n",
        "BASELINE_MAX_CLUSTERS = 30\n",
        "MAX_CLUSTERS = 50\n",
        "SIMILARITY_THRESHOLD = 0.7\n",
        "TOP_K_CLUSTERS = 3\n",
        "OUTPUT_DIR = \"./dbscan_results\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"üìä DBSCAN Configuration:\")\n",
        "print(f\"   eps={DBSCAN_EPS}, min_samples={DBSCAN_MIN_SAMPLES}, metric='{DBSCAN_METRIC}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Connect to Elasticsearch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "es = Elasticsearch(ES_URL)\n",
        "print(\"Connected to Elasticsearch ‚úÖ\")\n",
        "print(es.info().body[\"version\"][\"number\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_speeches(term, year, size=10000):\n",
        "    \"\"\"Fetch speeches for a specific term and year.\"\"\"\n",
        "    query = {\n",
        "        \"size\": size,\n",
        "        \"_source\": [\"content\", \"term\", \"year\"],\n",
        "        \"query\": {\n",
        "            \"bool\": {\n",
        "                \"must\": [\n",
        "                    {\"term\": {\"term\": term}},\n",
        "                    {\"term\": {\"year\": year}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    res = es.search(index=INDEX_NAME, body=query)\n",
        "    return [hit[\"_source\"][\"content\"] for hit in res[\"hits\"][\"hits\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_term_year_tuples(start_term, end_term):\n",
        "    result = []\n",
        "    for term in range(start_term, end_term + 1):\n",
        "        for year in range(1, 6):\n",
        "            result.append((term, year))\n",
        "    return result\n",
        "\n",
        "TERM_YEAR_TUPLES = make_term_year_tuples(START_TERM, END_TERM)\n",
        "print(f\"Processing {len(TERM_YEAR_TUPLES)} term-year pairs from {TERM_YEAR_TUPLES[0]} to {TERM_YEAR_TUPLES[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_contexts(texts, target_word, window=10):\n",
        "    \"\"\"Extract short context windows around target word and its morphological variations.\"\"\"\n",
        "    contexts = []\n",
        "    pattern = re.compile(rf\"\\b{re.escape(target_word.lower())}\\w*\\b\")\n",
        "    \n",
        "    for t in texts:\n",
        "        tokens = re.findall(r\"\\w+\", t.lower())\n",
        "        for i, tok in enumerate(tokens):\n",
        "            if pattern.match(tok):\n",
        "                start = max(0, i - window)\n",
        "                end = min(len(tokens), i + window + 1)\n",
        "                snippet = \" \".join(tokens[start:end])\n",
        "                contexts.append(snippet)\n",
        "    return contexts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embeddings(model, contexts):\n",
        "    \"\"\"Compute embeddings for context snippets.\"\"\"\n",
        "    if len(contexts) == 0:\n",
        "        return np.empty((0, model.get_sentence_embedding_dimension()))\n",
        "    return model.encode(contexts, show_progress_bar=True)\n",
        "\n",
        "def get_cluster_prototypes(X, labels, return_label_ids=False):\n",
        "    \"\"\"Compute centroids for each cluster and optionally return their IDs.\"\"\"\n",
        "    clusters = []\n",
        "    label_ids = []\n",
        "    for label in np.unique(labels):\n",
        "        if label == -1:\n",
        "            continue\n",
        "        members = X[labels == label]\n",
        "        if len(members) == 0:\n",
        "            continue\n",
        "        centroid = np.mean(members, axis=0)\n",
        "        clusters.append(centroid)\n",
        "        label_ids.append(label)\n",
        "    clusters = np.array(clusters)\n",
        "    if return_label_ids:\n",
        "        return clusters, label_ids\n",
        "    return clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_top_contexts(contexts, labels, n=3):\n",
        "    \"\"\"Print representative contexts for each cluster.\"\"\"\n",
        "    df = pd.DataFrame({\"cluster\": labels, \"context\": contexts})\n",
        "    grouped = df.groupby(\"cluster\")[\"context\"].apply(list)\n",
        "    for cluster, examples in grouped.items():\n",
        "        cluster_name = \"noise/outlier\" if cluster == -1 else cluster\n",
        "        print(f\"\\nüåÄ Cluster {cluster_name} ({len(examples)} examples):\")\n",
        "        for ex in examples[:n]:\n",
        "            print(\"   ‚Ä¢\", ex[:200].replace(\"\\n\", \" \") + (\"...\" if len(ex) > 200 else \"\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def limit_clusters(labels, max_clusters):\n",
        "    \"\"\"Keep only the largest max_clusters and map the rest to -1.\"\"\"\n",
        "    if max_clusters is None:\n",
        "        return labels, np.unique(labels).tolist()\n",
        "    labels = np.asarray(labels)\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    cluster_counts = [\n",
        "        (label, count) for label, count in zip(unique, counts) if label != -1\n",
        "    ]\n",
        "    cluster_counts.sort(key=lambda item: item[1], reverse=True)\n",
        "    keep = [label for label, _ in cluster_counts[:max_clusters]]\n",
        "    if not keep:\n",
        "        return np.full_like(labels, -1), []\n",
        "    filtered = np.array([label if label in keep else -1 for label in labels], dtype=labels.dtype)\n",
        "    return filtered, keep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClusterAligner:\n",
        "    \"\"\"Keeps global cluster IDs and assigns consistent colors over time.\"\"\"\n",
        "    \n",
        "    def __init__(self, max_clusters=100, similarity_threshold=0.8, cmap_name=\"gist_ncar\"):\n",
        "        self.max_clusters = max_clusters\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "        self.centroids = []\n",
        "        self.global_ids = []\n",
        "        self.cmap = plt.cm.get_cmap(cmap_name, max_clusters)\n",
        "        self.palette = [self.cmap(i) for i in range(self.cmap.N)]\n",
        "        self.overflow_color = (0.65, 0.65, 0.65, 1.0)\n",
        "    \n",
        "    def _add_centroid(self, centroid):\n",
        "        if len(self.global_ids) >= self.max_clusters:\n",
        "            return -1\n",
        "        new_id = len(self.global_ids)\n",
        "        self.centroids.append(centroid)\n",
        "        self.global_ids.append(new_id)\n",
        "        return new_id\n",
        "    \n",
        "    def _match_or_create(self, centroid):\n",
        "        centroid = centroid.reshape(1, -1)\n",
        "        if not self.centroids:\n",
        "            return self._add_centroid(centroid)\n",
        "        stacked = np.vstack(self.centroids)\n",
        "        sims = cosine_similarity(stacked, centroid)[:, 0]\n",
        "        best_idx = int(np.argmax(sims))\n",
        "        if sims[best_idx] >= self.similarity_threshold:\n",
        "            return self.global_ids[best_idx]\n",
        "        return self._add_centroid(centroid)\n",
        "    \n",
        "    def align(self, raw_labels, centroid_map):\n",
        "        aligned = np.full_like(raw_labels, -1)\n",
        "        for local_label, centroid in centroid_map.items():\n",
        "            global_id = self._match_or_create(centroid)\n",
        "            if global_id == -1:\n",
        "                continue\n",
        "            aligned[raw_labels == local_label] = global_id\n",
        "        return aligned\n",
        "    \n",
        "    def colors_for(self, labels):\n",
        "        return [\n",
        "            self.palette[label]\n",
        "            if 0 <= label < len(self.palette)\n",
        "            else self.overflow_color\n",
        "            for label in labels\n",
        "        ]\n",
        "    \n",
        "    def get_color(self, label):\n",
        "        \"\"\"Get color for a single label.\"\"\"\n",
        "        if 0 <= label < len(self.palette):\n",
        "            return self.palette[label]\n",
        "        return self.overflow_color\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_tsne_with_coords(term, year, word, tsne_coords, aligned_labels, aligner):\n",
        "    if len(tsne_coords) < 2:\n",
        "        print(\"  Skipping t-SNE (insufficient embeddings).\")\n",
        "        return\n",
        "    \n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # Plot by cluster using numeric markers\n",
        "    unique_labels = sorted(set(label for label in aligned_labels if label >= 0))\n",
        "    \n",
        "    for cluster_id in unique_labels:\n",
        "        mask = aligned_labels == cluster_id\n",
        "        cluster_coords = tsne_coords[mask]\n",
        "        color = aligner.get_color(cluster_id)\n",
        "        \n",
        "        plt.scatter(cluster_coords[:, 0], cluster_coords[:, 1],\n",
        "                   c=[color], marker=f'${cluster_id}$', s=200,\n",
        "                   alpha=0.8, edgecolors='black', linewidths=0.5)\n",
        "    \n",
        "    # Plot noise points if any\n",
        "    if -1 in aligned_labels:\n",
        "        mask = aligned_labels == -1\n",
        "        noise_coords = tsne_coords[mask]\n",
        "        plt.scatter(noise_coords[:, 0], noise_coords[:, 1],\n",
        "                   c=[aligner.overflow_color], marker='x', s=30,\n",
        "                   alpha=0.3, linewidths=0.5, label='Noise')\n",
        "    \n",
        "    plt.title(f\"'{word}' Term {term} Year {year} (t-SNE - DBSCAN)\", fontsize=14, fontweight='bold')\n",
        "    plt.xlabel(\"Dim 1\", fontsize=12)\n",
        "    plt.ylabel(\"Dim 2\", fontsize=12)\n",
        "    plt.grid(alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    tsne_path = os.path.join(OUTPUT_DIR, f\"tsne_term{term}_year{year}_{word}.png\")\n",
        "    plt.savefig(tsne_path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"  Saved t-SNE plot to {tsne_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_cluster_guide(cluster_contexts_map, target_word, output_dir, aligner):\n",
        "    \"\"\"Create a cluster guide with summary CSV and detailed context file.\"\"\"\n",
        "    \n",
        "    if not cluster_contexts_map:\n",
        "        print(\"  No clusters to document.\")\n",
        "        return\n",
        "    \n",
        "    guide_rows = []\n",
        "    for global_id in sorted(cluster_contexts_map.keys()):\n",
        "        contexts = cluster_contexts_map[global_id]\n",
        "        term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "        \n",
        "        guide_rows.append({\n",
        "            'global_id': global_id,\n",
        "            'color_index': global_id,\n",
        "            'total_contexts': len(contexts),\n",
        "            'term_year_span': ', '.join(term_years),\n",
        "            'num_appearances': len(term_years)\n",
        "        })\n",
        "    \n",
        "    df_summary = pd.DataFrame(guide_rows).sort_values('total_contexts', ascending=False)\n",
        "    summary_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_summary.csv\")\n",
        "    df_summary.to_csv(summary_path, index=False)\n",
        "    print(f\"  Saved cluster summary to {summary_path}\")\n",
        "    \n",
        "    context_file_path = os.path.join(output_dir, f\"cluster_guide_{target_word}_contexts.txt\")\n",
        "    with open(context_file_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"{'='*80}\\\\n\")\n",
        "        f.write(f\"CLUSTER GUIDE FOR '{target_word.upper()}' (DBSCAN)\\\\n\")\n",
        "        f.write(f\"Generated: {pd.Timestamp.now()}\\\\n\")\n",
        "        f.write(f\"Total clusters: {len(cluster_contexts_map)}\\\\n\")\n",
        "        f.write(f\"{'='*80}\\\\n\\\\n\")\n",
        "        \n",
        "        for global_id in sorted(cluster_contexts_map.keys(),\n",
        "                               key=lambda x: len(cluster_contexts_map[x]),\n",
        "                               reverse=True):\n",
        "            contexts = cluster_contexts_map[global_id]\n",
        "            term_years = sorted(set(f\"T{ctx['term']}Y{ctx['year']}\" for ctx in contexts))\n",
        "            \n",
        "            color = aligner.get_color(global_id)\n",
        "            color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "                int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "            )\n",
        "            \n",
        "            f.write(f\"\\\\n{'='*80}\\\\n\")\n",
        "            f.write(f\"CLUSTER {global_id} (Color: {color_hex})\\\\n\")\n",
        "            f.write(f\"{'-'*80}\\\\n\")\n",
        "            f.write(f\"Total contexts: {len(contexts)}\\\\n\")\n",
        "            f.write(f\"Appearances: {len(term_years)} term-years\\\\n\")\n",
        "            f.write(f\"Term-year span: {', '.join(term_years)}\\\\n\")\n",
        "            f.write(f\"\\\\nREPRESENTATIVE CONTEXTS:\\\\n\")\n",
        "            f.write(f\"{'-'*80}\\\\n\")\n",
        "            \n",
        "            for ctx_item in contexts[:15]:\n",
        "                f.write(f\"\\\\n[{ctx_item['term']}-{ctx_item['year']}] \")\n",
        "                f.write(ctx_item['context'][:250])\n",
        "                if len(ctx_item['context']) > 250:\n",
        "                    f.write(\"...\")\n",
        "                f.write(\"\\\\n\")\n",
        "            \n",
        "            if len(contexts) > 15:\n",
        "                f.write(f\"\\\\n... and {len(contexts) - 15} more contexts\\\\n\")\n",
        "    \n",
        "    print(f\"  Saved detailed contexts to {context_file_path}\")\n",
        "    print(f\"  Total clusters documented: {len(cluster_contexts_map)}\")\n",
        "    \n",
        "    return df_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_color_reference(cluster_contexts_map, target_word, output_dir, aligner):\n",
        "    \"\"\"Create color reference chart and mapping CSV.\"\"\"\n",
        "    \n",
        "    if not cluster_contexts_map:\n",
        "        print(\"  No clusters to map.\")\n",
        "        return\n",
        "    \n",
        "    used_cluster_ids = sorted(cluster_contexts_map.keys())\n",
        "    \n",
        "    color_mapping = []\n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        color_hex = '#{:02x}{:02x}{:02x}'.format(\n",
        "            int(color[0]*255), int(color[1]*255), int(color[2]*255)\n",
        "        )\n",
        "        color_rgb = f\"({int(color[0]*255)}, {int(color[1]*255)}, {int(color[2]*255)})\"\n",
        "        \n",
        "        color_mapping.append({\n",
        "            'global_id': global_id,\n",
        "            'hex_color': color_hex,\n",
        "            'rgb_color': color_rgb\n",
        "        })\n",
        "    \n",
        "    df_colors = pd.DataFrame(color_mapping)\n",
        "    color_csv_path = os.path.join(output_dir, f'cluster_colors_{target_word}.csv')\n",
        "    df_colors.to_csv(color_csv_path, index=False)\n",
        "    print(f\"  Saved color mapping to {color_csv_path}\")\n",
        "    \n",
        "    import matplotlib.patches as mpatches\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(12, max(6, len(used_cluster_ids) // 4)))\n",
        "    patches = []\n",
        "    \n",
        "    for global_id in used_cluster_ids:\n",
        "        color = aligner.get_color(global_id)\n",
        "        patches.append(mpatches.Patch(color=color, label=f'Cluster {global_id}'))\n",
        "    \n",
        "    ax.legend(handles=patches, loc='center', ncol=min(4, len(patches)), fontsize=10)\n",
        "    ax.axis('off')\n",
        "    plt.title(f\"Color Reference for '{target_word}' Clusters (DBSCAN)\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    chart_path = os.path.join(output_dir, f'color_reference_{target_word}.png')\n",
        "    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"  Saved color reference chart to {chart_path}\")\n",
        "    \n",
        "    return df_colors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Load Sentence Transformer Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Model loaded ‚úÖ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ Main Loop Over Words with DBSCAN Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for target_word in TARGET_WORDS:\n",
        "    print(f\"\\n\\n=== üîç Analyzing '{target_word}' across {len(TERM_YEAR_TUPLES)} term-year pairs (DBSCAN) ===\")\n",
        "    aligner = ClusterAligner(max_clusters=MAX_CLUSTERS, similarity_threshold=SIMILARITY_THRESHOLD)\n",
        "    baseline_used = False\n",
        "    cluster_contexts_map = {}\n",
        "    term_year_data = []\n",
        "    \n",
        "    for term, year in TERM_YEAR_TUPLES:\n",
        "        print(f\"\\n--- Term {term}, Year {year} ---\")\n",
        "        texts = fetch_speeches(term, year)\n",
        "        contexts = extract_contexts(texts, target_word)\n",
        "        print(f\"  Contexts: {len(contexts)}\")\n",
        "        \n",
        "        if len(contexts) < 10:\n",
        "            print(\"  Not enough contexts, skipping this slice.\")\n",
        "            continue\n",
        "        \n",
        "        embeddings = compute_embeddings(model, contexts)\n",
        "        \n",
        "        # DBSCAN Clustering (replaces Affinity Propagation)\n",
        "        dbscan = DBSCAN(\n",
        "            eps=DBSCAN_EPS,\n",
        "            min_samples=DBSCAN_MIN_SAMPLES,\n",
        "            metric=DBSCAN_METRIC\n",
        "        )\n",
        "        dbscan.fit(embeddings)\n",
        "        local_labels = dbscan.labels_\n",
        "        \n",
        "        # Count clusters (excluding noise points labeled as -1)\n",
        "        n_clusters = len(set(local_labels)) - (1 if -1 in local_labels else 0)\n",
        "        n_noise = list(local_labels).count(-1)\n",
        "        print(f\"  DBSCAN found: {n_clusters} clusters, {n_noise} noise points\")\n",
        "        \n",
        "        cap = BASELINE_MAX_CLUSTERS if not baseline_used else MAX_CLUSTERS\n",
        "        limited_labels, kept_clusters = limit_clusters(local_labels, cap)\n",
        "        print(f\"  Kept: {len(kept_clusters)} clusters (cap={cap})\")\n",
        "        \n",
        "        prototypes, proto_labels = get_cluster_prototypes(embeddings, limited_labels, return_label_ids=True)\n",
        "        centroid_map = dict(zip(proto_labels, prototypes))\n",
        "        \n",
        "        if not centroid_map:\n",
        "            print(\"  No clusters survived filtering, skipping visualization.\")\n",
        "            continue\n",
        "        \n",
        "        baseline_used = True\n",
        "        aligned_labels = aligner.align(limited_labels, centroid_map)\n",
        "        global_cluster_count = len(set(label for label in aligned_labels if label >= 0))\n",
        "        print(f\"  Global clusters represented: {global_cluster_count}\")\n",
        "        \n",
        "        # Store context examples for cluster guide\n",
        "        for global_id in set(label for label in aligned_labels if label >= 0):\n",
        "            cluster_context_examples = [\n",
        "                contexts[i] for i, label in enumerate(aligned_labels) if label == global_id\n",
        "            ]\n",
        "            if global_id not in cluster_contexts_map:\n",
        "                cluster_contexts_map[global_id] = []\n",
        "            cluster_contexts_map[global_id].extend([\n",
        "                {'term': term, 'year': year, 'context': ctx}\n",
        "                for ctx in cluster_context_examples[:10]\n",
        "            ])\n",
        "        \n",
        "        print(\"\\n=== Representative Contexts ===\")\n",
        "        show_top_contexts(contexts, aligned_labels)\n",
        "        \n",
        "        # Save CSV\n",
        "        df = pd.DataFrame({\n",
        "            \"term\": term,\n",
        "            \"year\": year,\n",
        "            \"context\": contexts,\n",
        "            \"local_cluster\": limited_labels,\n",
        "            \"global_cluster\": aligned_labels,\n",
        "        })\n",
        "        csv_path = os.path.join(OUTPUT_DIR, f\"widid_term{term}_year{year}_{target_word}.csv\")\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"  Saved clusters to {csv_path}\")\n",
        "        \n",
        "        # Store for unified t-SNE\n",
        "        term_year_data.append({\n",
        "            \"term\": term,\n",
        "            \"year\": year,\n",
        "            \"embeddings\": embeddings,\n",
        "            \"aligned_labels\": aligned_labels\n",
        "        })\n",
        "    \n",
        "    # Compute unified t-SNE across all term-years\n",
        "    if term_year_data:\n",
        "        print(\"\\n=== Computing Unified t-SNE ===\")\n",
        "        all_embeddings = np.vstack([item[\"embeddings\"] for item in term_year_data])\n",
        "        print(f\"  Total embeddings: {len(all_embeddings)}\")\n",
        "        \n",
        "        desired = max(5, len(all_embeddings) // 3)\n",
        "        max_valid = max(1, len(all_embeddings) - 1)\n",
        "        perplexity = min(desired, max_valid, 30)\n",
        "        \n",
        "        tsne_unified = TSNE(n_components=2, random_state=42, perplexity=perplexity).fit_transform(all_embeddings)\n",
        "        print(f\"  t-SNE completed with perplexity={perplexity}\")\n",
        "        \n",
        "        # Plot each term-year\n",
        "        start_idx = 0\n",
        "        for item in term_year_data:\n",
        "            end_idx = start_idx + len(item[\"embeddings\"])\n",
        "            tsne_coords = tsne_unified[start_idx:end_idx]\n",
        "            plot_tsne_with_coords(item[\"term\"], item[\"year\"], target_word,\n",
        "                                tsne_coords, item[\"aligned_labels\"], aligner)\n",
        "            start_idx = end_idx\n",
        "    \n",
        "    # Generate cluster guide\n",
        "    print(\"\\n=== Generating Cluster Guide ===\")\n",
        "    create_cluster_guide(cluster_contexts_map, target_word, OUTPUT_DIR, aligner)\n",
        "    \n",
        "    # Generate color reference\n",
        "    print(\"\\n=== Generating Color Reference ===\")\n",
        "    create_color_reference(cluster_contexts_map, target_word, OUTPUT_DIR, aligner)\n",
        "    print(\"-----------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r dbscan_results.zip dbscan_results/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä DBSCAN Parameter Tuning Guide\n",
        "\n",
        "### Key Parameters:\n",
        "\n",
        "1. **`eps` (epsilon)**: Maximum distance between two samples to be considered neighbors\n",
        "   - **Lower values** (e.g., 0.2): Tighter, more clusters, more noise\n",
        "   - **Higher values** (e.g., 0.5): Looser, fewer clusters, less noise\n",
        "   - Default: 0.3 works well for cosine similarity\n",
        "\n",
        "2. **`min_samples`**: Minimum points in neighborhood to form a core point\n",
        "   - **Lower values** (e.g., 2-3): More sensitive, more small clusters\n",
        "   - **Higher values** (e.g., 5-10): More conservative, larger clusters\n",
        "   - Default: 3 is a good starting point\n",
        "\n",
        "3. **`metric`**: Distance metric\n",
        "   - **'cosine'**: Best for semantic similarity (recommended)\n",
        "   - **'euclidean'**: Standard distance metric\n",
        "\n",
        "### Tuning Tips:\n",
        "- If too many noise points: decrease `min_samples` or increase `eps`\n",
        "- If too few clusters: decrease `eps` or `min_samples`\n",
        "- If too many clusters: increase `eps` or `min_samples`\n",
        "\n",
        "### Comparison with Affinity Propagation:\n",
        "- **AP**: Automatically determines cluster count, can be slow\n",
        "- **DBSCAN**: Requires parameter tuning, faster, better with noise\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
